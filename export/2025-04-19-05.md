# blackhole
# M87
# Brightest Cluster Galaxy
# Brightest Cluster Galaxies
# machine learning
## Quantum algorithm for solving nonlinear differential equations based on physics-informed effective Hamiltonians
- **Url**: http://arxiv.org/abs/2504.13174v1
- **Authors**: ['Hsin-Yu Wu', 'Annie E. Paine', 'Evan Philip', 'Antonio A. Gentile', 'Oleksandr Kyriienko']
- **Abstrat**: We propose a distinct approach to solving linear and nonlinear differential equations (DEs) on quantum computers by encoding the problem into ground states of effective Hamiltonian operators. Our algorithm relies on constructing such operators in the Chebyshev space, where an effective Hamiltonian is a sum of global differential and data constraints. Once the effective Hamiltonian is formed, solutions of differential equations can be obtained using the ground state preparation techniques (e.g. imaginary-time evolution and quantum singular value transformation), bypassing variational search. Unlike approaches based on discrete grids, the algorithm enables evaluation of solutions beyond fixed grid points and implements constraints in the physics-informed way. Our proposal inherits the best traits from quantum machine learning-based DE solving (compact basis representation, automatic differentiation, nonlinearity) and quantum linear algebra-based approaches (fine-grid encoding, provable speed-up for state preparation), offering a robust strategy for quantum scientific computing in the early fault-tolerant era.


**Translated Abstract**: 

我们提出了一种独特的方法，通过将线性和非线性微分方程（DE）编码为有效哈密顿算子的基态，在量子计算机上求解这些方程。我们的算法依赖于在切比雪夫空间中构造这样的算子，其中有效哈密顿算子是全球微分和数据约束的总和。一旦形成有效哈密顿算子，可以通过基态制备技术（例如，虚时间演化和量子奇异值变换）获得微分方程的解，绕过变分搜索。与基于离散网格的方法不同，该算法能够评估超出固定网格点的解，并以物理信息的方式实施约束。我们的提案继承了基于量子机器学习的微分方程求解的最佳特性（紧凑的基表示、自动微分、非线性）和基于量子线性代数的方法（细网格编码、可证明的状态制备加速），提供了一种在早期容错时代量子科学计算的强大策略。

**Summary**:

- (1): 本文研究的背景是微分方程在科学和工程中的重要性以及传统数值方法在处理复杂问题时的局限性。

- (2): 过去的方法主要依赖于网格离散和数值迭代，如有限差分和有限元法，通常需要高精度的细网格。此外，基于变分搜索的量子方法在训练模型时面临困难。与现有方法相比，所提出的方法通过构建有效哈密顿算子，避免了变分训练，更好地处理了微分方程与数据约束的问题。该方法老化了现有方案的缺陷，通过物理信息的方式嵌入约束，从而提高了求解效果。

- (3): 本文的贡献在于提出了一种结合量子机器学习与物理约束的量子微分方程求解器，能够在不依赖变分训练的情况下有效解决线性和非线性微分方程。

- (4): 本文提出的研究方法是通过构建有效哈密顿算子来编码微分方程并在量子Chebyshev空间中进行求解，利用基态制备技术来获得解。

- (5): 本文中的方法主要应用于求解具有恒定系数的二阶常微分方程，结果表明能够有效求解超出固定网格点的解，其性能支持了实现目标的可行性。


## MalMixer: Few-Shot Malware Classification with Retrieval-Augmented Semi-Supervised Learning
- **Url**: http://arxiv.org/abs/2409.13213v4
- **Authors**: ['Jiliang Li', 'Yifan Zhang', 'Yu Huang', 'Kevin Leach']
- **Abstrat**: Recent growth and proliferation of malware have tested practitioners ability to promptly classify new samples according to malware families. In contrast to labor-intensive reverse engineering efforts, machine learning approaches have demonstrated increased speed and accuracy. However, most existing deep-learning malware family classifiers must be calibrated using a large number of samples that are painstakingly manually analyzed before training. Furthermore, as novel malware samples arise that are beyond the scope of the training set, additional reverse engineering effort must be employed to update the training set. The sheer volume of new samples found in the wild creates substantial pressure on practitioners ability to reverse engineer enough malware to adequately train modern classifiers. In this paper, we present MalMixer, a malware family classifier using semi-supervised learning that achieves high accuracy with sparse training data. We present a domain-knowledge-aware data augmentation technique for malware feature representations, enhancing few-shot performance of semi-supervised malware family classification. We show that MalMixer achieves state-of-the-art performance in few-shot malware family classification settings. Our research confirms the feasibility and effectiveness of lightweight, domain-knowledge-aware data augmentation methods for malware features and shows the capabilities of similar semi-supervised classifiers in addressing malware classification issues.


**Translated Abstract**: 

近年来，恶意软件的增长和传播考验着从业者及时将新样本分类到恶意软件家族的能力。与劳动密集型的反向工程工作相比，机器学习方法展现了更高的速度和准确性。然而，现有的大多数深度学习恶意软件家族分类器需要使用大量样本进行校准，且这些样本是在培训前费时费力地手动分析的。此外，当出现超出训练集范围的新恶意软件样本时，必须进行额外的反向工程工作以更新训练集。在野外发现的新样本数量庞大，给从业者在反向工程中充分训练现代分类器施加了巨大压力。本文提出了MalMixer，一种利用半监督学习的恶意软件家族分类器，能够以稀疏的训练数据实现高准确率。我们提出了一种基于领域知识的数据增强技术，用于恶意软件特征表示，增强了半监督恶意软件家族分类的少样本性能。研究表明，MalMixer在少样本恶意软件家族分类设置中达到了最先进的性能。我们的研究确认了轻量级、基于领域知识的数据增强方法在恶意软件特征中的可行性和有效性，并展示了类似的半监督分类器在解决恶意软件分类问题方面的能力。

**Summary**:

- (1): 本文研究背景为恶意软件的快速增长和新变种的持续出现，使得及时和准确地将恶意软件样本分类到相应家族成为一种迫切需求。

- (2): 过去的方法多依赖大规模标注样本进行深度学习训练，但这些方法在处理新样本时效果较差，且更新需要耗费大量反向工程时间。与现有方法不同，本文提出的MalMixer使用稀疏的训练数据和半监督学习，可以在少样本情况下实现高准确率，有效缓解了样本不足的问题。

- (3): 本文的贡献在于提出了MalMixer这一新型的恶意软件家族分类器，使用领域知识驱动的数据增强技术，提升了少样本分类的性能，实现了最先进的分类效果。

- (4): 本文提出的研究方法是通过结合半监督学习和领域知识的数据增强技术，以提升恶意软件特征表示的准确性，从而提高分类器在稀疏标签数据下的表现。

- (5): 本文的方法在少样本恶意软件家族分类任务上达到了最先进的性能，表明该性能能有效支持其目标，并提升了恶意软件的及时识别能力。


## Predicting BVD Re-emergence in Irish Cattle From Highly Imbalanced Herd-Level Data Using Machine Learning Algorithms
- **Url**: http://arxiv.org/abs/2504.13116v1
- **Authors**: ['Niamh Mimnagh', 'Andrew Parnell', 'Conor McAloon', 'Jaden Carlson', 'Maria Guelbenzu', 'Jonas Brock', 'Damien Barrett', 'Guy McGrath', 'Jamie Tratalos', 'Rafael Moral']
- **Abstrat**: Bovine Viral Diarrhoea (BVD) has been the focus of a successful eradication programme in Ireland, with the herd-level prevalence declining from 11.3% in 2013 to just 0.2% in 2023. As the country moves toward BVD freedom, the development of predictive models for targeted surveillance becomes increasingly important to mitigate the risk of disease re-emergence. In this study, we evaluate the performance of a range of machine learning algorithms, including binary classification and anomaly detection techniques, for predicting BVD-positive herds using highly imbalanced herd-level data. We conduct an extensive simulation study to assess model performance across varying sample sizes and class imbalance ratios, incorporating resampling, class weighting, and appropriate evaluation metrics (sensitivity, positive predictive value, F1-score and AUC values). Random forests and XGBoost models consistently outperformed other methods, with the random forest model achieving the highest sensitivity and AUC across scenarios, including real-world prediction of 2023 herd status, correctly identifying 219 of 250 positive herds while halving the number of herds that require compared to a blanket-testing strategy.


**Translated Abstract**: 

牛病毒性腹泻（BVD）一直是爱尔兰成功根除计划的重点，动物群体的患病率从2013年的11.3%下降到2023年的0.2%。随着国家向BVD自由状态迈进，开发用于定向监测的预测模型变得越来越重要，以减轻疾病重新出现的风险。本研究评估了包括二元分类和异常检测技术在内的一系列机器学习算法在使用高度不平衡的动物群体数据预测BVD阳性群体的性能。我们进行了一项广泛的模拟研究，以评估模型在不同样本大小和类别不平衡比率下的表现，结合了重采样、类权重及适当的评估指标（敏感性、正预测值、F1分数和AUC值）。随机森林和XGBoost模型在其他方法中表现最佳，其中随机森林模型在各场景中实现了最高的敏感性和AUC，包括对2023年群体状态的现实预测，正确识别出250个阳性群体中的219个，同时将需要检测的动物群体数量减少了一半，相较于传统的全面检测策略。

**Summary**:

- (1): 这篇文章的研究背景是牛病毒性腹泻（BVD）对爱尔兰乳牛和肉牛产业造成的经济影响，医院通过根除计划成功将其患病率降低至极低水平。

- (2): 以往的方法主要依赖于广泛的监测和常规检测，然而当前的低患病率使得这些方法效率低下。这篇文章提出的机器学习方法，特别是随机森林和XGBoost，利用高度不平衡的数据进行预测，从而解决了传统方法在低阳性案例中的识别困难。这一方法是有充分动机的，通过其优越的表现证明了其可行性。

- (3): 本文的贡献在于开发了一套有效的机器学习模型用于BVD阳性群体的预测，提供了一种优化现有监测方法的手段。

- (4): 本文提出的研究方法包括多种机器学习算法的比较，尤其注重于二元分类和异常检测，解决了传统方法在处理高度不平衡数据时的问题。

- (5): 本文的方法在预测BVD阳性群体的任务上取得了良好表现，随机森林模型正确识别了250个阳性群体中的219个，验证了其目标的可实现性和有效性。


## Quorum: Zero-Training Unsupervised Anomaly Detection using Quantum Autoencoders
- **Url**: http://arxiv.org/abs/2504.13113v1
- **Authors**: ['Jason Zev Ludmir', 'Sophia Rebello', 'Jacob Ruiz', 'Tirthak Patel']
- **Abstrat**: Detecting mission-critical anomalous events and data is a crucial challenge across various industries, including finance, healthcare, and energy. Quantum computing has recently emerged as a powerful tool for tackling several machine learning tasks, but training quantum machine learning models remains challenging, particularly due to the difficulty of gradient calculation. The challenge is even greater for anomaly detection, where unsupervised learning methods are essential to ensure practical applicability. To address these issues, we propose Quorum, the first quantum anomaly detection framework designed for unsupervised learning that operates without requiring any training.


**Translated Abstract**: 

检测关键任务的异常事件和数据是各行业面临的一项重要挑战，包括金融、医疗和能源。量子计算最近已成为解决几个机器学习任务的强大工具，但训练量子机器学习模型仍然具有挑战性，尤其是在梯度计算困难的情况下。对于异常检测而言，该挑战更大，因为无监督学习方法是确保实际应用所必需的。为了解决这些问题，我们提出了Quorum，这是首个设计用于无监督学习的量子异常检测框架，且无需任何训练。

**Summary**:

- (1): 本文研究的背景包括在金融、医疗和能源等行业中，检测异常事件和数据是一个关键挑战，且传统机器学习方法在处理复杂数据集时面临可扩展性和准确性的问题。

- (2): 过去的方法主要依赖于量子机器学习（QML）模型，这些模型通常需要参数化电路的训练，但由于梯度计算的复杂性，难以实现。现有方法在于需要训练，并且通常依赖于监督或半监督学习，但在缺乏标记数据的实际场景中适用性有限。所提出的方法Quorum与现有方法的不同之处在于，它无需训练且能够进行无监督学习，有效解决了训练复杂性和无监督学习需求的问题。

- (3): 本文的贡献在于提出了Quorum，首个不需要训练的量子异常检测框架，有效应对了异常检测中的无监督学习需求，并实现了高效的异常识别。

- (4): 本文提出的研究方法是利用量子原理进行数据的幅度编码、随机量子变换和SWAP测试，以识别异常，完全消除了对参数优化的需求。

- (5): 本文在多个数据集上进行的评估表明，Quorum的平均F1得分比依赖训练的量子神经网络（QNN）高出23%。这一性能表现支持了其在无监督异常检测任务中的目标。


## EchoWorld: Learning Motion-Aware World Models for Echocardiography Probe Guidance
- **Url**: http://arxiv.org/abs/2504.13065v1
- **Authors**: ['Yang Yue', 'Yulin Wang', 'Haojun Jiang', 'Pan Liu', 'Shiji Song', 'Gao Huang']
- **Abstrat**: Echocardiography is crucial for cardiovascular disease detection but relies heavily on experienced sonographers. Echocardiography probe guidance systems, which provide real-time movement instructions for acquiring standard plane images, offer a promising solution for AI-assisted or fully autonomous scanning. However, developing effective machine learning models for this task remains challenging, as they must grasp heart anatomy and the intricate interplay between probe motion and visual signals. To address this, we present EchoWorld, a motion-aware world modeling framework for probe guidance that encodes anatomical knowledge and motion-induced visual dynamics, while effectively leveraging past visual-motion sequences to enhance guidance precision. EchoWorld employs a pre-training strategy inspired by world modeling principles, where the model predicts masked anatomical regions and simulates the visual outcomes of probe adjustments. Built upon this pre-trained model, we introduce a motion-aware attention mechanism in the fine-tuning stage that effectively integrates historical visual-motion data, enabling precise and adaptive probe guidance. Trained on more than one million ultrasound images from over 200 routine scans, EchoWorld effectively captures key echocardiographic knowledge, as validated by qualitative analysis. Moreover, our method significantly reduces guidance errors compared to existing visual backbones and guidance frameworks, excelling in both single-frame and sequential evaluation protocols. Code is available at https://github.com/LeapLabTHU/EchoWorld.


**Translated Abstract**: 

超声心动图在心血管疾病检测中至关重要，但在很大程度上依赖于经验丰富的超声医师。超声心动图探头引导系统，通过提供实时运动指令以获取标准平面图像，提供了一种AI辅助或完全自主扫描的前景。然而，开发有效的机器学习模型仍然具有挑战性，因为这些模型必须掌握心脏解剖结构以及探头运动与视觉信号之间的复杂相互作用。为了解决这个问题，我们提出了EchoWorld，这是一个运动感知的世界建模框架，用于探头引导，该框架编码了解剖知识和运动引起的视觉动态，同时有效利用过去的视觉-运动序列来增强引导精度。EchoWorld采用了一种基于世界建模原则的预训练策略，其中模型预测遮罩解剖区域，并模拟探头调整的视觉结果。在此预训练模型的基础上，我们在微调阶段引入了一种运动感知注意机制，能够有效整合历史视觉-运动数据，实现精确且适应性强的探头引导。EchoWorld在超过200例常规扫描中的100万张超声图像上进行训练，有效捕捉了关键的超声心动图知识，并通过定性分析进行验证。此外，我们的方法相比于现有的视觉骨干和引导框架显著减少了引导误差，在单帧和序列评估协议中表现出色。代码可在 https://github.com/LeapLabTHU/EchoWorld 获取。

**Summary**:

- (1): 本文的研究背景是心血管疾病检测的重要性及其对实时、准确超声心动图操作的需求。

- (2): 过去的方法多为辅助和自主扫描系统，但存在对心脏解剖结构和探头运动的理解不足的问题。提出的方法与现有方法的区别在于采用运动感知的框架，能够有效整合视觉和运动数据，从而解决上述问题，具有充分的动机。

- (3): 本文的贡献在于提出了EchoWorld，一个运动感知的世界建模框架，通过编码解剖知识和视觉动态，提高了探头引导的精确度和适应性。

- (4): 本文提出的研究方法论包括基于世界建模的预训练策略和在此基础上引入的运动感知注意机制，从而实现更好的引导效果。

- (5): 本文的方法在超声诊断任务中进行评估，获得了显著降低引导误差的性能，支持其目标的实现。


## Unifying Feature-Based Explanations with Functional ANOVA and Cooperative Game Theory
- **Url**: http://arxiv.org/abs/2412.17152v2
- **Authors**: ['Fabian Fumagalli', 'Maximilian Muschalik', 'Eyke Hüllermeier', 'Barbara Hammer', 'Julia Herbinger']
- **Abstrat**: Feature-based explanations, using perturbations or gradients, are a prevalent tool to understand decisions of black box machine learning models. Yet, differences between these methods still remain mostly unknown, which limits their applicability for practitioners. In this work, we introduce a unified framework for local and global feature-based explanations using two well-established concepts: functional ANOVA (fANOVA) from statistics, and the notion of value and interaction from cooperative game theory. We introduce three fANOVA decompositions that determine the influence of feature distributions, and use game-theoretic measures, such as the Shapley value and interactions, to specify the influence of higher-order interactions. Our framework combines these two dimensions to uncover similarities and differences between a wide range of explanation techniques for features and groups of features. We then empirically showcase the usefulness of our framework on synthetic and real-world datasets.


**Translated Abstract**:

特征基础解释，使用扰动或梯度，是理解黑箱机器学习模型决策的普遍工具。然而，这些方法之间的差异仍然大多未知，这限制了它们对从业者的适用性。在这项工作中，我们引入了一个统一的框架，用于本地和全局特征基础解释，利用两个成熟的概念：统计学中的函数方差分析（fANOVA），以及合作博弈论中的价值和互动概念。我们引入了三种 fANOVA 分解方法，以确定特征分布的影响，并使用博弈理论度量（如 Shapley 值和互动）来指定高阶互动的影响。我们的框架结合了这两个维度，以揭示各种特征和特征组的解释技术之间的相似之处和差异。我们随后在合成和真实世界数据集上实证展示了我们框架的实用性。

**Summary**:

- (1): 随着黑箱机器学习模型在高风险决策领域的应用增加，解释性人工智能（XAI）研究的兴起使得理解模型推理变得重要。

- (2): 过往方法包括多种特征基础解释工具，面临的主要问题是它们之间的关联性不明和选择困难。所提出的框架利用 fANOVA 和合作博弈论的观点来统一这些方法，解决了现有方法无法回答的关系和选择问题。

- (3): 本文贡献在于提出了一个综合性的特征基础解释框架，包括本地和全局解释，以及三种特征影响量度，帮助研究者理解现有方法的关联性。

- (4): 研究方法论采用 fANOVA 进行特征分布的分析，并结合合作博弈论的 Shapley 值和互动效应，以提供对特征影响的深入分析。

- (5): 本文在合成和真实世界数据集上展示了所提框架的有效性，达到了预期的性能目标，支持了研究目标。


## Impact of Data Duplication on Deep Neural Network-Based Image Classifiers: Robust vs. Standard Models
- **Url**: http://arxiv.org/abs/2504.00638v2
- **Authors**: ['Alireza Aghabagherloo', 'Aydin Abadi', 'Sumanta Sarkar', 'Vishnu Asutosh Dasu', 'Bart Preneel']
- **Abstrat**: The accuracy and robustness of machine learning models against adversarial attacks are significantly influenced by factors such as training data quality, model architecture, the training process, and the deployment environment. In recent years, duplicated data in training sets, especially in language models, has attracted considerable attention. It has been shown that deduplication enhances both training performance and model accuracy in language models. While the importance of data quality in training image classifier Deep Neural Networks (DNNs) is widely recognized, the impact of duplicated images in the training set on model generalization and performance has received little attention.   In this paper, we address this gap and provide a comprehensive study on the effect of duplicates in image classification. Our analysis indicates that the presence of duplicated images in the training set not only negatively affects the efficiency of model training but also may result in lower accuracy of the image classifier. This negative impact of duplication on accuracy is particularly evident when duplicated data is non-uniform across classes or when duplication, whether uniform or non-uniform, occurs in the training set of an adversarially trained model. Even when duplicated samples are selected in a uniform way, increasing the amount of duplication does not lead to a significant improvement in accuracy.


**Translated Abstract**: 

机器学习模型的准确性和对对抗攻击的鲁棒性受到训练数据质量、模型架构、训练过程和部署环境等因素的显著影响。近年来，训练集中的重复数据，尤其是在语言模型中，已引起相当大的关注。已有研究表明，去重可以增强语言模型的训练性能和模型准确性。尽管训练图像分类深度神经网络(DNN)时数据质量的重要性被广泛认可，但训练集中重复图像对模型泛化和性能的影响却鲜有关注。本文旨在解决这一空白，并提供关于图像分类中重复数据影响的全面研究。我们的分析表明，训练集中重复图像的存在不仅会负面影响模型训练的效率，还可能导致图像分类器的准确性降低。重复数据对准确性的负面影响在类之间不均匀重复时尤为明显，或者当对抗训练模型的训练集中发生重复时，无论是均匀还是不均匀选择重复样本，增加重复数量并不会显著提高准确性。

**Summary**:

- (1): 本文研究背景是机器学习模型的准确性和鲁棒性受训练数据质量影响，重点关注图像分类中的重复数据问题。

- (2): 过去的研究主要关注语言模型中的重复数据去除，未深入探讨图像分类中的重复数据影响。现有方法主要在于通过消除重复来提升模型性能。本文的方法通过全面分析重复数据对图像分类的影响，特别是探讨重复数据的均匀和不均匀分布对模型的影响，有效填补了文献中的空缺。

- (3): 本文的贡献在于提供了关于重复图像数据对图像分类深度神经网络影响的全面研究，揭示了重复数据对模型训练效率和准确性的负面影响，并指出在对抗训练中重复数据的问题。

- (4): 研究方法包括对训练集中不同水平的图像重复进行实验分析，比较均匀和不均匀的重复情况对模型泛化和准确性的影响。

- (5): 本文针对图像分类任务，研究发现重复数据会显著降低模型的准确性和训练效率，表明其性能支持了研究目标的有效性。


## The Dissipation Theory of Aging: A Quantitative Analysis Using a Cellular Aging Map
- **Url**: http://arxiv.org/abs/2504.13044v1
- **Authors**: ['Farhan Khodaee', 'Rohola Zandie', 'Yufan Xia', 'Elazer R. Edelman']
- **Abstrat**: We propose a new theory for aging based on dynamical systems and provide a data-driven computational method to quantify the changes at the cellular level. We use ergodic theory to decompose the dynamics of changes during aging and show that aging is fundamentally a dissipative process within biological systems, akin to dynamical systems where dissipation occurs due to non-conservative forces. To quantify the dissipation dynamics, we employ a transformer-based machine learning algorithm to analyze gene expression data, incorporating age as a token to assess how age-related dissipation is reflected in the embedding space. By evaluating the dynamics of gene and age embeddings, we provide a cellular aging map (CAM) and identify patterns indicative of divergence in gene embedding space, nonlinear transitions, and entropy variations during aging for various tissues and cell types. Our results provide a novel perspective on aging as a dissipative process and introduce a computational framework that enables measuring age-related changes with molecular resolution.


**Translated Abstract**: 

我们提出了一种基于动力系统的新老化理论，并提供了一种数据驱动的计算方法来量化细胞水平的变化。我们使用遍历理论来分解老化过程中变化的动态，表明老化在生物系统中本质上是一个耗散过程，类似于由于非保守力导致耗散的动力系统。为了量化耗散动态，我们采用基于变换器的机器学习算法分析基因表达数据，结合年龄作为标记，评估与年龄相关的耗散如何在嵌入空间中反映。通过评估基因和年龄嵌入的动态变化，我们提供了一个细胞老化图（CAM），并识别出在不同组织和细胞类型中老化过程中基因嵌入空间的发散、非线性转变和熵变化的模式。我们的结果为老化作为耗散过程提供了新的视角，并引入了一种计算框架，使得以分子级别测量与年龄相关的变化成为可能。

**Summary**:

- (1): 本文研究的背景是老化是一个普遍的生物过程，但揭示其背后的机制仍然是生物学中的一个挑战。

- (2): 过去的方法主要集中在各种老化理论和可观察的老化特征上，但通常限于生物观察，缺乏统一框架。与现有方法不同，本文提出了基于动力系统的耗散理论，利用机器学习对基因表达数据进行分析，从而克服了以往方法的局限。

- (3): 本文的贡献在于提出了一种新的老化理论，并构建了细胞老化图，揭示了不同组织和细胞类型中老化的耗散特征。

- (4): 本文的研究方法包括利用遍历理论和 Hopf 分解定理，通过变换器基础的机器学习算法分析基因表达数据，以量化老化过程中的耗散动态。

- (5): 本文主要任务是构建细胞老化图（CAM），分析基因和年龄嵌入的动态变化，以分子分辨率测量与年龄相关的变化，并成功揭示了老化过程中的关键模式，支持了其研究目标。


## Machine Learning Decoding of Circuit-Level Noise for Bivariate Bicycle Codes
- **Url**: http://arxiv.org/abs/2504.13043v1
- **Authors**: ['John Blue', 'Harshil Avlani', 'Zhiyang He', 'Liu Ziyin', 'Isaac L. Chuang']
- **Abstrat**: Fault-tolerant quantum computers will depend crucially on the performance of the classical decoding algorithm which takes in the results of measurements and outputs corrections to the errors inferred to have occurred. Machine learning models have shown great promise as decoders for the surface code; however, this promise has not yet been substantiated for the more challenging task of decoding quantum low-density parity-check (QLDPC) codes. In this paper, we present a recurrent, transformer-based neural network designed to decode circuit-level noise on Bivariate Bicycle (BB) codes, introduced recently by Bravyi et al (Nature 627, 778-782, 2024). For the $[[72,12,6]]$ BB code, at a physical error rate of $p=0.1\%$, our model achieves a logical error rate almost $5$ times lower than belief propagation with ordered statistics decoding (BP-OSD). Moreover, while BP-OSD has a wide distribution of runtimes with significant outliers, our model has a consistent runtime and is an order-of-magnitude faster than the worst-case times from a benchmark BP-OSD implementation. On the $[[144,12,12]]$ BB code, our model obtains worse logical error rates but maintains the speed advantage. These results demonstrate that machine learning decoders can out-perform conventional decoders on QLDPC codes, in regimes of current interest.


**Translated Abstract**: 

容错量子计算机的性能在很大程度上依赖于经典解码算法的表现，该算法接收测量结果并输出对推断发生的错误的更正。机器学习模型在表面码的解码中显示出巨大潜力；然而，这种潜力尚未在解码量子低密度奇偶检验（QLDPC）码的更具挑战性的任务中得到证实。在本文中，我们提出了一种基于递归和变压器的神经网络，旨在解码Bivariate Bicycle（BB）码上的电路级噪声。对于$[[72,12,6]]$ BB码，在物理错误率为$p=0.1\%$的情况下，我们的模型实现的逻辑错误率比带有有序统计解码（BP-OSD）的信念传播低近5倍。此外，与拥有显著离群值的BP-OSD的运行时间广泛分布不同，我们的模型拥有一致的运行时间，并且比基准BP-OSD实现的最坏情况时间快一个数量级。在$[[144,12,12]]$ BB码上，我们的模型实现了较差的逻辑错误率，但保持了速度优势。这些结果表明，机器学习解码器可以在当前关注的领域中超越传统解码器的表现。

**Summary**:

- (1): 文章背景涉及量子错误校正（QEC），其被认为是大规模量子计算机的必要组成部分，尤其依赖于有效的经典解码算法。

- (2): 以往的方法主要是基于信念传播与有序统计解码（BP-OSD），其存在较高的运行时间方差及最坏情况下的立方时间复杂度。本文提出的基于递归和变压器的神经网络，相比于传统方法解决了实时解码中速度不稳定的问题，并且在多个情况下显示出更低的逻辑错误率，因此具有良好的动机。

- (3): 本文的贡献在于展示了机器学习解码器对于量子低密度奇偶检验（QLDPC）码在实际应用中的超越传统解码器的潜力。

- (4): 本文研究方法包括设计了一种基于递归与变压器的神经网络，专门用于解码电路级噪声，结合了快速的训练和高效的运行时间。

- (5): 在$[[72,12,6]]$ BB码的任务上，模型在物理错误率为$p=0.1\%$时的逻辑错误率比BP-OSD低近5倍，并保持了较高的速度，从而支持了其研究目标。


## Mathematical programs with complementarity constraints and application to hyperparameter tuning for nonlinear support vector machines
- **Url**: http://arxiv.org/abs/2504.13006v1
- **Authors**: ['Samuel Ward', 'Alain Zemkoho', 'Selin Ahipasaoglu']
- **Abstrat**: We consider the Mathematical Program with Complementarity Constraints (MPCC). One of the main challenges in solving this problem is the systematic failure of standard Constraint Qualifications (CQs). Carefully accounting for the combinatorial nature of the complementarity constraints, tractable versions of the Mangasarian Fromovitz Constraint Qualification (MFCQ) have been designed and widely studied in the literature. This paper looks closely at two such MPCC-MFCQs and their influence on MPCC algorithms. As a key contribution, we prove the convergence of the sequential penalisation and Scholtes relaxation algorithms under a relaxed MPCC-MFCQ that is much weaker than the CQs currently used in the literature. We then form the problem of tuning hyperparameters of a nonlinear Support Vector Machine (SVM), a fundamental machine learning problem for classification, as a MPCC. For this application, we establish that the aforementioned relaxed MPCC-MFCQ holds under a very mild assumption. Moreover, we program robust implementations and comprehensive numerical experimentation on real-world data sets, where we show that the sequential penalisation method applied to the MPCC formulation for tuning SVM hyperparameters can outperform both the Scholtes relaxation technique and the state-of-the-art derivative-free methods from the machine learning literature.


**Translated Abstract**:

我们考虑带有互补约束的数学规划（MPCC）。解决该问题的主要挑战之一是标准约束条件的系统失败。仔细考虑互补约束的组合特性，文献中设计并广泛研究了可处理版本的Mangasarian Fromovitz约束条件（MFCQ）。本文仔细研究了两种此类MPCC-MFCQs及其对MPCC算法的影响。作为关键贡献，我们证明了在一种比当前文献中使用的约束条件弱得多的放松MPCC-MFCQ下，序列惩罚和Scholtes松弛算法的收敛性。然后我们将非线性支持向量机（SVM）的超参数调优问题形成一个MPCC。对于这个应用，我们证明上述放松的MPCC-MFCQ在非常温和的假设下成立。此外，我们编程进行稳健的实现和全面的数值实验，结果显示应用于MPCC表述的超参数调优的序列惩罚方法可以超越Scholtes松弛技术以及机器学习文献中的最新无导数方法。

**Summary**:

- (1): 本文研究了带有互补约束的数学规划（MPCC），这一领域在经济学、化学工程、能源分配、交通运输和机器学习等多个应用中具有广泛意义。

- (2): 过去的标准方法由于互补性质导致约束条件故障，许多文献提出面向互补约束的Mangasarian Fromovitz约束条件（MFCQ），但这些方法未能有效解决MPCC问题。本文提出了一种放松的MPCC-MFCQ，远比现有方法弱，能够应对这些约束条件带来的挑战。

- (3): 本文的贡献在于证明了序列惩罚和Scholtes松弛算法在放松的MPCC-MFCQ下的收敛性，并在超参数调优的背景下提供了有效的解决方案。

- (4): 本文提出的研究方法包括基于MPCC框架构建的序列惩罚方法和Scholtes松弛算法，并进行了理论分析与实施。

- (5): 本文在超参数调优的任务上进行了综合的数值实验，结果表明，该方法优于Scholtes松弛及现有的无导数优化方法，获取的性能结果支持了他们的研究目标。


## Automated Generation of Commit Messages in Software Repositories
- **Url**: http://arxiv.org/abs/2504.12998v1
- **Authors**: ['Varun Kumar Palakodeti', 'Abbas Heydarnoori']
- **Abstrat**: Commit messages are crucial for documenting software changes, aiding in program comprehension and maintenance. However, creating effective commit messages is often overlooked by developers due to time constraints and varying levels of documentation skills. Our research presents an automated approach to generate commit messages using Machine Learning (ML) and Natural Language Processing (NLP) by developing models that use techniques such as Logistic Regression with TF-IDF and Word2Vec, as well as more sophisticated methods like LSTM. We used the dataset of code changes and corresponding commit messages that was used by Liu et al., which we used to train and evaluate ML/NLP models and was chosen because it is extensively used in previous research, also for comparability in our study. The objective was to explore which ML/NLP techniques generate the most effective, clear, and concise commit messages that accurately reflect the code changes. We split the dataset into training, validation, and testing sets and used these sets to evaluate the performance of each model using qualitative and quantitative evaluation methods. Our results reveal a spectrum of effectiveness among these models, with the highest BLEU score achieved being 16.82, showcasing the models' capability in automating a clear and concise commit message generation. Our paper offers insights into the comparative effectiveness of different machine learning models for automating commit message generation in software development, aiming to enhance the overall practice of code documentation. The source code is available at https://doi.org/10.5281/zenodo.10888106.


**Translated Abstract**: 

提交信息对于记录软件变更至关重要，有助于程序理解和维护。然而，由于时间限制和文档技能水平不一，开发人员往往忽视有效提交信息的创建。我们的研究提出了一种使用机器学习（ML）和自然语言处理（NLP）自动生成提交信息的方法，通过开发使用逻辑回归与TF-IDF和Word2Vec等技术以及更复杂的LSTM方法的模型。我们使用了Liu等人使用的代码变更和相应提交信息的数据集，以此来训练和评估ML/NLP模型，并选择该数据集的原因是它在以往研究中的广泛使用，便于我们研究的可比性。我们的目标是探索哪些ML/NLP技术生成最有效、清晰和简明的提交信息，准确反映代码变更。我们将数据集拆分为训练集、验证集和测试集，并使用这些集评估每个模型的性能，采用定性和定量的评估方法。我们的结果揭示了这些模型之间的有效性差异，最高的BLEU得分为16.82，展示了模型在自动生成清晰、简洁的提交信息方面的能力。我们的论文提供了关于不同机器学习模型在软件开发中实现提交信息自动生成的比较有效性的见解，旨在增强代码文档的整体实践。

**Summary**:

- (1): 本文研究背景为提交信息在软件开发中对文档化代码变更的重要性，尤其是在大型软件项目和开源项目中。

- (2): 过去的方法包括基于信息检索和LSTM的方法，如NMT、NNGen、CoDiSum等。这些方法的问题在于计算需求高、数据限制和可扩展性问题。提出的方法选择传统机器学习模型，如逻辑回归和TF-IDF，与现有方法相比，侧重在效率和轻量化操作上，从而改善了生成提交信息的一致性和质量，对应对上述问题有良好的动机。

- (3): 本文的贡献在于提出了一种高效的自动生成提交信息的方法，探索了不同ML/NLP模型的有效性，并提供了有助于软件文档实践的见解。

- (4): 本文提出的研究方法包括使用ML/NLP模型训练，数据预处理与模型架构的合理选择，采用逻辑回归、TF-IDF和Word2Vec等技术进行实验。

- (5): 本文的方法在提交信息的生成任务中取得了最高16.82的BLEU得分，证明其性能达到了预期目标，能够有效支持其提出的自动化生成目标。


## SemML: Enhancing Automata-Theoretic LTL Synthesis with Machine Learning
- **Url**: http://arxiv.org/abs/2501.17496v2
- **Authors**: ['Jan Kretinsky', 'Tobias Meggendorfer', 'Maximilian Prokop', 'Ashkan Zarkhah']
- **Abstrat**: Synthesizing a reactive system from specifications given in linear temporal logic (LTL) is a classical problem, finding its applications in safety-critical systems design. We present our tool SemML, which won this year's LTL realizability tracks of SYNTCOMP, after years of domination by Strix. While both tools are based on the automata-theoretic approach, ours relies heavily on (i) Semantic labelling, additional information of logical nature, coming from recent LTL-to-automata translations and decorating the resulting parity game, and (ii) Machine Learning approaches turning this information into a guidance oracle for on-the-fly exploration of the parity game (whence the name SemML). Our tool fills the missing gaps of previous suggestions to use such an oracle and provides an efficeint implementation with additional algorithmic improvements. We evaluate SemML both on the entire set of SYNTCOMP as well as a synthetic data set, compare it to Strix, and analyze the advantages and limitations. As SemML solves more instances on SYNTCOMP and does so significantly faster on larger instances, this demonstrates for the first time that machine-learning-aided approaches can out-perform state-of-the-art tools in real LTL synthesis.


**Translated Abstract**: 

从线性时间逻辑（LTL）给定的规范中合成反应系统是一个经典问题，广泛应用于安全关键系统设计。我们提出了我们的工具 SemML，该工具在本年度的 SYNTCOMP LTL 可实现性比赛中获胜，结束了 Strix 多年的统治。虽然这两个工具都基于自动机理论方法，但我们的工具在很大程度上依赖于 (i) 语义标记，它是一种来自于最近的 LTL 到自动机转换的逻辑性质的附加信息，并装饰生成的奇偶博弈，以及 (ii) 将这些信息转化为指导 oracle 的机器学习方法，以支持奇偶博弈的即时探索（故名 SemML）。我们的工具填补了前人关于使用此类 oracle 的建议中的空白，并提供了高效的实现及额外的算法改进。我们在整个 SYNTCOMP 数据集和一组合成数据集上评估 SemML，并将其与 Strix 进行比较，分析其优缺点。SemML 在 SYNTCOMP 上解决了更多实例，并且在更大的实例上显著更快，这首次证明了机器学习辅助的方法可以在实际 LTL 合成中超越最先进的工具。

**Summary**:

- (1): 本文研究基于线性时间逻辑（LTL）的反应系统合成问题，该问题在安全关键系统设计中具有重要应用;

- (2): 过去的方法主要依赖于基于自动机的合成工具，如 Strix，存在效率低下和无法充分利用语义标记的问题。本文提出的方法结合了语义标记和机器学习，利用这些信息来生成即时探索的指导 oracle，有效解决了前述问题，并提出了合理的动机;

- (3): 本文的贡献在于开发了工具 SemML，该工具在 SYNTCOMP 竞赛中获胜，并在合成 LTL 系统方面表现优于现有工具，如 Strix;

- (4): 本文采用了结合语义标记和机器学习的研究方法，通过机器学习方法指导奇偶博弈的即时探索，并在合成流程中整合这些策略;

- (5): 在 SYNTCOMP 竞赛和合成数据集上进行评估，SemML 显示出更快的解决实例的能力，尤其是在大型实例中，性能优越可以支持其目标。


## Safe Physics-Informed Machine Learning for Dynamics and Control
- **Url**: http://arxiv.org/abs/2504.12952v1
- **Authors**: ['Jan Drgona', 'Truong X. Nghiem', 'Thomas Beckers', 'Mahyar Fazlyab', 'Enrique Mallada', 'Colin Jones', 'Draguna Vrabie', 'Steven L. Brunton', 'Rolf Findeisen']
- **Abstrat**: This tutorial paper focuses on safe physics-informed machine learning in the context of dynamics and control, providing a comprehensive overview of how to integrate physical models and safety guarantees. As machine learning techniques enhance the modeling and control of complex dynamical systems, ensuring safety and stability remains a critical challenge, especially in safety-critical applications like autonomous vehicles, robotics, medical decision-making, and energy systems. We explore various approaches for embedding and ensuring safety constraints, such as structural priors, Lyapunov functions, Control Barrier Functions, predictive control, projections, and robust optimization techniques, ensuring that the learned models respect stability and safety criteria. Additionally, we delve into methods for uncertainty quantification and safety verification, including reachability analysis and neural network verification tools, which help validate that control policies remain within safe operating bounds even in uncertain environments. The paper includes illustrative examples demonstrating the implementation aspects of safe learning frameworks that combine the strengths of data-driven approaches with the rigor of physical principles, offering a path toward the safe control of complex dynamical systems.


**Translated Abstract**: 

本教程论文专注于安全的物理信息机器学习在动力学和控制中的应用，提供了如何整合物理模型和安全保证的全面概述。随着机器学习技术增强对复杂动态系统的建模和控制，确保安全性和稳定性仍然是一个关键挑战，尤其是在自动驾驶汽车、机器人、医疗决策和能源系统等安全关键应用中。我们探讨了嵌入和确保安全约束的各种方法，如结构先验、Lyapunov 函数、控制障碍函数、预测控制、投影和鲁棒优化技术，以确保所学模型遵循稳定性和安全性标准。此外，我们深入探讨了不确定性量化和安全验证的方法，包括可达性分析和神经网络验证工具，这些方法帮助验证控制政策在不确定环境中仍然保持在安全操作范围内。本文包含了示例，展示了安全学习框架的实施方面，这些框架结合了数据驱动方法的优点与物理原理的严格性，为复杂动态系统的安全控制提供了一条途径。

**Summary**:

- (1): 本文的研究背景是机器学习技术在动力学和控制中的应用，特别是在安全关键领域（如自动驾驶、机器人等）中保障安全性和稳定性的挑战。

- (2): 过去的方法主要关注数据驱动的机器学习模型，但缺乏保证物理一致性和安全性的能力。这种研究方法与现有方法不同，因为它集成了物理原理和控制理论，以实现安全的物理信息机器学习（PIML），有效解决了传统方法的安全性问题，动机明确。

- (3): 本文的贡献在于全面展示安全 PIML 的方法论，并提供了集成物理模型与安全保证的实用框架，促进了动态系统的安全控制。

- (4): 本文提出的研究方法包括多种安全学习框架，重点在于稳定性保证、不确定性量化以及安全验证方法，如可达性分析和鲁棒优化技术。

- (5): 本文的方法在复杂动态系统的控制任务中获得了良好的效果，表现出在不确定环境中符合安全控制要求的能力，这支持了其研究目标。


## Which Optimizer Works Best for Physics-Informed Neural Networks and Kolmogorov-Arnold Networks?
- **Url**: http://arxiv.org/abs/2501.16371v3
- **Authors**: ['Elham Kiyani', 'Khemraj Shukla', 'Jorge F. Urbán', 'Jérôme Darbon', 'George Em Karniadakis']
- **Abstrat**: Physics-Informed Neural Networks (PINNs) have revolutionized the computation of PDE solutions by integrating partial differential equations (PDEs) into the neural network's training process as soft constraints, becoming an important component of the scientific machine learning (SciML) ecosystem. More recently, physics-informed Kolmogorv-Arnold networks (PIKANs) have also shown to be effective and comparable in accuracy with PINNs. In their current implementation, both PINNs and PIKANs are mainly optimized using first-order methods like Adam, as well as quasi-Newton methods such as BFGS and its low-memory variant, L-BFGS. However, these optimizers often struggle with highly non-linear and non-convex loss landscapes, leading to challenges such as slow convergence, local minima entrapment, and (non)degenerate saddle points. In this study, we investigate the performance of Self-Scaled BFGS (SSBFGS), Self-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton schemes, including BFGS and L-BFGS with different line search strategies approaches. These methods dynamically rescale updates based on historical gradient information, thus enhancing training efficiency and accuracy. We systematically compare these optimizers -- using both PINNs and PIKANs -- on key challenging linear, stiff, multi-scale and non-linear PDEs, including the Burgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations. Our findings provide state-of-the-art results with orders-of-magnitude accuracy improvements without the use of adaptive weights or any other enhancements typically employed in PINNs. More broadly, our results reveal insights into the effectiveness of second-order optimization strategies in significantly improving the convergence and accurate generalization of PINNs and PIKANs.


**Translated Abstract**: 

物理驱动神经网络（PINNs）通过将偏微分方程（PDEs）作为软约束整合到神经网络的训练过程中，革新了偏微分方程解的计算，成为科学机器学习（SciML）生态系统的重要组成部分。最近，物理驱动Kolmogorov-Arnold网络（PIKANs）在准确性上也显示出与PINNs的有效性和可比性。在目前的实现中，PINNs和PIKANs主要使用类似Adam的第一阶方法以及诸如BFGS及其低内存变种L-BFGS等拟牛顿方法进行优化。然而，这些优化器往往在高度非线性和非凸的损失景观中面临挑战，导致缓慢收敛、局部极值陷阱和非（退化）鞍点等问题。在本研究中，我们调查了自缩放BFGS（SSBFGS）、自缩放Broyden（SSBroyden）方法及其他先进的拟牛顿方案，包括具有不同线搜索策略的BFGS和L-BFGS。这些方法根据历史梯度信息动态缩放更新，从而提高训练效率和准确性。我们系统地比较了这些优化器——使用PINNs和PIKANs——在关键的线性、刚性、多尺度和非线性PDE上，包括Burgers、Allen-Cahn、Kuramoto-Sivashinsky和Ginzburg-Landau方程。我们的研究结果提供了最新的结果，在没有采用自适应权重或PINNs中通常使用的任何其他增强的情况下，准确性提高了几个数量级。从更广泛的角度看，我们的结果揭示了二阶优化策略在显著改善PINNs和PIKANs的收敛性和准确泛化能力中的有效性。

**Summary**:

- (1): 本文的研究背景是物理驱动神经网络（PINNs）和物理驱动Kolmogorov-Arnold网络（PIKANs）用于解决偏微分方程（PDEs）的算例方法及其面临的挑战。

- (2): 过去的方法主要使用类似Adam的第一阶方法和拟牛顿方法（如BFGS和L-BFGS）。这些方法在高度非线性和非凸的损失景观中表现不佳，存在慢收敛、局部极小值陷阱等问题。提出的方法采用自缩放BFGS（SSBFGS）和自缩放Broyden（SSBroyden）等二阶优化策略，通过动态缩放更新来解决这些问题，具有较强的动机。

- (3): 本文贡献在于提供了先进的优化器，显著提高了PINNs和PIKANs的收敛性和准确性，并无须依赖自适应权重或其他常见的增强技术。

- (4): 本文的研究方法涉及对不同优化算法的系统比较，特别是基于SSBFGS和SSBroyden的动态更新策略，并在多个挑战性PDE（如Burgers、Allen-Cahn等）上进行测试。

- (5): 在处理多种线性、刚性、多尺度和非线性PDE任务时，所提方法在准确性上实现了质的飞跃，支持其研究目标。


## Sliced-Wasserstein Distance-based Data Selection
- **Url**: http://arxiv.org/abs/2504.12918v1
- **Authors**: ['Julien Pallage', 'Antoine Lesage-Landry']
- **Abstrat**: We propose a new unsupervised anomaly detection method based on the sliced-Wasserstein distance for training data selection in machine learning approaches. Our filtering technique is interesting for decision-making pipelines deploying machine learning models in critical sectors, e.g., power systems, as it offers a conservative data selection and an optimal transport interpretation. To ensure the scalability of our method, we provide two efficient approximations. The first approximation processes reduced-cardinality representations of the datasets concurrently. The second makes use of a computationally light Euclidian distance approximation. Additionally, we open the first dataset showcasing localized critical peak rebate demand response in a northern climate. We present the filtering patterns of our method on synthetic datasets and numerically benchmark our method for training data selection. Finally, we employ our method as part of a first forecasting benchmark for our open-source dataset.


**Translated Abstract**: 

我们提出了一种基于切片-瓦瑟斯坦距离的新型无监督异常检测方法，用于机器学习中的训练数据选择。我们的过滤技术对在关键领域（如电力系统）中部署机器学习模型的决策流程具有重要意义，因为它提供了一种保守的数据选择和最优传输的解释。为了确保我们方法的可扩展性，我们提供了两种高效的近似方法。第一种近似方法并行处理数据集的降维表示。第二种方法利用计算量轻的欧几里得距离近似。此外，我们首次发布了一个数据集，展示了北方气候中的局部关键峰回扣需求响应。我们展示了我们的过滤方法在合成数据集上的过滤模式，并对我们的训练数据选择方法进行了数值基准测试。最后，我们将我们的方法作为预测基准的一部分应用于我们的开源数据集中。

**Summary**:

- (1): 本文的研究背景为虚拟电力厂（Virtual Power Plants, VPPs）利用分布式能源资源（DERs）优化电网服务的挑战，其中数据质量对机器学习模型表现至关重要。

- (2): 过去的方法包括局部离群因子（LOF）、孤立森林（Isolation Forest）等无监督异常检测方法，这些方法在处理数据时往往依赖于人造标签，复杂度较高，且可能会影响模型的可解释性和稳定性。提出的方法采用切片-瓦瑟斯坦（SW）距离，通过给出最优传输的直观诠释来克服这些问题，具有更强的理论动机。

- (3): 本文的贡献在于提出一种新的无监督异常过滤方法，并发布针对北方气候的关键峰回扣（Critical Peak Rebates, CPR）机制的开源数据集，以促进数据驱动的可信赖机器学习模型研究。

- (4): 本文的研究方法是基于切片-瓦瑟斯坦距离的异常检测方法，该方法提供了有效的近似策略，分别通过处理降维的表示和轻量级的欧几里得距离来提高计算效率。

- (5): 本文的方法在合成数据集上进行了数值基准测试，显示出较好的数据选择性能，并且在开源数据集的预测基准中表现出色，支持其促进可靠预测的目标。


## Can Neural Networks Bridge the Gap Between Lagrangian Mesh-Free Methods and High-Order Interpolants?
- **Url**: http://arxiv.org/abs/2503.23230v2
- **Authors**: ['Lucas Gerken Starepravo', 'Georgios Fourtakas', 'Steven Lind', 'Ajay Harish', 'Jack R. C. King']
- **Abstrat**: Mesh-free numerical methods offer flexibility in discretising complex geometries, showing potential where mesh-based methods struggle. While high-order approximations can be obtained via consistency correction using linear systems, they remain prohibitively expensive in Lagrangian formulations, which often exhibit low-order convergence. Here, we explore the use of machine learning (ML) to bridge the gap between mesh-free Lagrangian simulations and high-order approximations. We develop strategies to couple data-driven models, in particular multilayer perceptrons and residual MLPs with the Local Anisotropic Basis Function Method (LABFM), as an exemplar high-order mesh-free method. In the first strategy, we use neural networks to surrogate the high-order kernel; in the second, we develop surrogate models for computing the solutions of dense, low-rank linear systems present in high-order mesh-free methods. Results from networks aimed at predicting support nodal weights yield a qualitative match with validation data, but fall short in eliminating lower-order errors due to inaccuracies in the ML-computed weights, and thus leading to divergent behaviour. Regarding the second strategy, the ML-computed solution vector generates residuals with mean absolute errors of $\mathcal{O}(10^{-5})$. However, convergence studies reveal this level of accuracy to be insufficient, causing derivative operators to diverge at a lower resolution and achieve a lower accuracy than LABFM theoretically allows. Furthermore, there is marginal computational gain when computing the solution vector with neural networks compared to LU factorisation. These findings indicate that insufficient accuracy challenges both using neural networks as surrogates for high-order kernels and solve ill-conditioned linear systems, while the additional high computational cost systems further limits the latter's practicality.


**Translated Abstract**: 

无网格数值方法在离散复杂几何方面提供了灵活性，并在网格方法难以解决的问题中显示出潜力。尽管可以通过求解线性系统获得一致性修正的高阶近似，但在拉格朗日公式中，涉及的成本仍然令人难以承受，而这类方法通常表现出低阶收敛。在这里，我们探索了机器学习(ML)在填补无网格拉格朗日模拟与高阶近似之间的差距中所能发挥的作用。我们开发了将数据驱动模型（特别是多层感知器和残差MLPs）与局部各向异性基函数方法（Local Anisotropic Basis Function Method, LABFM）相结合的策略，作为一种高阶无网格方法的示例。第一个策略中，我们使用神经网络替代高阶核；第二个策略则开发了用于计算高阶无网格方法中存在的稠密低秩线性系统解的替代模型。旨在预测支持节点权重的网络结果与验证数据量化匹配，但由于ML计算的权重不准确，未能消除低阶误差，从而导致发散行为。在第二个策略中，使用机器学习计算的解向量产生平均绝对误差为$\mathcal{O}(10^{-5})$的残差。然而，收敛性研究显示这种准确度水平不足，导致导数算子在较低分辨率下发散，且达不到LABFM理论上允许的较低准确性。此外，在计算解向量时，与LU分解相比，使用神经网络的计算收益微薄。这些发现表明，使用神经网络作为高阶核替代和解决病态线性系统时的准确性不足，以及额外的高计算成本，进一步限制了后者的实用性。

**Summary**:

- (1): 本文研究的背景是无网格数值方法在处理复杂几何体时的有效性，并关注拉格朗日方法通常表现出的低阶收敛性及其高阶近似的高计算成本。

- (2): 过去的方法包括通过求解线性系统获取一致性修正的高阶近似，这种方法在拉格朗日公式中代价高昂，并且普遍存在低阶收敛的问题。本文提出的方法通过结合机器学习和LABFM来改善这一现状，尝试使用神经网络替代高阶核，以及对低秩线性系统的求解，从而提高效率和准确性。该方法具有良好的动机，意在弥补传统方法的不足。

- (3): 本文的贡献在于开发了结合机器学习的无网格拉格朗日方法与高阶近似之间的桥梁，并提出了新的策略来处理高阶核和稠密线性系统的计算。

- (4): 本文提出的研究方法包括两种策略：一是用神经网络做高阶核的替代，二是用神经网络解决高阶无网格方法中的低秩线性系统。

- (5): 本文的方法在预测支持节点权重方面显示出与验证数据的量化匹配，但由于准确性不足，导致响应行为的发散。在解决线性系统时，使用机器学习所得到的解具有微薄的计算收益。整体性能虽然有所提升，但仍未能支持其目标的实现。


## DrivAerML: High-Fidelity Computational Fluid Dynamics Dataset for Road-Car External Aerodynamics
- **Url**: http://arxiv.org/abs/2408.11969v2
- **Authors**: ['Neil Ashton', 'Charles Mockett', 'Marian Fuchs', 'Louis Fliessbach', 'Hendrik Hetmann', 'Thilo Knacke', 'Norbert Schonwald', 'Vangelis Skaperdas', 'Grigoris Fotiadis', 'Astrid Walle', 'Burkhard Hupertz', 'Danielle Maddix']
- **Abstrat**: Machine Learning (ML) has the potential to revolutionise the field of automotive aerodynamics, enabling split-second flow predictions early in the design process. However, the lack of open-source training data for realistic road cars, using high-fidelity CFD methods, represents a barrier to their development. To address this, a high-fidelity open-source (CC-BY-SA) public dataset for automotive aerodynamics has been generated, based on 500 parametrically morphed variants of the widely-used DrivAer notchback generic vehicle. Mesh generation and scale-resolving CFD was executed using consistent and validated automatic workflows representative of the industrial state-of-the-art. Geometries and rich aerodynamic data are published in open-source formats. To our knowledge, this is the first large, public-domain dataset for complex automotive configurations generated using high-fidelity CFD.


**Translated Abstract**: 

机器学习（ML）有潜力彻底改变汽车空气动力学领域，使早期设计过程中的流动预测能够实现瞬时化。然而，缺乏基于高保真CFD方法的现实道路汽车的开源训练数据，构成了其发展的障碍。为了解决这一问题，基于500个参数化变型的广泛使用的DrivAer轿车的高保真开源（CC-BY-SA）公共数据集已被生成。网格生成和尺度解析CFD是使用代表工业最新水平的一致和经过验证的自动工作流程执行的。几何形状和丰富的空气动力学数据以开源格式发布。据我们所知，这是第一个使用高保真CFD生成的复杂汽车配置的大型公共领域数据集。

**Summary**:

- (1): 本文的研究背景是机器学习在汽车空气动力学中的潜力工具，特别是其能够快速进行流动预测以改善设计过程，但高保真CFD数据的缺乏限制了这一工具的应用。

- (2): 过去的方法主要依赖于风洞实验和传统的CFD模型，例如RANS方法，其成本高且无法精准捕捉复杂气流的特征。提出的方法是生成一个基于500个变型的高保真开源CFD数据集，旨在解决数据不足的问题。这种方法提供了丰富的几何和气动性能数据，有助于机器学习模型的训练，有效克服了以往方法的局限性。

- (3): 本文的贡献在于创建了一个高保真开源数据集，首次提供了用于汽车空气动力学的复杂配置的公共领域数据，这是对工业界的巨大补充。

- (4): 本文采用的研究方法是基于多个DrivAer车辆变型生成高保真的CFD数据，通过先进的网格生成和尺度解析CFD技术，利用一致的自动工作流程进行数据收集和处理。

- (5): 本文中的方法在空气动力学性能预测任务上取得了显著成绩，提供了高质量的训练数据，该数据集为机器学习模型在汽车空气动力学的应用和优化提供了支持，达到研究目标。


## Protecting Confidentiality, Privacy and Integrity in Collaborative Learning
- **Url**: http://arxiv.org/abs/2412.08534v2
- **Authors**: ['Dong Chen', 'Alice Dethise', 'Istemi Ekin Akkus', 'Ivica Rimac', 'Klaus Satzke', 'Antti Koskela', 'Marco Canini', 'Wei Wang', 'Ruichuan Chen']
- **Abstrat**: A collaboration between dataset owners and model owners is needed to facilitate effective machine learning (ML) training. During this collaboration, however, dataset owners and model owners want to protect the confidentiality of their respective assets (i.e., datasets, models and training code), with the dataset owners also caring about the privacy of individual users whose data is in their datasets. Existing solutions either provide limited confidentiality for models and training code, or suffer from privacy issues due to collusion.   We present Citadel++, a collaborative ML training system designed to simultaneously protect the confidentiality of datasets, models and training code as well as the privacy of individual users. Citadel++ enhances differential privacy mechanisms to safeguard the privacy of individual user data while maintaining model utility. By employing Virtual Machine-level Trusted Execution Environments (TEEs) as well as the improved sandboxing and integrity mechanisms through OS-level techniques, Citadel++ effectively preserves the confidentiality of datasets, models and training code, and enforces our privacy mechanisms even when the models and training code have been maliciously designed. Our experiments show that Citadel++ provides model utility and performance while adhering to the confidentiality and privacy requirements of dataset owners and model owners, outperforming the state-of-the-art privacy-preserving training systems by up to 543x on CPU and 113x on GPU TEEs.


**Translated Abstract**: 

在协作式机器学习（ML）训练中，数据集所有者和模型所有者之间的合作是必要的。然而，在这种合作过程中，数据集所有者和模型所有者希望保护各自资产（即数据集、模型和训练代码）的机密性，同时数据集所有者还关心其数据集中用户的隐私。现有解决方案要么对模型和训练代码提供有限的机密性，要么由于串通而存在隐私问题。我们提出了 Citadel++，这是一种协作式 ML 训练系统，旨在同时保护数据集、模型和训练代码的机密性以及个人用户的隐私。Citadel++ 增强了差分隐私机制，以保护个别用户数据的隐私，同时保持模型效用。通过采用虚拟机级可信执行环境（TEEs）以及通过操作系统级技术改进沙箱和完整性机制，Citadel++ 有效地保护了数据集、模型和训练代码的机密性，并在模型和训练代码被恶意设计时强制执行我们的隐私机制。我们的实验表明，Citadel++ 在遵循数据集所有者和模型所有者的机密性和隐私要求的同时，提供了模型效用和性能，相比最先进的隐私保护训练系统在 CPU 上提高了 543 倍，在 GPU TEEs 上提高了 113 倍。

**Summary**:

- (1): 本文研究的背景是在协作式机器学习中，数据集所有者和模型所有者需要在保护各自资产机密性的同时进行有效的合作。

- (2): 过去的方法主要包括联邦学习（Federated Learning）和基于加密的协作训练等方案，这些方法存在模型和数据泄露的风险及计算和通信开销大的问题。与这些方法不同，提出的 Citadel++ 通过采用 TEEs 和增强的差分隐私机制解决了机密性和隐私问题，且不需要对训练代码进行先前检查，从而提高了灵活性。

- (3): 论文的贡献在于提出了 Citadel++，一个同时保护数据集、模型和训练代码的机密性以及用户隐私的高效协作 ML 训练系统。

- (4): 本文提出的研究方法结合了基于 TEEs 的隐私保护机制，通过将训练代码拆分为数据处理代码和模型更新代码，利用动态梯度裁剪、噪声修正等技术建立隐私屏障，有效确保梯度计算的差分隐私特性。

- (5): Citadel++ 在不同的协作学习任务上表现良好，实验结果显示在 CPU 上的性能提高达到 543 倍，GPU TEEs 上达到 113 倍，支持其保护机密性和用户隐私的目标。


## Evaluation of Active Feature Acquisition Methods for Time-varying Feature Settings
- **Url**: http://arxiv.org/abs/2312.01530v4
- **Authors**: ['Henrik von Kleist', 'Alireza Zamanian', 'Ilya Shpitser', 'Narges Ahmidi']
- **Abstrat**: Machine learning methods often assume that input features are available at no cost. However, in domains like healthcare, where acquiring features could be expensive or harmful, it is necessary to balance a feature's acquisition cost against its predictive value. The task of training an AI agent to decide which features to acquire is called active feature acquisition (AFA). By deploying an AFA agent, we effectively alter the acquisition strategy and trigger a distribution shift. To safely deploy AFA agents under this distribution shift, we present the problem of active feature acquisition performance evaluation (AFAPE). We examine AFAPE under i) a no direct effect (NDE) assumption, stating that acquisitions do not affect the underlying feature values; and ii) a no unobserved confounding (NUC) assumption, stating that retrospective feature acquisition decisions were only based on observed features. We show that one can apply missing data methods under the NDE assumption and offline reinforcement learning under the NUC assumption. When NUC and NDE hold, we propose a novel semi-offline reinforcement learning framework. This framework requires a weaker positivity assumption and introduces three new estimators: A direct method (DM), an inverse probability weighting (IPW), and a double reinforcement learning (DRL) estimator.


**Translated Abstract**: 

机器学习方法通常假定输入特征是在没有成本的情况下可用。然而，在医疗保健等领域，获取特征可能会非常昂贵或有害，因此有必要在特征的获取成本与其预测价值之间取得平衡。训练一个 AI 代理来决定获取哪些特征的任务被称为主动特征获取（AFA）。通过部署 AFA 代理，我们有效地改变了获取策略并引发了分布转移。为了在这种分布转移下安全地部署 AFA 代理，我们提出了主动特征获取性能评估（AFAPE）的问题。我们在 i) 无直接效应（NDE）假设下进行 AFAPE 研究，该假设表明获取不会影响基础特征值；以及 ii) 无未观察混杂（NUC）假设下，该假设表明回顾性特征获取决策仅基于观察到的特征。我们展示了在 NDE 假设下可以使用缺失数据方法，而在 NUC 假设下可以使用离线强化学习。当 NUC 和 NDE 成立时，我们提出了一种新颖的半离线强化学习框架。该框架需要更弱的正性假设，并引入了三种新估计器：直接方法（DM）、逆概率加权（IPW）和双重强化学习（DRL）估计器。

**Summary**:

- (1): 本文的研究背景是机器学习方法通常假定特征获取没有成本，而在医疗等领域，获取某些特征可能会带来高昂的费用或潜在的伤害。

- (2): 过去的方法如缺失数据方法和离线强化学习存在假设限制问题，可能无法有效处理特征获取时的分布转移。与现有方法不同，本文提出的框架包含更弱的正性假设，并通过引入新估计器（DM、IPW、DRL）来解决上述问题，从而增强了主动特征获取的评估能力。这种方法确实是有充分动机的。

- (3): 本文的贡献在于提出一种新的半离线强化学习框架，以评估主动特征获取性能，在特征获取决策与历史数据收集方式之间的分布转移中提供有效的解决方案。

- (4): 本文所提出的研究方法是基于 NDE 和 NUC 假设的半离线强化学习框架，结合了传统缺失数据方法与强化学习机制进行主动特征获取性能评估。

- (5): 本文通过在医疗诊断情景中的主动特征获取任务验证了方法的有效性，虽然具体的性能指标未详细列出，但所提出的方法足以支持其在主动特征获取中的目标。


## iHHO-SMOTe: A Cleansed Approach for Handling Outliers and Reducing Noise to Improve Imbalanced Data Classification
- **Url**: http://arxiv.org/abs/2504.12850v1
- **Authors**: ['Khaled SH. Raslan', 'Almohammady S. Alsharkawy', 'K. R. Raslan']
- **Abstrat**: Classifying imbalanced datasets remains a significant challenge in machine learning, particularly with big data where instances are unevenly distributed among classes, leading to class imbalance issues that impact classifier performance. While Synthetic Minority Over-sampling Technique (SMOTE) addresses this challenge by generating new instances for the under-represented minority class, it faces obstacles in the form of noise and outliers during the creation of new samples. In this paper, a proposed approach, iHHO-SMOTe, which addresses the limitations of SMOTE by first cleansing the data from noise points. This process involves employing feature selection using a random forest to identify the most valuable features, followed by applying the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm to detect outliers based on the selected features. The identified outliers from the minority classes are then removed, creating a refined dataset for subsequent oversampling using the hybrid approach called iHHO-SMOTe. The comprehensive experiments across diverse datasets demonstrate the exceptional performance of the proposed model, with an AUC score exceeding 0.99, a high G-means score of 0.99 highlighting its robustness, and an outstanding F1-score consistently exceeding 0.967. These findings collectively establish Cleansed iHHO-SMOTe as a formidable contender in addressing imbalanced datasets, focusing on noise reduction and outlier handling for improved classification models.


**Translated Abstract**: 
分类不平衡数据集仍然是机器学习中的一个重大挑战，尤其是在大数据环境下，不同类之间的实例分布不均，导致类别不平衡问题影响分类器性能。虽然合成少数类过采样技术（SMOTE）通过为代表性不足的少数类生成新实例来解决此问题，但在创建新样本时却面临噪声和离群点的障碍。本文提出了一种名为iHHO-SMOTe的方法，通过先清除噪声点来克服SMOTE的局限性。该过程包括使用随机森林进行特征选择，以识别最有价值的特征，然后应用基于密度的空间聚类算法（DBSCAN）检测基于所选特征的离群点。识别出的少数类离群点随后被移除，从而创建一个精炼的数据集，以便后续使用混合方法iHHO-SMOTe进行过采样。在多种数据集上的综合实验证明了该模型的卓越性能，AUC分数超过0.99，高G均值分数为0.99突显其稳健性，以及基于F1分数持续超过0.967的出色表现。这些发现共同确立了清理后的iHHO-SMOTe作为解决不平衡数据集的强大竞选者，专注于噪声减少和离群点处理，以改善分类模型。

**Summary**:

- (1): 本文的研究背景是随着机器学习在多个领域的广泛应用，尤其是在大数据环境下，准确分类不平衡数据集的挑战日益严重。

- (2): 过去的方法主要是合成少数类过采样技术（SMOTE），其主要问题在于在合成样本的过程中容易受到噪声和离群点的干扰。与现有方法的不同之处在于，本文提出的iHHO-SMOTe方法通过先进行数据清洗来去除噪声和离群点，解决了SMOTE在合成过程中存在的不足，并且具有充分的动机。

- (3): 本文的贡献在于提出了一种新的处理噪声和离群点的清洗方法，通过改进过采样策略，提高了不平衡数据集的分类模型性能。

- (4): 本文提出的研究方法包括两步过程：首先利用随机森林进行特征选择以识别重要特征，然后使用DBSCAN算法清除离群点，最后应用混合过采样方法iHHO-SMOTe。

- (5): 在多种数据集上的任务中，所提方法的AUC分数超过0.99，G均值分数为0.99，F1分数持续超过0.967，这些性能指标支持了他们提高不平衡数据集分类模型的目标。


## Explainable Scene Understanding with Qualitative Representations and Graph Neural Networks
- **Url**: http://arxiv.org/abs/2504.12817v1
- **Authors**: ['Nassim Belmecheri', 'Arnaud Gotlieb', 'Nadjib Lazaar', 'Helge Spieker']
- **Abstrat**: This paper investigates the integration of graph neural networks (GNNs) with Qualitative Explainable Graphs (QXGs) for scene understanding in automated driving. Scene understanding is the basis for any further reactive or proactive decision-making. Scene understanding and related reasoning is inherently an explanation task: why is another traffic participant doing something, what or who caused their actions? While previous work demonstrated QXGs' effectiveness using shallow machine learning models, these approaches were limited to analysing single relation chains between object pairs, disregarding the broader scene context. We propose a novel GNN architecture that processes entire graph structures to identify relevant objects in traffic scenes. We evaluate our method on the nuScenes dataset enriched with DriveLM's human-annotated relevance labels. Experimental results show that our GNN-based approach achieves superior performance compared to baseline methods. The model effectively handles the inherent class imbalance in relevant object identification tasks while considering the complete spatial-temporal relationships between all objects in the scene. Our work demonstrates the potential of combining qualitative representations with deep learning approaches for explainable scene understanding in autonomous driving systems.


**Translated Abstract**: 

本文探讨了将图神经网络（GNN）与定性可解释图（QXG）结合，以支持自动驾驶中的场景理解。场景理解是任何进一步反应或主动决策的基础。对场景的理解和相关推理本质上是一项解释任务：另一个交通参与者为什么会这样做，是什么或谁导致了他们的行为？虽然以前的工作使用浅层机器学习模型证明了QXG的有效性，但这些方法仅限于分析对象对之间的单一关系链，忽视了更广泛的场景上下文。我们提出了一种新颖的GNN架构，该架构处理整个图结构，以识别交通场景中的相关对象。我们在nuScenes数据集上评估了我们的方法，该数据集经过DriveLM的人类标注的相关性标签丰富。实验结果表明，我们基于GNN的方法与基线方法相比，表现优越。该模型有效应对相关对象识别任务中固有的类别不平衡，同时考虑场景中所有对象之间的完整时空关系。我们的工作展示了将定性表征与深度学习方法结合以实现自动驾驶系统中的可解释场景理解的潜力。

**Summary**:

- (1): 本文研究背景是自动驾驶中的场景理解，该理解是基于对环境的感知以做出知情决策。

- (2): 过去的方法主要依赖于浅层机器学习模型（如随机森林），其主要问题在于只能处理对象之间的单一关系链，无法反映多个对象之间的丰富上下文信息。本文提出的方法与现有方法的不同之处在于，采用了GNN，可处理完整的图结构，考虑所有对象关系，从而有效解决了上下文信息不足的问题。该方法的动机充足，旨在提升场景理解的解释性和准确性。

- (3): 本文的贡献包括提出新颖的GNN架构，以处理QXG，同时保持其可解释性；引入专业的训练方法以应对相关对象识别中的类别不平衡；通过在真实驾驶数据上进行全面实验，展示了比传统浅层学习方法更好的性能。

- (4): 本文提出的研究方法是结合QXG与GNN，通过GNN处理图结构数据，使模型在做出预测时考虑所有对象之间的关系。

- (5): 本文的方法在nuScenes数据集上进行评估，表现出优于基线方法的性能。实验结果表明，性能提升支持其在自动驾驶场景理解中的目标。


# AGN
## The RESOLVE and ECO G3 Initiative: Drivers of HI Content and X-ray Emission in Galaxy Groups
- **Url**: http://arxiv.org/abs/2504.13103v1
- **Authors**: ['Zackary L. Hutchens', 'Sheila J. Kannappan', 'Kelley M. Hess', 'Andrew J. Baker', 'Ming Sun', 'Derrick S. Carr', 'Kathleen D. Eckert', 'David V. Stark']
- **Abstrat**: Adding to the RESOLVE and ECO Gas in Galaxy Groups (G3) initiative, we examine possible drivers of group-integrated HI-to-halo mass ratios ($M_{\rm HI,grp}/M_{\rm halo}$) and group X-ray emission, including group halo mass ($M_{\rm halo}$), virialization as probed by crossing time ($t_{\rm cross}$), presence of active galactic nuclei (AGN), and group-integrated fractional stellar mass growth rate (FSMGR$_{\rm grp}$). G3 groups span $M_{\rm halo}=10^{11-14.5}\,M_\odot$ with comprehensive HI and AGN information, which we combine with X-ray stacking of ROSAT All-Sky data. We detect hot gas emission exceeding AGN and X-ray binary backgrounds confidently for $M_{\rm halo}=10^{12.6-14}\,M_\odot$ and unambiguously for $M_{\rm halo}>10^{14}\,M_\odot$, reflecting an inverse dependence of $M_{\rm\,HI,grp}/M_{\rm halo}$ and hot gas emission on halo mass. At fixed halo mass, $M_{\rm\,HI,grp}/M_{\rm halo}$ transitions to greater spread below $t_{\rm cross}\sim2$ Gyr. Dividing groups across this transition, lower-$t_{\rm cross}$ groups show elevated X-ray emission compared to higher-$t_{\rm cross}$ groups for $M_{\rm halo}>10^{13.3}\,M_\odot$, but this trend reverses for $M_{\rm halo}=10^{12.6-13.3}\,M_\odot$. Additionally, AGN-hosting halos below $M_{\rm halo}\sim10^{12.1}\,M_\odot$ exhibit a broad, $\sim$0.25 dex deep valley in $M_{\rm HI,grp}/M_{\rm halo}$ compared to non-AGN-hosting halos with correspondingly reduced FSMGR$_{\rm grp}$. When diluted by non-AGN-hosting halos, this valley becomes shallower and narrower, falling roughly between $M_{\rm halo}=10^{11.5}\,M_\odot$ and $M_{\rm halo}=10^{12.1}\,M_\odot$ in the overall $M_{\rm\,HI,grp}/M_{\rm\,halo}$ vs. $M_{\rm halo}$ relation. We may also detect a second, less easily interpreted valley at $M_{\rm halo}\sim10^{13}\,M_\odot$. Neither valley matches theoretical predictions of a deeper valley at or above $M_{\rm halo}=10^{12.1}\,M_\odot$.


**Translated Abstract**: 

在 RESOLVE 和 ECO 群体气体（G3）计划的基础上，我们研究群体综合 HI 与晕质量比（$M_{\rm HI,grp}/M_{\rm halo}$）和群体 X 射线发射的潜在驱动因素，包括群体晕质量（$M_{\rm halo}$）、通过过渡时间（$t_{\rm cross}$）探测的准稳态、活动星系核（AGN）的存在以及群体综合的恒星质量生长速率（FSMGR$_{\rm grp}$）。G3 群体的质量范围为 $M_{\rm halo}=10^{11-14.5}\,M_\odot$，我们结合了全面的 HI 和 AGN 信息以及 ROSAT 全天数据的 X 射线堆积。我们自信地发现，对于 $M_{\rm halo}=10^{12.6-14}\,M_\odot$ 超过 AGN 和 X 射线双星背景的热气体发射，并且在 $M_{\rm halo}>10^{14}\,M_\odot$ 时无疑反映出 $M_{\rm\,HI,grp}/M_{\rm halo}$ 和热气体发射与晕质量之间的反向依赖关系。在固定晕质量下，$M_{\rm\,HI,grp}/M_{\rm halo}$ 在 $t_{\rm cross}\sim2$ Gyr 之下过渡为更大的散布。将群体分为这两个过渡，较低$t_{\rm cross}$的群体相比于较高$t_{\rm cross}$群体，在 $M_{\rm halo}>10^{13.3}\,M_\odot$ 时显示出增加的 X 射线发射，但在 $M_{\rm halo}=10^{12.6-13.3}\,M_\odot$ 时这一趋势反转。此外，$M_{\rm halo}\sim10^{12.1}\,M_\odot$ 之下的 AGN 主持晕相比于非 AGN 主持晕展现出 $M_{\rm HI,grp}/M_{\rm halo}$ 中约 $0.25$ dex 深的谷，与相应的 FSMGR$_{\rm grp}$ 降低。随着非 AGN 主持晕的稀释，该谷在总体 $M_{\rm\,HI,grp}/M_{\rm\,halo}$ 与 $M_{\rm halo}$ 关系中变得更浅且更窄，落在大约 $M_{\rm halo}=10^{11.5}\,M_\odot$ 和 $M_{\rm halo}=10^{12.1}\,M_\odot$ 之间。我们可能还检测到在 $M_{\rm halo}\sim10^{13}\,M_\odot$ 的第二个较难解释的谷。两个谷都与理论预测的在 $M_{\rm halo}=10^{12.1}\,M_\odot$ 及以上的更深谷不符。

**Summary**:

- (1): 本文研究了 HI 物质在星系群中的重要性，并探索了多种因素对群体综合 HI 与晕质量比及 X 射线发射的影响。

- (2): 过去的研究方法主要关注 HI-晕质量关系，发现其存在显著的散布和斜率变化；然而，未能深入探讨晕的动态状态对 HI 内容的影响。本文提出的研究方法通过结合 X 射线检测与综合的 HI 数据来克服这些局限，能更全面地理解不同环境对 HI 内容的影响。

- (3): 本文的贡献在于深化了对 HI-晕质量关系的理解，特别是在群体的动态演化和 AGN 影响下，提出了不同于以往的观察结果。

- (4): 研究方法通过结合 ROSAT 数据进行 X 射线堆积，探讨了群体的晕质量、过渡时间与 AGN 之间的关系，分析了不同条件下群体 HI 内容及 X 射线发射的特征。

- (5): 本文在 HI-晕质量关系及其与 X 射线发射的交互影响上取得了新的成果，这些结果支持了分析群体演化及其环境影响的目标，并表明 AGN 在这一过程中的重要性。


## Analysis of Multi-epoch JWST Images of $\sim 300$ Little Red Dots: Tentative Detection of Variability in a Minority of Sources
- **Url**: http://arxiv.org/abs/2411.02729v2
- **Authors**: ['Zijian Zhang', 'Linhua Jiang', 'Weiyang Liu', 'Luis C. Ho']
- **Abstrat**: James Webb Space Telescope (JWST) has revealed a population of red and compact sources at $z \gtrsim 5$ known as ``Little Red Dots'' (LRDs) that are likely active galactic nuclei (AGNs). Here we present a comprehensive study of the variability of 314 LRDs with multi-epoch JWST observations in five deep fields: UDS, GOODS-S, GOODS-N, Abell 2744, and COSMOS. Our analyses use all publicly available JWST NIRCam imaging data in these fields, together with multi-epoch JWST MIRI images available. We measure the significance (signal-to-noise ratio or ${\rm SNR}_{\rm var}$) of the variabilities for all LRDs and statistically evaluate their variabilities using the ${\rm SNR}_{\rm var}$ distributions. We pay particular attention to the systematic offsets of photometric zero points among different epochs that seem to commonly exist. The derived ${\rm SNR}_{\rm var}$ distributions of the LRDs, including those with broad H$\alpha$/H$\beta$ emission lines, follow the standard Gaussian distribution, and are generally consistent with those of the comparison samples of objects detected in the same images. This finding suggests that the LRD population on average does not show strong variability, which can be explained by super-Eddington accretion of the black holes in AGNs. Alternatively, many of them may be dominated by galaxies. We also find eight strongly variable LRD candidates with variability amplitudes of 0.24 -- 0.82 mag. The rest-frame optical SEDs of these variable LRDs should have significant AGN contribution. Future JWST observations will provide more variability information of LRDs.


**Translated Abstract**: 

这篇文章分析了James Webb太空望远镜（JWST）揭示的一类称为“小红点”（Little Red Dots, LRDs）的红色紧凑源，在红移z ≳ 5的情况下，可能是活动星系核（AGN）。我们对314个LRDs进行了多时序JWST观察的综合研究，覆盖五个深场：UDS、GOODS-S、GOODS-N、Abell 2744和COSMOS。我们的分析使用了这些领域中所有公开可用的JWST NIRCam成像数据，以及可获得的多时序JWST MIRI图像。我们测量了所有LRDs的变动显著性（信噪比或SNR_var），并通过SNR_var分布对它们的变动进行了统计评估。我们特别关注不同时间段之间的光度零点的系统性偏差。得出的LRDs的SNR_var分布，包括那些具有宽Hα/Hβ发射线的对象，遵循标准高斯分布，并且通常与在相同图像中检测到的比较样本一致。这一发现表明LRD群体在平均上并没有表现出强烈的变动，可能是由于黑洞超爱丁顿吸积引起的。或者，许多LRDs可能主要由星系主导。我们还发现了八个强变化的LRD候选者，变动幅度为0.24到0.82等。未来的JWST观察将提供更多LRDs的变动信息。

**Summary**:

- (1): 本文的研究背景是小红点（LRDs）作为红移z ≳ 5的可能活动星系核（AGN），在JWST的发现中显得尤为引人关注。

- (2): 以往的方法主要集中在通过静态光谱和光度分析LDRs的性质上，存在无法有效区分星系和AGN成分的问题。本文通过分析变动性，提出了一种新的方法，这能更加有效地揭示LRDs的真实身份。

- (3): 文章的贡献在于提供了314个LRDs的变动性分析，未发现所有对象的强烈变动性，强调了黑洞超爱丁顿吸积的可能性，并发现了八个明显的变动候选者。

- (4): 本文提出的研究方法主要是通过多时序JWST观察，测量LRDs的信噪比（SNR_var），并通过系统的统计评估这些变动性。

- (5): 本文在变动性任务上取得的初步成果是发现八个具有显著变动的LRD候选源，这支持了研究者希望揭示LRDs真实身份和演化历史的目标。


## A Galaxy with an Extremely Blue UV Slope $β=-3$ at $z=9.25$ Identified by JWST Spectroscopy: Evidence for a Weak Nebular Continuum and Efficient Ionizing Photon Escape?
- **Url**: http://arxiv.org/abs/2411.19893v3
- **Authors**: ['Hiroto Yanagisawa', 'Masami Ouchi', 'Kimihiko Nakajima', 'Yuichi Harikane', 'Seiji Fujimoto', 'Yoshiaki Ono', 'Hiroya Umeda', 'Minami Nakane', 'Hidenobu Yajima', 'Hajime Fukushima', 'Yi Xu']
- **Abstrat**: We investigate UV continuum slopes $\beta$ of 863 galaxies at $z=4-14$ using archival JWST/NIRSpec PRISM spectra obtained from major JWST GTO, ERS, and GO programs, including JADES, CEERS, and UNCOVER. Among these galaxies, we identify a remarkable galaxy at $z=9.25$, dubbed EBG-1, with a significantly blue UV slope $\beta=-2.99\pm0.15$, unlike the rest of the galaxies that exhibit red continua or ambiguous blue continua hindered by large uncertainties. We confirm that the $\beta$ value negligibly changes by the data reduction and fitting wavelength ranges for UV emission/absorption line masking. The extreme blue slope, $\beta=-3.0$, rules out significant contributions from dust extinction or AGN activity. Comparing with stellar and nebular emission models, we find that such a blue UV slope cannot be reproduced solely by stellar models even with very young, metal-poor, or top-heavy contiguous star formation associated with strong nebular continua making the UV slopes red, but with a high ionizing photon escape fraction, $f_\mathrm{esc}^\mathrm{ion} \gtrsim 0.5$, for a weak nebular continuum. While the H$\beta$ emission line is not detected, likely due to the limited sensitivity of the spectrum, we find moderately weak [O III] $\lambda\lambda$4959,5007 emission lines for the given star-formation rate ($3\, \mathrm{M_\odot}$ yr$^{-1}$) and stellar mass ($10^{8.0} \, \mathrm{M_\odot}$) that are about three times weaker than the average emission lines, again suggestive of the high ionizing photon escape fraction, $f_\mathrm{esc}^\mathrm{ion} \sim 0.7$ or more. EBG-1 would provide crucial insights into stellar and nebular continuum emission in high-redshift galaxies, serving as an example of the ionizing photon escaping site at the epoch of reionization.


**Translated Abstract**: 

我们使用从主要的JWST GTO、ERS和GO项目（包括JADES、CEERS和UNCOVER）获得的档案JWST/NIRSpec PRISM光谱，研究了在红移4到14之间的863个星系的紫外连续谱斜率β。我们在红移9.25识别出一颗显著的星系，命名为EBG-1，其紫外斜率为β=-2.99±0.15，这与其余的星系明显不同，后者显示出红色连续谱或由于较大不确定性而模糊的蓝色连续谱。我们确认，β值在数据处理和紫外发射/吸收线掩蔽的拟合波长范围内几乎没有变化。极端的蓝色斜率β=-3.0排除了灰尘灭绝或活跃星系核（AGN）活动的显著贡献。与恒星和气体辐射模型相比，我们发现，仅凭恒星模型无法重现如此蓝色的紫外斜率，即使是在非常年轻、低金属或以强气体辐射为特征的顶部重金属连续星形成的情况下，同时高电离光子逃逸率f_esc^ion≥0.5，也会使得气体辐射较弱。虽然未探测到Hβ发射线，但我们发现适合给定的星形成率（3 M⊙ yr^{-1}）和恒星质量（10^{8.0} M⊙）的[O III] λλ4959,5007发射线中等偏弱，约为平均发射线的三分之一，进一步暗示高电离光子逃逸率f_esc^ion∼0.7或更高。EBG-1将为高红移星系中的恒星和气体辐射发射提供重要见解，作为在再电离时代中电离光子逃逸地点的一个例子。

**Summary**:

- (1): 本文研究高红移宇宙中星系的紫外光谱斜率，探索星系如何发射和逃逸电离光子，驱动宇宙再电离的过程。

- (2): 过去的方法主要依赖于光度法来测量紫外斜率，但这受到发射线污染的影响。本文采用JWST/NIRSpec PRISM光谱提供的高质量数据，以解决这些局限性，能够更准确地测量紫外斜率。

- (3): 本文的贡献在于发现一个显著的高红移星系EBG-1，具有极低的紫外斜率β=-2.99±0.15，这为理解高红移星系的电离光子逃逸提供了关键洞察。

- (4): 本文采用比较恒星和气体辐射模型的方法，通过分析EBG-1的光谱特征，确定其高电离光子逃逸率和弱气体辐射的特性。

- (5): 通过对EBG-1的研究，本文成功展示其高电离光子逃逸率，达到f_esc^ion∼0.7或更高，这支持了其在再电离过程中的重要性。


# ALMA# blackhole
# M87
# Brightest Cluster Galaxy
# Brightest Cluster Galaxies
# machine learning
# AGN
## The RESOLVE and ECO G3 Initiative: Drivers of HI Content and X-ray Emission in Galaxy Groups
- **Url**: http://arxiv.org/abs/2504.13103v1
- **Authors**: ['Zackary L. Hutchens', 'Sheila J. Kannappan', 'Kelley M. Hess', 'Andrew J. Baker', 'Ming Sun', 'Derrick S. Carr', 'Kathleen D. Eckert', 'David V. Stark']
- **Abstrat**: Adding to the RESOLVE and ECO Gas in Galaxy Groups (G3) initiative, we examine possible drivers of group-integrated HI-to-halo mass ratios ($M_{\rm HI,grp}/M_{\rm halo}$) and group X-ray emission, including group halo mass ($M_{\rm halo}$), virialization as probed by crossing time ($t_{\rm cross}$), presence of active galactic nuclei (AGN), and group-integrated fractional stellar mass growth rate (FSMGR$_{\rm grp}$). G3 groups span $M_{\rm halo}=10^{11-14.5}\,M_\odot$ with comprehensive HI and AGN information, which we combine with X-ray stacking of ROSAT All-Sky data. We detect hot gas emission exceeding AGN and X-ray binary backgrounds confidently for $M_{\rm halo}=10^{12.6-14}\,M_\odot$ and unambiguously for $M_{\rm halo}>10^{14}\,M_\odot$, reflecting an inverse dependence of $M_{\rm\,HI,grp}/M_{\rm halo}$ and hot gas emission on halo mass. At fixed halo mass, $M_{\rm\,HI,grp}/M_{\rm halo}$ transitions to greater spread below $t_{\rm cross}\sim2$ Gyr. Dividing groups across this transition, lower-$t_{\rm cross}$ groups show elevated X-ray emission compared to higher-$t_{\rm cross}$ groups for $M_{\rm halo}>10^{13.3}\,M_\odot$, but this trend reverses for $M_{\rm halo}=10^{12.6-13.3}\,M_\odot$. Additionally, AGN-hosting halos below $M_{\rm halo}\sim10^{12.1}\,M_\odot$ exhibit a broad, $\sim$0.25 dex deep valley in $M_{\rm HI,grp}/M_{\rm halo}$ compared to non-AGN-hosting halos with correspondingly reduced FSMGR$_{\rm grp}$. When diluted by non-AGN-hosting halos, this valley becomes shallower and narrower, falling roughly between $M_{\rm halo}=10^{11.5}\,M_\odot$ and $M_{\rm halo}=10^{12.1}\,M_\odot$ in the overall $M_{\rm\,HI,grp}/M_{\rm\,halo}$ vs. $M_{\rm halo}$ relation. We may also detect a second, less easily interpreted valley at $M_{\rm halo}\sim10^{13}\,M_\odot$. Neither valley matches theoretical predictions of a deeper valley at or above $M_{\rm halo}=10^{12.1}\,M_\odot$.


**Translated Abstract**: 

本研究在“RESOLVE和ECO气体在星系团”（G3）倡议的基础上，探讨了可能影响星系团整合HI-与光晕质量比率（$M_{\rm HI,grp}/M_{\rm halo}$）和星系团X射线发射的因素，包括光晕质量（$M_{\rm halo}$）、通过交叉时间（$t_{\rm cross}$）探测的宇宙圈化状态、活动星系核（AGN）的存在，以及整合的恒星质量增长率（FSMGR$_{\rm grp}$）。G3星系团的光晕质量范围为$M_{\rm halo}=10^{11-14.5}\,M_\odot$，结合了全面的HI和AGN信息，并与ROSAT全天数据的X射线堆积相结合。我们在$M_{\rm halo}=10^{12.6-14}\,M_\odot$的范围内，自信地发现了超过AGN和X射线双星背景的热气体发射，在$M_{\rm halo}>10^{14}\,M_\odot$时无可置疑地实现，反映了$M_{\rm\,HI,grp}/M_{\rm halo}$和热气体发射与光晕质量的反比关系。在固定的光晕质量下，$M_{\rm\,HI,grp}/M_{\rm halo}$在$t_{\rm cross}\sim2$ Gyr下表现出更大的变化范围。将星系团按此转折点进行划分时，$t_{\rm cross}$较低的星系团在光晕质量大于$10^{13.3}\,M_\odot$时显示出较高的X射线发射，而在$M_{\rm halo}=10^{12.6-13.3}\,M_\odot$情况下则趋势相反。此外，光晕质量低于$M_{\rm halo}\sim10^{12.1}\,M_\odot$的AGN宿主光晕在$M_{\rm HI,grp}/M_{\rm halo}$中展现出一个约0.25 dex深的谷底，相比于非AGN宿主光晕，其对应的FSMGR$_{\rm grp}$有所降低。当被非AGN宿主光晕稀释时，该谷底变得更加平缓和狭窄，整体上位于$M_{\rm halo}=10^{11.5}\,M_\odot$至$M_{\rm halo}=10^{12.1}\,M_\odot$之间的$M_{\rm\,HI,grp}/M_{\rm\,halo}$与$M_{\rm halo}$的关系中。我们可能还在$M_{\rm halo}\sim10^{13}\,M_\odot$处检测到第二个难以解释的谷底。这些谷底均未符合理论预测的在$M_{\rm halo}=10^{12.1}\,M_\odot$及以上应该出现的更深谷底。

**Summary**:

- (1): 本文研究的背景是探讨中性原子氢（H I）在星系团中的含量及其如何受环境影响，主要关注H I与光晕质量的关系。

- (2): 过去的研究通常使用单一的星系团或小样本进行研究，未能全面分析热气体与H I气体的相互关系。这种方法对星系团的阶段和环境考量不足。提出的方法通过广泛包含H I、AGN信息，以及结合X射线堆积数据，解决了过去方法的问题，提供了更全面的星系团质量分析。

- (3): 论文的贡献在于揭示了H I与星系团热气体发射之间的反比关系，并阐明了交叉时间、AGN存在与H I内容之间的复杂关系。

- (4): 本文提出的研究方法是结合不同光晕质量的星系团，通过X射线数据分析与H I数据对比，考察不同星系团的物理特性及其相互影响。

- (5): 本文的方法揭示了H I与星系团X射线发射的相互作用，并在$M_{\rm halo}$范围内达成了一致的结果，支持了研究目标的实现。


## Analysis of Multi-epoch JWST Images of $\sim 300$ Little Red Dots: Tentative Detection of Variability in a Minority of Sources
- **Url**: http://arxiv.org/abs/2411.02729v2
- **Authors**: ['Zijian Zhang', 'Linhua Jiang', 'Weiyang Liu', 'Luis C. Ho']
- **Abstrat**: James Webb Space Telescope (JWST) has revealed a population of red and compact sources at $z \gtrsim 5$ known as ``Little Red Dots'' (LRDs) that are likely active galactic nuclei (AGNs). Here we present a comprehensive study of the variability of 314 LRDs with multi-epoch JWST observations in five deep fields: UDS, GOODS-S, GOODS-N, Abell 2744, and COSMOS. Our analyses use all publicly available JWST NIRCam imaging data in these fields, together with multi-epoch JWST MIRI images available. We measure the significance (signal-to-noise ratio or ${\rm SNR}_{\rm var}$) of the variabilities for all LRDs and statistically evaluate their variabilities using the ${\rm SNR}_{\rm var}$ distributions. We pay particular attention to the systematic offsets of photometric zero points among different epochs that seem to commonly exist. The derived ${\rm SNR}_{\rm var}$ distributions of the LRDs, including those with broad H$\alpha$/H$\beta$ emission lines, follow the standard Gaussian distribution, and are generally consistent with those of the comparison samples of objects detected in the same images. This finding suggests that the LRD population on average does not show strong variability, which can be explained by super-Eddington accretion of the black holes in AGNs. Alternatively, many of them may be dominated by galaxies. We also find eight strongly variable LRD candidates with variability amplitudes of 0.24 -- 0.82 mag. The rest-frame optical SEDs of these variable LRDs should have significant AGN contribution. Future JWST observations will provide more variability information of LRDs.


**Translated Abstract**: 

詹姆斯·韦布太空望远镜（JWST）揭示了一类位于红移 z ≳ 5 的红色紧凑源群，被称为“小红点”（LRDs），很可能是活动星系核（AGNs）。在这项研究中，我们呈现了对314个LRDs的变异性进行的全面研究，利用在五个深场（UDS、GOODS-S、GOODS-N、阿贝尔2744和COSMOS）的多时代JWST观测数据。我们的分析使用了这些领域中所有公开的JWST NIRCam成像数据，以及可用的多时代JWST MIRI图像。我们测量了所有LRDs变异性的显著性（信噪比或SNR_var），并通过SNR_var分布对其变异性进行了统计评估。我们特别关注不同观测时代之间普遍存在的光度零点系统偏移。LRDs的SNR_var分布，包括那些具有宽Hα/Hβ发射线的对象，遵循标准的高斯分布，并与在相同图像中检测的比较样本一致。这一发现表明，LRDs群体在平均上未表现出显著的变异性，可以通过黑洞的超爱丁顿吸积进行解释。或者，其中许多可能以星系为主。我们还发现了八个强变异的LRD候选者，其变异幅度为0.24到0.82 magnitudes。这些变量LRDs的视界光谱分布应有显著的AGN贡献。未来的JWST观测将提供更多LRDs变异性的信息。

**Summary**:

- (1): 本文研究的背景是JWST发现的红移 z ≳ 5 的“小红点”（LRDs），这些对象可能是活动星系核（AGNs），但其真正性质仍存在争议。

- (2): 过去的方法主要依赖于光谱学和静态光谱/光度分布对LRDs进行分析，但未能充分揭示其本质。所提出的方法通过多时代JWST观测分析变异性，克服了静态数据的局限，使得研究能够区分AGNs和星系的贡献，具有清晰的动机。

- (3): 本文的贡献在于综合分析了314个LRDs的变异性，证明大多数LRDs表现出不强的变异性，同时识别出少数强变异的LRD候选者，为理解LRDs的本质提供了新的视角。

- (4): 研究方法论包括利用JWST在多个时间点的图像数据，计算变异性显著性（SNR_var），并统计评估LRDs的变异性分布，特别关注系统光度偏差。

- (5): 本文针对LRDs的变异性表现进行了研究，获得了大多数源表现出不显著变异性，以及八个展示强变异的候选源。结果支持了对LRDs本质理解的目标。


## A Galaxy with an Extremely Blue UV Slope $β=-3$ at $z=9.25$ Identified by JWST Spectroscopy: Evidence for a Weak Nebular Continuum and Efficient Ionizing Photon Escape?
- **Url**: http://arxiv.org/abs/2411.19893v3
- **Authors**: ['Hiroto Yanagisawa', 'Masami Ouchi', 'Kimihiko Nakajima', 'Yuichi Harikane', 'Seiji Fujimoto', 'Yoshiaki Ono', 'Hiroya Umeda', 'Minami Nakane', 'Hidenobu Yajima', 'Hajime Fukushima', 'Yi Xu']
- **Abstrat**: We investigate UV continuum slopes $\beta$ of 863 galaxies at $z=4-14$ using archival JWST/NIRSpec PRISM spectra obtained from major JWST GTO, ERS, and GO programs, including JADES, CEERS, and UNCOVER. Among these galaxies, we identify a remarkable galaxy at $z=9.25$, dubbed EBG-1, with a significantly blue UV slope $\beta=-2.99\pm0.15$, unlike the rest of the galaxies that exhibit red continua or ambiguous blue continua hindered by large uncertainties. We confirm that the $\beta$ value negligibly changes by the data reduction and fitting wavelength ranges for UV emission/absorption line masking. The extreme blue slope, $\beta=-3.0$, rules out significant contributions from dust extinction or AGN activity. Comparing with stellar and nebular emission models, we find that such a blue UV slope cannot be reproduced solely by stellar models even with very young, metal-poor, or top-heavy contiguous star formation associated with strong nebular continua making the UV slopes red, but with a high ionizing photon escape fraction, $f_\mathrm{esc}^\mathrm{ion} \gtrsim 0.5$, for a weak nebular continuum. While the H$\beta$ emission line is not detected, likely due to the limited sensitivity of the spectrum, we find moderately weak [O III] $\lambda\lambda$4959,5007 emission lines for the given star-formation rate ($3\, \mathrm{M_\odot}$ yr$^{-1}$) and stellar mass ($10^{8.0} \, \mathrm{M_\odot}$) that are about three times weaker than the average emission lines, again suggestive of the high ionizing photon escape fraction, $f_\mathrm{esc}^\mathrm{ion} \sim 0.7$ or more. EBG-1 would provide crucial insights into stellar and nebular continuum emission in high-redshift galaxies, serving as an example of the ionizing photon escaping site at the epoch of reionization.


**Translated Abstract**: 

我们研究了利用JWST/NIRSpec PRISM光谱获取的863个红移为4-14的星系的紫外连续谱斜率β。我们发现了一个显著的星系（称为EBG-1），其红移为9.25，具有显著的蓝色紫外斜率β=-2.99±0.15，这与其他表现出红色连续谱或由于较大不确定性而模糊的蓝色连续谱的星系不同。我们确认β值在数据处理和光谱拟合波长范围的变化都很小。极端蓝色的斜率β=-3.0排除了尘埃消光和活动星系核（AGN）贡献的可能性。与恒星和星云发射模型相比，我们发现如此蓝色的UV斜率只能通过较高的电离光子逃逸率f_esc^ion ≥ 0.5，与微弱的星云连续谱结合才能重现。尽管Hβ发射线未被检测到，但我们发现[O III]λλ4959,5007的发射线相对较弱，可能与星形成率（3 M⊙年⁻¹）和恒星质量（10^8.0 M⊙）有关，且相较于平均发射线弱三倍，这再次提示了较高的电离光子逃逸率f_esc^ion ∼ 0.7或更高。EBG-1为高红移星系中的恒星和星云连续发射提供了关键见解，成为了在再电离时期电离光子逃逸的一个示例。

**Summary**:

- (1): 本文的研究背景是早期宇宙中第一批星系的星形成与电离光子的逃逸对于宇宙重离子的贡献。

- (2): 过去的研究主要依赖于光度法来估算紫外斜率β，但存在发射线污染的问题。与现有方法不同，本文采用了高质量的光谱数据分析，从而提高了斜率測量的准确性。该方法明确提出了基于音量的标准，解决了光度法中存在的系统性偏差。

- (3): 本文的贡献在于识别了一个红移为9.25且具有显著蓝色紫外斜率的星系EBG-1，为高红移星系的恒星和星云连续发射及电离光子逃逸提供了新见解。

- (4): 本文的方法通过大规模光谱样本的分析，并计算紫外斜率β，结合对星系发射线的观测，特别强调了电离光子逃逸率的作用。

- (5): 本研究在识别EBG-1及其特征方面取得了成功，表明该星系的电离光子逃逸率可能超过0.5，支持了其对宇宙重离子的贡献的假设。


# ALMA# blackhole
# M87
# Brightest Cluster Galaxy
# Brightest Cluster Galaxies
# AGN
## The RESOLVE and ECO G3 Initiative: Drivers of HI Content and X-ray Emission in Galaxy Groups
- **Url**: http://arxiv.org/abs/2504.13103v1
- **Authors**: ['Zackary L. Hutchens', 'Sheila J. Kannappan', 'Kelley M. Hess', 'Andrew J. Baker', 'Ming Sun', 'Derrick S. Carr', 'Kathleen D. Eckert', 'David V. Stark']
- **Abstrat**: Adding to the RESOLVE and ECO Gas in Galaxy Groups (G3) initiative, we examine possible drivers of group-integrated HI-to-halo mass ratios ($M_{\rm HI,grp}/M_{\rm halo}$) and group X-ray emission, including group halo mass ($M_{\rm halo}$), virialization as probed by crossing time ($t_{\rm cross}$), presence of active galactic nuclei (AGN), and group-integrated fractional stellar mass growth rate (FSMGR$_{\rm grp}$). G3 groups span $M_{\rm halo}=10^{11-14.5}\,M_\odot$ with comprehensive HI and AGN information, which we combine with X-ray stacking of ROSAT All-Sky data. We detect hot gas emission exceeding AGN and X-ray binary backgrounds confidently for $M_{\rm halo}=10^{12.6-14}\,M_\odot$ and unambiguously for $M_{\rm halo}>10^{14}\,M_\odot$, reflecting an inverse dependence of $M_{\rm\,HI,grp}/M_{\rm halo}$ and hot gas emission on halo mass. At fixed halo mass, $M_{\rm\,HI,grp}/M_{\rm halo}$ transitions to greater spread below $t_{\rm cross}\sim2$ Gyr. Dividing groups across this transition, lower-$t_{\rm cross}$ groups show elevated X-ray emission compared to higher-$t_{\rm cross}$ groups for $M_{\rm halo}>10^{13.3}\,M_\odot$, but this trend reverses for $M_{\rm halo}=10^{12.6-13.3}\,M_\odot$. Additionally, AGN-hosting halos below $M_{\rm halo}\sim10^{12.1}\,M_\odot$ exhibit a broad, $\sim$0.25 dex deep valley in $M_{\rm HI,grp}/M_{\rm halo}$ compared to non-AGN-hosting halos with correspondingly reduced FSMGR$_{\rm grp}$. When diluted by non-AGN-hosting halos, this valley becomes shallower and narrower, falling roughly between $M_{\rm halo}=10^{11.5}\,M_\odot$ and $M_{\rm halo}=10^{12.1}\,M_\odot$ in the overall $M_{\rm\,HI,grp}/M_{\rm\,halo}$ vs. $M_{\rm halo}$ relation. We may also detect a second, less easily interpreted valley at $M_{\rm halo}\sim10^{13}\,M_\odot$. Neither valley matches theoretical predictions of a deeper valley at or above $M_{\rm halo}=10^{12.1}\,M_\odot$.


**Translated Abstract**: 

本研究在RESOLVE和ECO气体在星系团（G3）计划的基础上，探讨了影响团体整体氢原子（H I）-光晕质量比（M_{HI,grp}/M_{halo}）及团体X射线发射的可能驱动因素，包括团体光晕质量（M_{halo}）、通过交叉时间（t_{cross}）探测的团体的热化程度、活动星系核（AGN）的存在以及团体整体的恒星质量增长率（FSMGR_{grp}）。G3团体覆盖了M_{halo} = 10^{11-14.5} M⊙的范围，具有全面的H I和AGN信息，并结合了ROSAT全天空数据的X射线叠加。我们自信地检测到的热气体发射超出AGN和X射线双星背景，M_{halo} = 10^{12.6-14} M⊙时，而对于M_{halo} > 10^{14} M⊙则无疑得到确认，反映了M_{HI,grp}/M_{halo}与热气体发射在光晕质量上的反向依赖关系。在固定的光晕质量下，M_{HI,grp}/M_{halo}在t_{cross}约为2 Gyr时转变为更大的散布。跨越这个转变的团体中，低t_{cross}团体的X射线发射显著高于高t_{cross}团体，适用于M_{halo} > 10^{13.3} M⊙，但对于M_{halo} = 10^{12.6-13.3} M⊙则这一趋势相反。此外，M_{halo} 低于10^{12.1} M⊙的AGN宿主光晕表现出显著的、约0.25 dex深的M_{HI,grp}/M_{halo}谷，与对应的非AGN宿主光晕相比，FSMGR_{grp}明显降低。当被非AGN宿主光晕稀释时，这一谷变得更浅且更窄，大致落在M_{halo} = 10^{11.5} M⊙和M_{halo} = 10^{12.1} M⊙之间的整体M_{HI,grp}/M_{halo}与M_{halo}关系中。我们可能还探测到M_{halo} ≈ 10^{13} M⊙处的第二个不易解释的谷。没有一个谷与理论预测的在M_{halo} ≥ 10^{12.1} M⊙处的更深谷相匹配。

**Summary**:

- (1): 本文研究背景是星系团中H I含量与光晕质量的关系，探讨其如何受环境影响。

- (2): 以往的方法主要通过观察群体H I与光晕质量的关系来研究这些影响，但通常未考虑其他因素如动态状态和AGN的作用。提出的方法通过系统结合H I、X射线和AGN信息，深入分析多种驱动因素，克服了过去研究的不足，具有良好的动机。

- (3): 本文的贡献在于提供了G3团体中H I与光晕质量比及X射线发射的深入理解，探讨了不同质量光晕及其动态状态对H I含量的影响。

- (4): 本文采用了基于ROSAT X射线数据的叠加分析，结合H I和AGN数据，系统研究了团体的热气体发射、光晕质量及相关数量，形成了一个综合的研究方法。

- (5): 本文在M_{halo}与M_{HI,grp}/M_{halo}的关系分析中取得了显著结果，检测到与光晕质量相关的热气体发射趋势，同时揭示了AGN对H I含量的影响，这些表现支持了研究目标的实现。


## Analysis of Multi-epoch JWST Images of $\sim 300$ Little Red Dots: Tentative Detection of Variability in a Minority of Sources
- **Url**: http://arxiv.org/abs/2411.02729v2
- **Authors**: ['Zijian Zhang', 'Linhua Jiang', 'Weiyang Liu', 'Luis C. Ho']
- **Abstrat**: James Webb Space Telescope (JWST) has revealed a population of red and compact sources at $z \gtrsim 5$ known as ``Little Red Dots'' (LRDs) that are likely active galactic nuclei (AGNs). Here we present a comprehensive study of the variability of 314 LRDs with multi-epoch JWST observations in five deep fields: UDS, GOODS-S, GOODS-N, Abell 2744, and COSMOS. Our analyses use all publicly available JWST NIRCam imaging data in these fields, together with multi-epoch JWST MIRI images available. We measure the significance (signal-to-noise ratio or ${\rm SNR}_{\rm var}$) of the variabilities for all LRDs and statistically evaluate their variabilities using the ${\rm SNR}_{\rm var}$ distributions. We pay particular attention to the systematic offsets of photometric zero points among different epochs that seem to commonly exist. The derived ${\rm SNR}_{\rm var}$ distributions of the LRDs, including those with broad H$\alpha$/H$\beta$ emission lines, follow the standard Gaussian distribution, and are generally consistent with those of the comparison samples of objects detected in the same images. This finding suggests that the LRD population on average does not show strong variability, which can be explained by super-Eddington accretion of the black holes in AGNs. Alternatively, many of them may be dominated by galaxies. We also find eight strongly variable LRD candidates with variability amplitudes of 0.24 -- 0.82 mag. The rest-frame optical SEDs of these variable LRDs should have significant AGN contribution. Future JWST observations will provide more variability information of LRDs.


**Translated Abstract**: 

詹姆斯·韦伯太空望远镜（JWST）揭示了一种红色紧凑源群体，称为“时光红点”（Little Red Dots, LRDs），它们位于红移$z \gtrsim 5$，极有可能是活动星系核（AGNs）。本文对314个LRDs的可变性进行了全面研究，利用JWST在五个深场（UDS、GOODS-S、GOODS-N、阿贝尔2744和COSMOS）的多时段观测数据。我们的分析使用了这些区域内所有公开可用的JWST NIRCam成像数据，并结合可用的多时段JWST MIRI图像。我们测量了所有LRDs的可变性的显著性（信噪比或${\rm SNR}_{\rm var}$），并使用${\rm SNR}_{\rm var}$分布对其可变性进行了统计评估。我们特别注意到不同时间的光度零点系统偏移问题，这似乎是普遍存在的。LRDs的${\rm SNR}_{\rm var}$分布，包括那些具有宽H$\alpha$/H$\beta$发射线的对象，遵循标准的高斯分布，并且通常与在相同图像中检测的比较样本保持一致。这一发现表明，LRD群体在平均上并不显示出明显的可变性，这可以通过活动星系中黑洞的超埃ddington吸积来解释。或者，它们中的许多可能由星系主导。我们还发现了八个强烈可变的LRD候选者，其可变幅度为0.24至0.82 mag。这些可变的LRDs的视框光学SED应有显著的AGN贡献。未来的JWST观测将提供更多关于LRDs可变性的信息。

**Summary**:

- (1): 本文研究的背景是JWST发现了一类位于红移$z \gtrsim 5$的紧凑红色源群体，称为“时光红点”（LRDs），这些对象极有可能是活动星系核（AGNs）。

- (2): 过去的研究主要依赖静态的光谱能量分布（SED）和光度数据，但未能明确LRDs的真实性质。通过分析LRDs的可变性，不同于以往单一时刻的特征，这种方法能更加清晰地识别LRDs是否为AGNs，引入了基于变异性特征的评估方法，有效解决了LRDs分类中的不确定性。

- (3): 本文的贡献是在广泛的LRDs样本中，利用多时段JWST观测结构，首次系统性地量化了LRDs的可变性并发现了一小部分具有强烈可变性的LRD候选者。

- (4): 本文采用的研究方法包括从JWST的多时段观测中提取NIRCam与MIRI的数据，并计算信噪比${\rm SNR}_{\rm var}$来统计LRDs的可变性，重点考虑不同观察时刻间的光度偏移。

- (5): 本文在对314个LRDs的光变性分析中，发现了八个可变幅度为0.24至0.82 mag的LRD候选者。虽然总体上LRDs的可变性不显著，但这些结果表明潜在的AGN成分，支持了研究目标。


## A Galaxy with an Extremely Blue UV Slope $β=-3$ at $z=9.25$ Identified by JWST Spectroscopy: Evidence for a Weak Nebular Continuum and Efficient Ionizing Photon Escape?
- **Url**: http://arxiv.org/abs/2411.19893v3
- **Authors**: ['Hiroto Yanagisawa', 'Masami Ouchi', 'Kimihiko Nakajima', 'Yuichi Harikane', 'Seiji Fujimoto', 'Yoshiaki Ono', 'Hiroya Umeda', 'Minami Nakane', 'Hidenobu Yajima', 'Hajime Fukushima', 'Yi Xu']
- **Abstrat**: We investigate UV continuum slopes $\beta$ of 863 galaxies at $z=4-14$ using archival JWST/NIRSpec PRISM spectra obtained from major JWST GTO, ERS, and GO programs, including JADES, CEERS, and UNCOVER. Among these galaxies, we identify a remarkable galaxy at $z=9.25$, dubbed EBG-1, with a significantly blue UV slope $\beta=-2.99\pm0.15$, unlike the rest of the galaxies that exhibit red continua or ambiguous blue continua hindered by large uncertainties. We confirm that the $\beta$ value negligibly changes by the data reduction and fitting wavelength ranges for UV emission/absorption line masking. The extreme blue slope, $\beta=-3.0$, rules out significant contributions from dust extinction or AGN activity. Comparing with stellar and nebular emission models, we find that such a blue UV slope cannot be reproduced solely by stellar models even with very young, metal-poor, or top-heavy contiguous star formation associated with strong nebular continua making the UV slopes red, but with a high ionizing photon escape fraction, $f_\mathrm{esc}^\mathrm{ion} \gtrsim 0.5$, for a weak nebular continuum. While the H$\beta$ emission line is not detected, likely due to the limited sensitivity of the spectrum, we find moderately weak [O III] $\lambda\lambda$4959,5007 emission lines for the given star-formation rate ($3\, \mathrm{M_\odot}$ yr$^{-1}$) and stellar mass ($10^{8.0} \, \mathrm{M_\odot}$) that are about three times weaker than the average emission lines, again suggestive of the high ionizing photon escape fraction, $f_\mathrm{esc}^\mathrm{ion} \sim 0.7$ or more. EBG-1 would provide crucial insights into stellar and nebular continuum emission in high-redshift galaxies, serving as an example of the ionizing photon escaping site at the epoch of reionization.


**Translated Abstract**: 

我们研究了来自JWST/NIRSpec PRISM光谱的863个在红移z=4-14的星系的紫外线连续谱斜率β。我们发现了一个极其显著的星系EBG-1，在z=9.25，具有非常蓝的紫外线斜率β=-2.99±0.15，明显不同于其他展现红色连续谱或不明确蓝色连续谱的星系。我们确认，β值对数据处理和紫外线发射/吸收线掩蔽的适配波长范围几乎不变。极端的蓝色斜率β=-3.0排除了显著的尘埃消光或活动星系核（AGN）活动的贡献。通过与恒星和冽气发射模型的比较，我们发现这种蓝色紫外斜率不能仅通过恒星模型重现，即使采用非常年轻、金属贫乏或顶部较重的连续星形成伴随强冽气连续谱的方式使UV斜率显红，反而需要高的电离光逃逸分数fesc^ion ≥ 0.5。虽然Hβ发射线未被检测到，可能是由于光谱的灵敏度限制，我们发现中等偏弱的[O III] λλ4959,5007发射线，其给定的星形成率（3 M⊙ yr−1）和恒星质量（10^8.0 M⊙）约为平均发射线的三分之一，进一步暗示了高电离光逃逸分数fesc^ion ∼ 0.7甚至更高。EBG-1将为高红移星系的恒星与冽气连续谱发射提供关键见解，同时作为电离光逃逸场所的例证，助力理解再电离时代。

**Summary**:

- (1): 本文研究了高红移星系的紫外连续谱斜率，以理解这些星系对宇宙再电离的贡献。

- (2): 过去的方法侧重于光度观测，但容易受到发射线的干扰，难以准确测量紫外线斜率。新提出的方法使用高质量的光谱数据，能够更准确地评估紫外斜率，并解决了过往方法的测量不确定性。该方法具有很强的动机，因其能揭示极端蓝色紫外斜率的物理机制。

- (3): 本文的贡献是发现了一个在z=9.25的星系EBG-1，其极蓝的紫外斜率可能与弱冽气连续谱和高电离光逃逸分数相关，为理解高红移星系提供了重要见解。

- (4): 研究方法包括利用JWST/NIRSpec PRISM光谱资料，对紫外斜率进行系统测量和对比分析，结合恒星和冽气发射模型进行解释。

- (5): 本文针对紫外线连续谱斜率的研究，达成了有效识别高电离光逃逸分数的目标。支持了星系对早期宇宙再电离的重要性，并为将来的研究提供了数据基础。


# ALMA