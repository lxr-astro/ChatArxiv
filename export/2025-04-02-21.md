# blackhole
# galaxy
## Galactic foreground emissions randomization due to chaotic/turbulent dynamics of magnetized plasma dominated by magnetic helicity
- **Url**: http://arxiv.org/abs/2502.18379v3
- **Authors**: ['A. Bershadskii']
- **Abstrat**: Using results of numerical simulations and astrophysical observations (mainly in the WMAP and Planck frequency bands) it is shown that Galactic foreground emission becomes more sensitive to the mean magnetic field with the frequency, that results in the appearance of two levels of its randomization due to chaotic/turbulent dynamics of magnetized interstellar medium dominated by the magnetic helicity. The galactic foreground emission is more randomized at higher frequencies. The Galactic synchrotron and polarized dust emissions have been studied in detail. It is shown that the magnetic field imposes its level of randomization on the synchrotron and dust emission. The background magnetic field and emission have also been briefly discussed in this context. It is shown that they are considerably less randomized than the foreground ones. The main method for the theoretical consideration used in this study is the Kolmogorov-Iroshnikov phenomenology in the frames of distributed chaos notion. Despite the vast differences in the values of physical parameters and spatio-temporal scales between the numerical simulations and the astrophysical observations, there is a quantitative agreement between the results of the astrophysical observations and the numerical simulations in the frames of the distributed chaos notion.


**Translated Abstract**: 

通过数值模拟和天体物理观察（主要是在 WMAP 和 Planck 频带）结果显示，银河前景辐射对平均磁场的敏感性随着频率的增加而增强，这导致由于主导于磁能量的磁螺旋的混沌/湍流动态而出现两级随机化现象。银河前景辐射在高频时更随机化。对银河同步辐射和极化尘埃辐射进行了详细研究。研究表明，磁场对同步辐射和尘埃辐射施加了其随机化的水平。背景磁场和背景辐射在此背景下也得到了简要讨论，结果表明它们的随机化程度明显低于前景辐射。该研究所采用的主要理论考虑方法是基于分布混沌概念的 Kolmogorov-Iroshnikov 现象学。尽管数值模拟与天体物理观察之间存在广泛的物理参数和时空尺度差异，但在分布混沌的框架中，天体物理观察结果与数值模拟结果有定量一致性。

**Summary**:

- (1): 本文研究银河前景辐射的混沌/湍流动态对电磁场的影响，探讨了其在天体物理学中的重要性。

- (2): 过去采用的主要方法是基于扩展指数谱的尺度方法，存在理论基础薄弱的问题。提出的方法利用了 Kolmogorov-Iroshnikov 现象学，与现有方法形成对比，能够更好地量化前景辐射的随机化水平，具有良好的动机。

- (3): 本文贡献在于揭示了银河前景辐射随机化的两级现象，并确定了与磁螺旋主导的动态规律的关系，为理解银河前景辐射提供了新视角。

- (4): 本文研究方法采用了数值模拟与天体物理观察相结合，通过 Kolmogorov-Iroshnikov 现象学分析不同时频率下的随机化程度。

- (5): 研究表明，在 WMAP 和 Planck 的频率带下，提出的方法能够有效地揭示不同频率下银河前景辐射的随机化情况，达到了预期的研究目标。


## Galaxy Assembly and Evolution in the P-Millennium simulation: galaxy clustering
- **Url**: http://arxiv.org/abs/2409.02194v2
- **Authors**: ['Fabio Fontanot', 'Gabriella De Lucia', 'Lizhi Xie', 'Michaela Hirschmann', 'Carlton Baugh', 'John C. Helly']
- **Abstrat**: [abridged] We present results from the latest version of the GAEA theoretical model of galaxy formation coupled with merger trees extracted from the Planck Millennium Simulation (PMS). With respect to the Millennium Simulation, the PMS provides a better mass resolution, a larger volume and assumes cosmological parameters consistent with latest results from the Planck mission. The model includes a treatment for the partition of cold gas into atomic and molecular (H$_2$) components; a better treatment for environmental processes; an updated modelling of cold gas accretion on Super-Massive Black Holes. We compare GAEA predictions based on the PMS, with model realizations based on other simulations in the Millennium Suite, showing that the new model provides a remarkable consistency for most statistical properties of galaxy populations. We interpret this as due to the interplay between AGN feedback and H$_2$-based SFR, as model versions considering only one of the two mechanisms do not show the same level of consistency. We then compare model predictions with available data for the galaxy 2-point correlation function (2pCF) in the redshift range 0<z$\lesssim$3. We show that GAEA runs correctly recover the main dependencies of the 2pCF as a function of stellar mass, star formation activity, HI-content and redshift for galaxies more massive than 10$^{9}$ M$_\odot$. These results suggest that our model correctly captures both the distribution of galaxy populations in the Large Scale Structure and the interplay between the main physical processes regulating their baryonic content, both for central and satellite galaxies. The model predicts a small redshift evolution of the clustering amplitude, that results in an overprediction of z$\sim$3 clustering strength with respect to the available estimates, but is still consistent with data within 1-$\sigma$ uncertainties.


**Translated Abstract**: 

本文展示了最新版本的GAlaxy Evolution and Assembly (GAEA)理论模型与从Planck Millennium Simulation (PMS)提取的合并树相结合的结果。与传统使用的千年模拟相比，PMS提供了更好的质量分辨率、更大的体积，并且假设的宇宙学参数与Planck任务的最新结果一致。该模型同时考虑了冷气体划分为原子和分子(H$_2$)组分的方式；改进了对环境过程的处理；更新了冷气体累积在超大质量黑洞上的建模。我们将基于PMS的GAEA预测与千年序列中其他模拟的模型实现进行了比较，表明新模型在大多数星系群体的统计特性上具有显著一致性。我们将此解释为AGN反馈与基于H$_2$的SFR之间相互作用的结果，因为只考虑其中一种机制的模型未能显示出相同水平的一致性。然后，我们将模型预测与0<z<∼3的星系二维相关函数(2pCF)的现有数据进行比较，显示GAEA能够正确恢复2pCF作为星系质量、星形成活动、HI含量和红移的函数的主要依赖性。该结果表明我们的模型正确捕捉了星系群体在大尺度结构中的分布，以及调节其重子内容的主要物理过程之间的相互作用。这一模型预测了聚类幅度的小红移演化，导致在z∼3时的聚类强度被高估，但与数据在1-σ不确定性范围内仍一致。

**Summary**:

- (1): 本文研究了星系的大尺度结构及其演化，利用更新的Planck Millennium Simulation (PMS)数据来改进对星系形成的理论模型。

- (2): 过去的方法主要依赖于千年模拟，这些方法在质量分辨率和体积上存在限制，导致无法准确模拟星系的统计特性。与之相比，提出的GAEA模型基于PMS，利用了更高的质量分辨率和更大的体积，能更好地处理冷气体的分配及其对星系演化的影响，从而克服前述问题。

- (3): 本文的贡献在于提出了GAEA模型，该模型通过改进的气体分配和AGN反馈的建模，能够更准确地预测星系的统计特性并与现有观测数据一致。

- (4): 本文采用的方法通过结合来自PMS的合并树和GAEA理论模型，综合考虑了气体成分、环境效应以及黑洞的反馈机制，建立了一个相对完整的星系形成与演化模型。

- (5): 本文在星系的二维相关函数(2pCF)的任务上取得良好表现，正确恢复了不同红移下星系的主要依赖性。尽管在z∼3时存在轻微的聚类强度过估计，该模型仍在1-σ的不确定性范围内与数据一致，表明其结果支持研究目标。


## Strong nebular HeII emission induced by He$^+$ ionizing photons escaping through the clumpy winds of massive stars
- **Url**: http://arxiv.org/abs/2501.08376v3
- **Authors**: ['Arpita Roy', 'Mark R. Krumholz', 'Stefania Salvadori', 'Georges Meynet', 'Sylvia Ekström', 'Jorick S. Vink', 'Andreas A. C. Sander', 'Ralph S. Sutherland', 'Sourabh Paul', 'Andrea Pallottini', 'Ása Skúladóttir']
- **Abstrat**: The origin of nebular HeII-emission in both local and high-redshift galaxies remains an unsolved problem. Various theories have been proposed to explain it, including HeII-ionization by high mass X-ray binaries, ultra-luminous X-ray sources, or "stripped" He stars, shock ionization, and hidden AGNs. All these theories have shortcomings, however, leaving the cause of nebular HeII emission unclear. We investigate the hypothesis that the photons responsible for driving nebular HeII emissions are produced by the evolution of single massive stars and/or WR stars. We combine models of stellar evolution with population synthesis and nebular models to identify the most favorable scenarios for producing nebular HeII via this channel. We find that, if WR winds are clumpy enough to become close to optically thin, stellar populations with a wide range of metallicities and rotation rates can produce HeII ionizing photons at rates sufficient to explain the observed nebular $I(HeII)/I(\mathrm{H}\beta)$ ratio $\sim 0.004-0.07$ found in HeII-emitting galaxies. Metal-poor, rapidly rotating stellar populations ($[\mathrm{Fe}/\mathrm{H}]=-2.0$, $v/v_\mathrm{crit}=0.4$) also reach these levels of HeII production even for partially clumpy winds. These scenarios also yield HeII, H$\beta$, and "Blue-Bump" line equivalent widths comparable to those observed in HeII emitters. Only for laminar, non-clumpy winds, do we fail to find combinations of metallicity and stellar rotation rate that yield $I(HeII)/I(\mathrm{H}\beta)$ values as high as those observed in HeII-emitters. Contrary to previous findings, we conclude that single WR stars can be a strong source for nebular HeII emission if their winds are sufficiently clumpy allowing significant escape of hard ionizing photons.


**Translated Abstract**: 

在局部和高红移星系中，天体HeII发射的起源仍然是一个尚未解决的问题。已经提出了多种理论来解释这一现象，包括由高质量X射线双星、超大质量X射线源或“剥离”的He星、冲击电离和隐藏的活动星系核等产生的HeII电离光子。然而，所有这些理论都有其缺陷，因此天体HeII发射的原因仍不清楚。我们研究了这样的假设：驱动天体HeII发射的光子是由单一的质量较大的恒星和/或WR星的演化产生的。我们结合恒星演化模型与种群合成及天体模型，识别出通过这一途径产生天体HeII的最有利场景。我们发现，如果WR星风足够发团，接近光学薄弱，则具有广泛金属丰度和旋转速率的恒星种群能够以足够的速率产生HeII电离光子，以解释在HeII发射星系中观测到的HeII/Hβ比率∼0.004-0.07。金属贫乏、快速旋转的恒星种群（[Fe/H] = -2.0，v/v_crit = 0.4）即使在部分发团的风中也能达到这些HeII生产水平。这些场景还产生了与HeII发射体观测到的HeII、Hβ和“蓝峰”光谱线等效宽度相当的结果。仅在层流、无发团的风中，我们未能找到金属丰度和恒星旋转速率的组合，产生足够高的HeII/Hβ值，与HeII发射体观察到的相符。与先前的研究结果相反，我们得出结论，单个WR星可以成为天体HeII发射的强大来源，只要它们的风足够发团，从而允许大量硬电离光子的逃逸。

**Summary**:

- (1): 本文研究了天体HeII发射的起源，指出现有各种理论的不足，强调了高质量X射线双星和WR星的可能性。

- (2): 过去的方法包括多种理论解释，但无法充分说明观测到的HeII发射现象。与现有方法不同，本文提出用单一WR星的演化模型结合发团的风进行分析，以解决传统理论的局限性，认为发团风可使硬电离光子逃逸。

- (3): 本文的贡献在于提出了单个WR星在发团条件下可以作为天体HeII发射的重要来源，提供了新的解释框架，并验证了多种金属丰度和旋转速率的组合情景。

- (4): 本文研究方法结合了恒星演化模型、种群合成和天体模型，使用MESA代码计算恒星进化轨迹，以支持生成足够的HeII电离光子，并通过Cloudy计算天体发射。

- (5): 本文的研究针对天体HeII发射，达到了与观测相符的HeII/Hβ比率，并确认不同金属丰度和旋转速率的组合能够实现目标，支持了所提出的假设。


## Formation of the Little Red Dots from the Core-collapse of Self-interacting Dark Matter Halos
- **Url**: http://arxiv.org/abs/2503.23710v2
- **Authors**: ['Fangzhou Jiang', 'Zixiang Jiang', 'Haonan Zheng', 'Luis C. Ho', 'Kohei Inayoshi', 'Xuejian Shen', 'Mark Vogelsberger', 'Wei-Xiang Feng']
- **Abstrat**: We present a statistical study on the formation and growth of black holes (BHs) seeded by gravothermal core-collapse of self-interacting dark matter (SIDM) halos at high redshifts, using a semi-analytical framework based on Monte-Carlo merger trees. We demonstrate that BH formation via gravothermal collapse naturally occurs in high-concentration halos at a characteristic mass scale determined by the SIDM cross section, and only during the early Universe. This mechanism is particularly promising for explaining the abundance of little red dots (LRDs) -- a population of early, apparently galaxy-less active galactic nuclei hosting supermassive BHs. By incorporating this seeding process with simplified models of BH growth and mergers, we successfully reproduce the observed LRD mass function for moderately large cross sections of $\sigma_{0m} \sim 30 \mathrm{cm^2\,g^{-1}}$ and $\omega \sim 80\,\mathrm{km\,s^{-1}}$, intriguingly consistent with independent local constraints derived from galaxy rotation curves. Our results highlight the potential of high-redshift BH statistics as a complementary probe for constraining SIDM models.


**Translated Abstract**: 

我们展示了一项统计研究，关于自交互暗物质 (SIDM) 暗晕在高红移下的引力热核心崩塌所种植的黑洞 (BHs) 的形成与增长，使用基于蒙特卡罗合并树的半解析框架。我们证明，在高浓度暗晕中，黑洞通过引力热崩塌形成自然发生，其特征质量规模由SIDM的截面决定，并且仅发生在早期宇宙中。该机制特别有希望解释“小红点”（LRDs）的丰度——一种早期、明显缺乏星系的活跃星系核群体，宿主为超大质量黑洞。通过将这一种植过程与简化的黑洞增长和合并模型结合，我们成功地重现了LRD质量函数，适用于中等大的截面σ_{0m} ∼ 30 cm^2 g^{-1} 及ω ∼ 80 km s^{-1}，这一结果与基于星系旋转曲线的独立本地约束惊人一致。我们的结果强调了高红移黑洞统计作为约束SIDM模型的补充探测手段的潜力。

**Summary**:

- (1): 本文讨论自交互暗物质（SIDM）如何通过引力热核心崩塌形成黑洞，以解释早期宇宙中“小红点”（LRDs）的存在。

- (2): 过去的方法主要基于标准冷暗物质（CDM）模型，无法解释LRD的丰度及其星系缺失问题。本文提出的半解析框架结合了蒙特卡罗合并树，为BH种植及其后续成长提供了新的视角，有效克服了传统方法的限制。

- (3): 本文贡献在于提出了一种SIDM生成黑洞的新机制，通过统计和建模重现了LRD的质量函数，为理解小红点及其生成来源提供了解释。

- (4): 本研究的方法论包括基于蒙特卡罗合并树的半解析框架，用以模拟BH的种植及生长，从而分析不同质量规模下的核心崩塌特征。

- (5): 该方法在复现小红点质量函数方面表现良好，支持其目标，即利用SIDM模型解释高红移黑洞的统计特征。


## A shock crashing into confined dense circumstellar matter brightens the nascent SN 2023ixf
- **Url**: http://arxiv.org/abs/2411.06351v2
- **Authors**: ['Maokai Hu', 'Lifan Wang', 'Xiaofeng Wang']
- **Abstrat**: Red supergiants may experience a short-lived period of episodic mass loss rather than steady winds before their core collapses, leading to dense circumstellar matter (CSM) close to core-collapse supernovae (SNe). Interaction of SN ejecta with such nearby CSM can generate additional radiation, appending to the cooling radiation from the shock breakout of the progenitor envelope, to brighten the nascent SN explosion. This phenomenon is conspicuous for SN 2023ixf as its V-band brightness showed a rapid increase of about three magnitudes from the first to the third day after the explosion, which is distinctive among type II SNe with flash ionized signatures. In this paper, we employ a Monte Carlo method to simulate the radiative diffusion process in the unshocked CSM. Considering a wide range of mass-loss rates from 10^-5 to 10^-2 Msun/yr, we confirmed that the fast-rising light curve of SN 2023ixf can be fitted by the interaction of the SN ejecta with a CSM having a mass-loss rate of about 10^-2 Msun/yr located within 10^15 cm to the progenitor.


**Translated Abstract**: 红超巨星在核心坍塌之前可能经历短暂的间歇性质量损失，而非稳定的恒星风，导致在核心塌缩超新星（SNe）附近形成密集的星际物质（CSM）。超新星喷射物与这种附近的CSM相互作用可以产生额外辐射，附加在祖先包层的冲击爆发冷却辐射上，从而使新生超新星爆炸变得更加明亮。SN 2023ixf 特别明显，因为其 V 纸带亮度在爆炸后第一至第三天之间急剧增加了约三个星等，这在具有闪光电离特征的 II 型 SNe 中非常独特。本文采用蒙特卡洛方法模拟未被冲击的CSM中的辐射扩散过程。考虑到从 10^-5 到 10^-2 M⊙/yr 的宽范围的质量损失率，我们确认，SN 2023ixf 快速上升的光曲线可以通过 SN 喷射物与一个质量损失率约为 10^-2 M⊙/yr、距离祖先星在 10^15 cm 内的 CSM 的相互作用来拟合。

**Summary**:

- (1): 本文研究了红超巨星（RSG）在核心坍缩前的间歇性质量损失及其对超新星爆炸的影响，特别是超新星 SN 2023ixf 的亮度变化。

- (2): 过去的方法主要依赖于观察超新星前的恒星风强度及其影响，但往往无法准确模拟复杂的CSE环境。本文采用蒙特卡洛方法来解决这一问题，通过模拟未被冲击的CSM中的辐射扩散过程，能更准确地描述CSM的特性和其对超新星亮度的影响。

- (3): 本文的贡献在于提出了一种新的方法来理解和预测红超巨星和其附近CSM的相互作用如何影响超新星的光曲线，尤其是通过拟合SN 2023ixf的快速上升光曲线，验证了高质量损失率的CSM存在。

- (4): 本文提出的研究方法是采用蒙特卡洛模拟来考察CSM中辐射扩散的过程，并分析SN喷射物与CSM之间的相互作用。

- (5): 本文的任务是模拟SN 2023ixf的光曲线并准确拟合；通过考虑质量损失率约为10^-2 M⊙/yr的CSM，成功实现了光曲线的快速上升，表明方法能够有效支持研究目标。


# galaxies
## Galactic foreground emissions randomization due to chaotic/turbulent dynamics of magnetized plasma dominated by magnetic helicity
- **Url**: http://arxiv.org/abs/2502.18379v3
- **Authors**: ['A. Bershadskii']
- **Abstrat**: Using results of numerical simulations and astrophysical observations (mainly in the WMAP and Planck frequency bands) it is shown that Galactic foreground emission becomes more sensitive to the mean magnetic field with the frequency, that results in the appearance of two levels of its randomization due to chaotic/turbulent dynamics of magnetized interstellar medium dominated by the magnetic helicity. The galactic foreground emission is more randomized at higher frequencies. The Galactic synchrotron and polarized dust emissions have been studied in detail. It is shown that the magnetic field imposes its level of randomization on the synchrotron and dust emission. The background magnetic field and emission have also been briefly discussed in this context. It is shown that they are considerably less randomized than the foreground ones. The main method for the theoretical consideration used in this study is the Kolmogorov-Iroshnikov phenomenology in the frames of distributed chaos notion. Despite the vast differences in the values of physical parameters and spatio-temporal scales between the numerical simulations and the astrophysical observations, there is a quantitative agreement between the results of the astrophysical observations and the numerical simulations in the frames of the distributed chaos notion.


**Translated Abstract**: 

通过数值模拟结果和天体物理观察（主要是在WMAP和Planck频率带中的观察），显示银河前景发射对均匀磁场的敏感性随着频率的提高而增加。这导致银河前景发射由于受磁能量主导的混沌/湍流动力学而出现两种随机化水平。银河前景发射在较高频率下更趋随机化。对银河同步辐射和极化尘埃发射进行了详细研究。研究表明，磁场对同步辐射和尘埃发射施加了随机化水平。背景磁场和发射在这个背景下也被简要讨论，结果显示它们比前景发射的随机化程度要低得多。本研究主要采用的理论考虑方法是Kolmogorov-Iroshnikov现象学，属于分布混沌的框架。尽管数值模拟与天体物理观察在物理参数和时空尺度上存在巨大差异，但在分布混沌的框架内，天体物理观察结果与数值模拟结果之间有定量一致性。

**Summary**:

- (1): 本文研究背景是银河前景发射在天体物理研究中的重要性，既是了解磁化星际介质物理过程的关键来源，又是获得干净的宇宙微波背景（CMB）辐射图的主要障碍。

- (2): 过去的方法主要基于标度（功率律）方法来解释前景图的功率谱，但这种方法在实践中难以实现所需的广泛尺度。本文提出的Kolmogorov-Iroshnikov现象学在分布混沌框架下为混沌/湍流过程提供了一种新的量化方式，从而解决了传统方法中的随机性解释问题，因而这一方法具有很强的动机。

- (3): 本文的贡献在于揭示了高频率下银河前景发射随机化的两种水平，并且通过数值模拟和观测数据之间的定量一致性支持了分布混沌模型的有效性。

- (4): 本文提出的研究方法结合了Kolmogorov-Iroshnikov现象学与分布混沌概念，分析了磁能量主导的湍流动力学对银河前景发射随机化的影响。

- (5): 本文方法针对银河前景和背景磁场的发射随机化任务，表明在较高频率下前景发射的随机化程度显著增强。这一性能支持了其研究的目标，即更好地理解银河前景发射及其对CMB观测的影响。


## Galaxy Assembly and Evolution in the P-Millennium simulation: galaxy clustering
- **Url**: http://arxiv.org/abs/2409.02194v2
- **Authors**: ['Fabio Fontanot', 'Gabriella De Lucia', 'Lizhi Xie', 'Michaela Hirschmann', 'Carlton Baugh', 'John C. Helly']
- **Abstrat**: [abridged] We present results from the latest version of the GAEA theoretical model of galaxy formation coupled with merger trees extracted from the Planck Millennium Simulation (PMS). With respect to the Millennium Simulation, the PMS provides a better mass resolution, a larger volume and assumes cosmological parameters consistent with latest results from the Planck mission. The model includes a treatment for the partition of cold gas into atomic and molecular (H$_2$) components; a better treatment for environmental processes; an updated modelling of cold gas accretion on Super-Massive Black Holes. We compare GAEA predictions based on the PMS, with model realizations based on other simulations in the Millennium Suite, showing that the new model provides a remarkable consistency for most statistical properties of galaxy populations. We interpret this as due to the interplay between AGN feedback and H$_2$-based SFR, as model versions considering only one of the two mechanisms do not show the same level of consistency. We then compare model predictions with available data for the galaxy 2-point correlation function (2pCF) in the redshift range 0<z$\lesssim$3. We show that GAEA runs correctly recover the main dependencies of the 2pCF as a function of stellar mass, star formation activity, HI-content and redshift for galaxies more massive than 10$^{9}$ M$_\odot$. These results suggest that our model correctly captures both the distribution of galaxy populations in the Large Scale Structure and the interplay between the main physical processes regulating their baryonic content, both for central and satellite galaxies. The model predicts a small redshift evolution of the clustering amplitude, that results in an overprediction of z$\sim$3 clustering strength with respect to the available estimates, but is still consistent with data within 1-$\sigma$ uncertainties.


**Translated Abstract**: 

我们展示了基于Planck千年模拟（PMS）的最新版本GAlaxy演化与组装（GAEA）理论模型的结果。与传统的千年模拟相比，PMS提供了更好的质量分辨率、更大的体积，并且假设与Planck任务最新结果一致的宇宙学参数。该模型对冷气体分成原子和分子（H$_2$）成分的处理、对环境过程的改进处理以及对超大质量黑洞的冷气体吸积更新建模都进行了考虑。我们将基于PMS的GAEA预测与千年系列中其他模拟的模型实现相比较，显示出新模型在大多数统计特性方面提供了显著一致性。我们认为这是由于AGN反馈和基于H$_2$的星形成率（SFR）之间的相互作用所致。然后，我们将模型预测与0<z<3红移范围内银河系的二维点关联函数（2pCF）的可用数据进行比较，显示GAEA模拟正确恢复了2pCF的主要依赖性。这些结果表明，我们的模型正确捕捉了银河系在大尺度结构中的分布特性以及调节其物质内容的主要物理过程间的相互作用。

**Summary**:


- (1): 本文研究了宇宙大尺度结构（LSS）的演化及其对银河系群集特性的影响，强调了银河系的分布对天文学和宇宙学的约束。

- (2): 过去的方法主要通过较低分辨率和不一致的宇宙学参数模型进行研究。这些方法未能有效捕捉银河系星形成及其环境影响。本文提出的GAEA模型利用更新的PMS，解决了这些问题，强调了AGN反馈和H$_2$基础星形成率的相互作用，确保了更一致的统计特性。

- (3): 本文的主要贡献是通过GAEA模型，系统地改进了对银河系群集特性和其环境因素影响的理解，提供了基于新模拟数据的更精确的银河系聚类预测。

- (4): 本文采用GAEA模型，该模型通过使用集成的PMS数据进行银河系的质量分辨率处理、冷气体成分划分等方面的创新，结合环境因素，更为准确地模拟银河系的形成与演化过程。

- (5): 本文针对2pCF任务开展研究，取得了良好的性能，能够正确恢复不同红移和质量范围下的银河系分布特性，从而支持了模型有效性和科学目标。


## Strong nebular HeII emission induced by He$^+$ ionizing photons escaping through the clumpy winds of massive stars
- **Url**: http://arxiv.org/abs/2501.08376v3
- **Authors**: ['Arpita Roy', 'Mark R. Krumholz', 'Stefania Salvadori', 'Georges Meynet', 'Sylvia Ekström', 'Jorick S. Vink', 'Andreas A. C. Sander', 'Ralph S. Sutherland', 'Sourabh Paul', 'Andrea Pallottini', 'Ása Skúladóttir']
- **Abstrat**: The origin of nebular HeII-emission in both local and high-redshift galaxies remains an unsolved problem. Various theories have been proposed to explain it, including HeII-ionization by high mass X-ray binaries, ultra-luminous X-ray sources, or "stripped" He stars, shock ionization, and hidden AGNs. All these theories have shortcomings, however, leaving the cause of nebular HeII emission unclear. We investigate the hypothesis that the photons responsible for driving nebular HeII emissions are produced by the evolution of single massive stars and/or WR stars. We combine models of stellar evolution with population synthesis and nebular models to identify the most favorable scenarios for producing nebular HeII via this channel. We find that, if WR winds are clumpy enough to become close to optically thin, stellar populations with a wide range of metallicities and rotation rates can produce HeII ionizing photons at rates sufficient to explain the observed nebular $I(HeII)/I(\mathrm{H}\beta)$ ratio $\sim 0.004-0.07$ found in HeII-emitting galaxies. Metal-poor, rapidly rotating stellar populations ($[\mathrm{Fe}/\mathrm{H}]=-2.0$, $v/v_\mathrm{crit}=0.4$) also reach these levels of HeII production even for partially clumpy winds. These scenarios also yield HeII, H$\beta$, and "Blue-Bump" line equivalent widths comparable to those observed in HeII emitters. Only for laminar, non-clumpy winds, do we fail to find combinations of metallicity and stellar rotation rate that yield $I(HeII)/I(\mathrm{H}\beta)$ values as high as those observed in HeII-emitters. Contrary to previous findings, we conclude that single WR stars can be a strong source for nebular HeII emission if their winds are sufficiently clumpy allowing significant escape of hard ionizing photons.


**Translated Abstract**: 

在局部和高红移星系中，星云 He II 发射的起源仍然是一个未解的问题。各种理论已经被提出以解释这一现象，包括由高质量 X 射线双星、超光谱 X 射线源或“剥离” He 恒星、冲击电离以及隐藏的活动星系核产生的 He II 离子化光子。然而，所有这些理论都有其不足之处，使得星云 He II 发射的原因尚不明确。我们调查了一个假设，即驱动星云 He II 发射的光子是由单个大质量恒星和/或 Wolf-Rayet（WR）恒星的演化产生的。这项研究结合了恒星演化模型、种群合成和星云模型，以确定通过这一渠道产生星云 He II 的最有利情景。我们发现，如果 WR 风足够稀疏接近光学薄，则具有广泛金属丰度和旋转速率的恒星群体能够产生足够的 He II 离子化光子，以解释在 He II 发射星系中观察到的星云 I(He II)/I(Hβ) 比率约为 0.004-0.07。金属贫乏、快速旋转的恒星群体（[Fe/H] = -2.0, v/v_crit = 0.4）即使在部分稀疏风情况下也能达到这种 He II 生产水平。这些情景还产生与 He II 发射源中观察到的 He II、Hβ 和“蓝峰”线相当的宽度。只有在统一的非稀疏风情况下，我们未能找到能够产生与 He II 发射器中观察到的 I(He II)/I(Hβ) 比率一样高的金属丰度和恒星旋转速率组合。与之前的发现相悖，我们得出的结论是，如果 WR 恒星的风足够稀疏，它们可以是星云 He II 发射的强大来源。

**Summary**:

- (1): 本文研究星云 He II 发射的起源，在局部和高红移星系中尚无明确解释。

- (2): 以往的方法包括高质量 X 射线双星、超光谱 X 射线源、剥离 He 恒星等，这些理论存在不足，使得无法清楚解释 He II 发射的原因。本文提出结合单个大质量恒星和 WR 恒星演化的假设，强调 WR 风的光学稀疏性，从而允许硬离子化光子的显著逃逸。

- (3): 本文的贡献在于提出 WR 恒星是星云 He II 发射的强大来源，尤其是在风稀疏条件下，并且该理论能够解释观测到的其他现象。

- (4): 本文使用恒星演化模型与种群合成、星云模型结合的研究方法，采用 MESA 代码计算不同金属丰度和旋转速率下的演化轨迹。

- (5): 本文的研究成果表明，当 WR 风足够稀疏时，能够产生满足 I(He II)/I(Hβ) 的观测比率。这一性能指标支持了提出的目标，具有较强的验证性。


## Formation of the Little Red Dots from the Core-collapse of Self-interacting Dark Matter Halos
- **Url**: http://arxiv.org/abs/2503.23710v2
- **Authors**: ['Fangzhou Jiang', 'Zixiang Jiang', 'Haonan Zheng', 'Luis C. Ho', 'Kohei Inayoshi', 'Xuejian Shen', 'Mark Vogelsberger', 'Wei-Xiang Feng']
- **Abstrat**: We present a statistical study on the formation and growth of black holes (BHs) seeded by gravothermal core-collapse of self-interacting dark matter (SIDM) halos at high redshifts, using a semi-analytical framework based on Monte-Carlo merger trees. We demonstrate that BH formation via gravothermal collapse naturally occurs in high-concentration halos at a characteristic mass scale determined by the SIDM cross section, and only during the early Universe. This mechanism is particularly promising for explaining the abundance of little red dots (LRDs) -- a population of early, apparently galaxy-less active galactic nuclei hosting supermassive BHs. By incorporating this seeding process with simplified models of BH growth and mergers, we successfully reproduce the observed LRD mass function for moderately large cross sections of $\sigma_{0m} \sim 30 \mathrm{cm^2\,g^{-1}}$ and $\omega \sim 80\,\mathrm{km\,s^{-1}}$, intriguingly consistent with independent local constraints derived from galaxy rotation curves. Our results highlight the potential of high-redshift BH statistics as a complementary probe for constraining SIDM models.


**Translated Abstract**: 我们呈现了一个统计研究，研究了由自相互作用暗物质（SIDM）晕的重力热核坍缩所催生的黑洞（BH）的形成与增长，采用基于Monte-Carlo合并树的半分析框架。我们证明了在高浓度晕中，黑洞通过重力热坍缩的形成在特征质量尺度上自然发生，该尺度由SIDM横截面决定，并且仅在早期宇宙中发生。这一机制特别有希望解释小红点（LRDs）的丰度——一种早期、明显没有星系的活跃星系核，宿主为超大质量黑洞。通过将这一催生过程与简化的黑洞增长和合并模型结合，我们成功再现了在中等大横截面下观察到的LRD质量函数（σ0m ∼ 30 cm² g⁻¹和ω ∼ 80 km s⁻¹），这与从星系旋转曲线推导的独立局部约束惊人一致。我们的结果凸显了高红移黑洞统计作为限制SIDM模型的补充探测工具的潜力。

**Summary**:

- (1): 本文的研究背景是自相互作用暗物质（SIDM）作为潜在的宇宙学模型，以应对长期存在的宇宙学挑战，同时探讨重力热核坍缩在早期宇宙中如何导致超大质量黑洞的形成。

- (2): 过去的方法包括基于冷暗物质（CDM）模型的黑洞形成理论，但它们无法解决小红点（LRDs）这一新发现的观测挑战。本文提出的方法通过分析SIDM的重力热坍缩过程来解决这一问题，适用于早期宇宙中没有显著恒星成分的晕。这一方法得到了良好的理论支持。

- (3): 本文的贡献在于通过SIDM核心坍缩解释了早期宇宙中小红点（LRDs）的形成机制，并成功再现了与观测数据相符的黑洞质量函数。

- (4): 本文提出的研究方法为半分析框架结合Monte-Carlo合并树，这样可以模拟黑洞的形成和随后的增长过程。

- (5): 本文的任务是研究超大质量黑洞的形成，通过对小红点（LRDs）质量函数的成功再现，证明了在适度SIDM横截面的情况下达成的性能可以支持其研究目标。


## A shock crashing into confined dense circumstellar matter brightens the nascent SN 2023ixf
- **Url**: http://arxiv.org/abs/2411.06351v2
- **Authors**: ['Maokai Hu', 'Lifan Wang', 'Xiaofeng Wang']
- **Abstrat**: Red supergiants may experience a short-lived period of episodic mass loss rather than steady winds before their core collapses, leading to dense circumstellar matter (CSM) close to core-collapse supernovae (SNe). Interaction of SN ejecta with such nearby CSM can generate additional radiation, appending to the cooling radiation from the shock breakout of the progenitor envelope, to brighten the nascent SN explosion. This phenomenon is conspicuous for SN 2023ixf as its V-band brightness showed a rapid increase of about three magnitudes from the first to the third day after the explosion, which is distinctive among type II SNe with flash ionized signatures. In this paper, we employ a Monte Carlo method to simulate the radiative diffusion process in the unshocked CSM. Considering a wide range of mass-loss rates from 10^-5 to 10^-2 Msun/yr, we confirmed that the fast-rising light curve of SN 2023ixf can be fitted by the interaction of the SN ejecta with a CSM having a mass-loss rate of about 10^-2 Msun/yr located within 10^15 cm to the progenitor.


**Translated Abstract**: 

红超巨星在核心坍缩之前可能经历短暂的周期性质量损失，而非稳定的风，这导致核心坍缩超新星（SNe）附近形成密集的星际物质（CSM）。超新星（SN）喷发物与这种附近的CSM相互作用可以产生额外的辐射，补充来自前体包层冲击突破的冷却辐射，从而使新生超新星爆炸变亮。这一现象在SN 2023ixf中尤为明显，因为它的V波段亮度在爆炸后前两天内迅速增加约三个星等，这在具有闪光离子化特征的II型超新星中显得独特。本文采用Monte Carlo方法模拟未被冲击的CSM中的辐射扩散过程。考虑到从10^-5到10^-2 M⊙/年范围内的各种质量损失率，我们确认SN 2023ixf的快速上升光曲线可以通过与存在于离前体10^15厘米内的质量损失率约为10^-2 M⊙/年的CSM的SN喷发物相互作用来拟合。

**Summary**:

- (1): 本文的研究背景是红超巨星在核心坍缩前可能经历的周期性质量损失，以及这些变化如何影响超新星爆炸前的星际物质（CSM）密度。

- (2): 过去的方法主要依赖于观测数据分析，未能有效模拟CSM与超新星喷发物之间的复杂相互作用。本文提出的Monte Carlo方法能够更精确地模拟辐射扩散过程，改善了现有模型无法处理的细节问题，因此具有很强的动机。

- (3): 本文的贡献在于通过引入Monte Carlo方法，揭示了SN 2023ixf的快速上升光曲线与CSM相互作用的关系，并为超新星爆炸的机制提供了新的理解。

- (4): 本文提出的研究方法为使用Monte Carlo模拟未被冲击的CSM中的辐射扩散过程，涵盖了不同质量损失率的情况下进行的光曲线拟合。

- (5): 本文的方法在分析SN 2023ixf的光曲线上取得了显著的性能，能够有效拟合CSM的质量损失率与超新星爆炸的亮度变化，支持研究目标的达成。


# machine learning
## Low-Rank Thinning
- **Url**: http://arxiv.org/abs/2502.12063v3
- **Authors**: ['Annabelle Michael Carrell', 'Albert Gong', 'Abhishek Shetty', 'Raaz Dwivedi', 'Lester Mackey']
- **Abstrat**: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.


**Translated Abstract**: 

该论文的目标是通过一小组具有代表性的点来总结数据集。显著的，像 Kernel Halving 和 Compress 这样的次高斯稀疏算法可以在显著减少摘要点数量的同时匹配均匀子抽样的质量。然而，现有的保证仅覆盖有限范围的分布和基于核的质量度量，并且在维度依赖性上过于悲观。为了解决这些缺陷，我们引入了一种新的低秩分析，以适用于任何分布和任何核，确保高质量压缩，只要核或数据矩阵近似低秩。为了展示这些技术的广泛适用性，我们设计了实际的次高斯稀疏方法，改进了在变压器中近似注意力、通过重新排序加速随机梯度训练以及在近线性时间内区分分布的已知最佳保证。

**Summary**:

- (1): 本文研究的数据稀疏化是指寻找少量代表点以准确总结更大的数据集。

- (2): 过去的方法如次高斯稀疏算法（Kernel Halving 和 Compress）能够改善均匀子抽样的质量，但只适用于有限的分布和核度量，并且在维度依赖性上存在悲观性。所提出的方法通过引入新的低秩分析克服了这些限制，使其能够适用于任何分布和核，从而确保高质量的压缩。

- (3): 本文的贡献在于提出了一种新的低秩分析方法，并设计了实用的次高斯稀疏算法，改进了在多个应用场景中的质量保证，如变压器中的近似注意力和加速随机梯度训练。

- (4): 文章中的研究方法通过构造次高斯稀疏算法，利用低秩近似确保选择的代表点能够更好地总结输入数据，并介绍了相关的理论分析以支持方法的有效性。

- (5): 采用所提出的方法在变压器中的近似注意力、随机梯度训练的加速及近线性时间内区分分布的任务上取得了良好的性能，表现出其目标的可行性。


## Scalable Mechanistic Neural Networks for Differential Equations and Machine Learning
- **Url**: http://arxiv.org/abs/2410.06074v3
- **Authors**: ['Jiale Chen', 'Dingling Yao', 'Adeel Pervez', 'Dan Alistarh', 'Francesco Locatello']
- **Abstrat**: We propose Scalable Mechanistic Neural Network (S-MNN), an enhanced neural network framework designed for scientific machine learning applications involving long temporal sequences. By reformulating the original Mechanistic Neural Network (MNN) (Pervez et al., 2024), we reduce the computational time and space complexities from cubic and quadratic with respect to the sequence length, respectively, to linear. This significant improvement enables efficient modeling of long-term dynamics without sacrificing accuracy or interpretability. Extensive experiments demonstrate that S-MNN matches the original MNN in precision while substantially reducing computational resources. Consequently, S-MNN can drop-in replace the original MNN in applications, providing a practical and efficient tool for integrating mechanistic bottlenecks into neural network models of complex dynamical systems. Source code is available at https://github.com/IST-DASLab/ScalableMNN.


**Translated Abstract**: 

我们提出了可扩展机械神经网络（Scalable Mechanistic Neural Network，S-MNN），这是一种增强的神经网络框架，旨在处理涉及长时间序列的科学机器学习应用。通过重新构造原始的机械神经网络（MNN）（Pervez et al.，2024），我们将与序列长度相关的计算时间和空间复杂性分别从立方和二次降至线性。这一显著改善使得在不牺牲精度或可解释性的情况下高效建模长期动态。大量实验表明，S-MNN在精度上与原始MNN相当，但大幅减少了计算资源。因此，S-MNN可以替代原始MNN，成为将机械瓶颈融入复杂动态系统神经网络模型的实际高效工具。源代码可在https://github.com/IST-DASLab/ScalableMNN获取。

**Summary**:

- (1): 本文研究背景是机械神经网络（MNN）在科学机器学习中的应用，尤其是在处理长时间序列时遇到的计算效率和内存使用问题。

- (2): 过去的方法主要是原始的机械神经网络（MNN），其特点是计算时间复杂度为立方，空间复杂度为二次。这使得其在长序列或高分辨率数据上的应用受限。相比之下，提出的S-MNN通过重构线性系统，将复杂度降低到线性，从而有效解决了原有方法的计算瓶颈。该方法具有良好的激励机制，能够实现高效建模。

- (3): 本文的贡献在于提出了可扩展机械神经网络（S-MNN），显著降低了计算复杂性，同时保持了与原始MNN相当的精度，扩大了其实际应用范围。

- (4): 研究方法论包括对原始MNN的线性系统进行改革，消除松弛变量，简化为最小二乘回归，并设计了有效的求解器，利用了重构线性系统的稀疏性和带状结构。

- (5): 本文在多个基准任务上验证了S-MNN的效果，包括Lorenz系统的控制方程发现、Korteweg-de Vries（KdV）方程求解以及海表温度（SST）的长期预测。结果表明，S-MNN在精度上与原始MNN相当，同时显著减少计算时间和内存使用，从而达成了预期目标。


## Large-Scale Multi-omic Biosequence Transformers for Modeling Protein-Nucleic Acid Interactions
- **Url**: http://arxiv.org/abs/2408.16245v3
- **Authors**: ['Sully F. Chen', 'Robert J. Steele', 'Glen M. Hocky', 'Beakal Lemeneh', 'Shivanand P. Lad', 'Eric K. Oermann']
- **Abstrat**: The transformer architecture has revolutionized bioinformatics and driven progress in the understanding and prediction of the properties of biomolecules. Almost all research on large-scale biosequence transformers has focused on one domain at a time (single-omic), usually DNA/RNA or proteins. These models have seen incredible success in downstream tasks in each domain, and have achieved particularly noteworthy breakthroughs in sequence modeling and structural modeling. However, these single-omic models are naturally incapable of efficiently modeling multi-omic tasks, one of the most biologically critical being protein-nucleic acid interactions. We present our work training the largest open-source multi-omic foundation model to date. We show that these multi-omic models (MOMs) can learn joint representations between various single-omic distributions that are emergently consistent with the Central Dogma of molecular biology despite only being trained on unlabeled biosequences. We further demonstrate that MOMs can be fine-tuned to achieve state-of-the-art results on protein-nucleic acid interaction tasks, namely predicting the change in Gibbs free energy ($\Delta G$) of the binding interaction between a given nucleic acid and protein. Remarkably, we show that multi-omic biosequence transformers emergently learn useful structural information without any \textit{a priori} structural training, allowing us to predict which protein residues are most involved in the protein-nucleic acid binding interaction. Lastly, we provide evidence that multi-omic biosequence models are in many cases superior to foundation models trained on single-omics distributions, both in performance-per-FLOP and absolute performance, suggesting a more generalized or foundational approach to building these models for biology.


**Translated Abstract**: 

转化器架构已经彻底改变了生物信息学，并推动了对生物大分子性质的理解和预测的进展。几乎所有的大规模生物序列转化器的研究都专注于一次处理一个领域（单元组），通常是DNA/RNA或蛋白质。这些模型在各自领域的下游任务中取得了巨大的成功，并在序列建模和结构建模方面取得了特别显著的突破。然而，这些单元组模型自然无法有效地建模多组任务，其中一个生物学上最关键的任务是蛋白质-核酸相互作用。我们提出了训练迄今为止最大的开源多组基础模型的工作。我们表明这些多组模型（MOMs）可以学习与中央法则一致的各种单元组分布的联合表示，尽管仅在未标记的生物序列上进行训练。我们进一步证明，MOMs可以微调以在蛋白质-核酸相互作用任务上取得最先进的结果，特别是预测给定核酸和蛋白质之间的结合反应的吉布斯自由能变化（ΔG）。值得注意的是，我们表明多组生物序列转化器在没有任何先前结构训练的情况下，自发学习有用的结构信息，使我们能够预测哪些蛋白质残基最参与蛋白质-核酸结合相互作用。最后，我们提供证据表明，多组生物序列模型在许多情况下优于仅在单组分布上训练的基础模型，无论是在每个FLOP的性能还是绝对性能上，这表明这种模型构建生物学方法的更普遍或基础化的方向。

**Summary**:

- (1): 本文的研究背景是针对生物信息学在提取核酸和蛋白质序列信息方面的需求，以及深度学习的兴起推动了该领域的进展。

- (2): 过去的方法主要集中于单元组（针对DNA/RNA或蛋白质），尽管在序列建模和结构预测上取得了成功，但无法有效处理多组任务。本文的方法通过训练多组模型（MOMs）来解决这个问题，并发现MOMs能够学习不同单元组之间的联合表示，且这一方法是合理的。

- (3): 本文的贡献在于提出了最大的开源多组基础模型OmniBioTE，展现了其在蛋白质-核酸结合反应预测中的优越性能，同时揭示了其在没有先前结构训练的情况下学习结构信息的能力。

- (4): 本文提出的研究方法包括训练多组模型（MOMs），并在未标记的生物序列上学习联合表示，随后通过微调在特定任务上验证其性能。

- (5): 本文的方法在蛋白质-核酸结合相互作用任务中取得了最先进的结果，比如预测ΔG值。这一性能支持了模型用于多组生物信息的目标。


## Rehearsal-free Federated Domain-incremental Learning
- **Url**: http://arxiv.org/abs/2405.13900v2
- **Authors**: ['Rui Sun', 'Haoran Duan', 'Jiahua Dong', 'Varun Ojha', 'Tejal Shah', 'Rajiv Ranjan']
- **Abstrat**: We introduce a rehearsal-free federated domain incremental learning framework, RefFiL, based on a global prompt-sharing paradigm to alleviate catastrophic forgetting challenges in federated domain-incremental learning, where unseen domains are continually learned. Typical methods for mitigating forgetting, such as the use of additional datasets and the retention of private data from earlier tasks, are not viable in federated learning (FL) due to devices' limited resources. Our method, RefFiL, addresses this by learning domain-invariant knowledge and incorporating various domain-specific prompts from the domains represented by different FL participants. A key feature of RefFiL is the generation of local fine-grained prompts by our domain adaptive prompt generator, which effectively learns from local domain knowledge while maintaining distinctive boundaries on a global scale. We also introduce a domain-specific prompt contrastive learning loss that differentiates between locally generated prompts and those from other domains, enhancing RefFiL's precision and effectiveness. Compared to existing methods, RefFiL significantly alleviates catastrophic forgetting without requiring extra memory space, making it ideal for privacy-sensitive and resource-constrained devices.


**Translated Abstract**: 

我们介绍了一种无重演的联邦领域增量学习框架RefFiL，该框架基于全球提示共享的范式，以缓解在联邦领域增量学习中面临的灾难性遗忘挑战，在这一方法中，不断学习未见领域。典型的减轻遗忘的方法，例如使用额外的数据集和保留早期任务的私有数据，在联邦学习中并不可行，因为设备的资源有限。我们的方法RefFiL通过学习领域不变的知识并结合不同FL参与者所代表的各种领域特定提示来解决这个问题。RefFiL的一项关键特征是由我们的领域自适应提示生成器生成地方细粒度的提示，该生成器有效地利用地方领域知识，同时在全球范围内保持独特的边界。我们还引入了一种领域特定的提示对比学习损失，该损失可以区分本地生成的提示和来自其他领域的提示，从而增强RefFiL的精确性和有效性。与现有方法相比，RefFiL显著减轻了灾难性遗忘，而无需额外的内存空间，非常适合隐私敏感和资源受限的设备。

**Summary**:

- (1): 该文章的研究背景是当前联邦学习（Federated Learning, FL）主要集中在静态数据分布场景，未能适应动态现实世界数据的变化，导致灾难性遗忘问题尤为突出。

- (2): 过去的方法包括重演式方法、基于正则化的方法和网络扩展技术，这些方法在内存受限的联邦环境中实用性有限。提出的RefFiL方法与现有方法的不同之处在于其通过全球提示共享来增强本地模型的领域不变学习能力，解决了重演和内存需求的问题。所提方法在理论上具有很强的动机，旨在通过使用跨域信息来提升本地模型的鲁棒性。

- (3): 本文的贡献在于提出了新颖的RefFiL框架，它通过地方提示生成和对比学习策略，显著减少了灾难性遗忘，不需额外内存，适用于隐私敏感的环境。

- (4): 本文提出的研究方法包括一个客户端自适应提示生成器，结合了地方与全球的提示，利用对比学习以增加提示的区分度，从而有效学习领域不变的知识。

- (5): 提出的RefFiL方法在联邦领域增量学习（Federated Domain-Incremental Learning, FDIL）任务中展示了显著的性能改进，能够减少灾难性遗忘的影响，支持其研究目标。


## Identifying Predictions That Influence the Future: Detecting Performative Concept Drift in Data Streams
- **Url**: http://arxiv.org/abs/2412.10545v2
- **Authors**: ['Brandon Gower-Winter', 'Georg Krempl', 'Sergey Dragomiretskiy', 'Tineke Jelsma', 'Arno Siebes']
- **Abstrat**: Concept Drift has been extensively studied within the context of Stream Learning. However, it is often assumed that the deployed model's predictions play no role in the concept drift the system experiences. Closer inspection reveals that this is not always the case. Automated trading might be prone to self-fulfilling feedback loops. Likewise, malicious entities might adapt to evade detectors in the adversarial setting resulting in a self-negating feedback loop that requires the deployed models to constantly retrain. Such settings where a model may induce concept drift are called performative. In this work, we investigate this phenomenon. Our contributions are as follows: First, we define performative drift within a stream learning setting and distinguish it from other causes of drift. We introduce a novel type of drift detection task, aimed at identifying potential performative concept drift in data streams. We propose a first such performative drift detection approach, called CheckerBoard Performative Drift Detection (CB-PDD). We apply CB-PDD to both synthetic and semi-synthetic datasets that exhibit varying degrees of self-fulfilling feedback loops. Results are positive with CB-PDD showing high efficacy, low false detection rates, resilience to intrinsic drift, comparability to other drift detection techniques, and an ability to effectively detect performative drift in semi-synthetic datasets. Secondly, we highlight the role intrinsic (traditional) drift plays in obfuscating performative drift and discuss the implications of these findings as well as the limitations of CB-PDD.


**Translated Abstract**: 

概念漂移在流学习的背景下得到了广泛研究。然而，通常假设已部署模型的预测对系统经历的概念漂移没有影响。 closer inspection reveals that this is not always the case. 自动交易可能容易受到自我实现反馈循环的影响。同样，恶意实体可能会调整以躲避检测器，从而在对抗环境中形成自我否定的反馈循环，这要求已部署的模型不断重新训练。这样的模型可能引发概念漂移的设置被称为"performative"。在这项工作中，我们调查了这个现象。我们的贡献如下：首先，我们在流学习的背景下定义了可执行漂移，并将其与其他漂移原因区分开。我们引入了一种新型的漂移检测任务，旨在识别数据流中潜在的可执行概念漂移。我们提出了一种名为CheckerBoard Performative Drift Detection (CB-PDD)的漂移检测方法。我们将CB-PDD应用于展示不同自我实现反馈循环程度的合成和半合成数据集。结果显示，CB-PDD表现出高效性、低假阳性率、对内在漂移的韧性、与其他漂移检测技术的可比性，以及在半合成数据集中有效检测可执行漂移的能力。其次，我们强调了内在（传统）漂移在掩盖可执行漂移中的作用，并讨论了这些发现的影响以及CB-PDD的局限性。

**Summary**:

- (1): 本文的研究背景是概念漂移在流学习中的广泛研究，主要关注模型预测如何影响未来的分布，尤其是在自动交易和对抗环境中。

- (2): 过去的方法主要集中在已知可执行漂移存在的情境中，问题在于传统漂移检测器无法区分不同漂移类型，导致不能有效识别可执行漂移。本文提出的CheckerBoard Performative Drift Detection (CB-PDD)方法能够特定识别可执行漂移，与现有方法区分开来，解决了传统方法的局限性。

- (3): 论文的贡献在于定义了可执行漂移，提出了一种新的漂移检测任务，并介绍了CB-PDD方法，评估了其在不同条件下的有效性。

- (4): 论文所提出的研究方法是CB-PDD，这是一种能够在流数据中识别可执行漂移的新型检测方法。

- (5): 本文的方法在合成和半合成数据集上进行评估，表现出高效性、低假阳性率和对内在漂移的韧性，支持其在流学习中有效识别可执行漂移的目标。


## A Survey on Unlearnable Data
- **Url**: http://arxiv.org/abs/2503.23536v2
- **Authors**: ['Jiahao Li', 'Yiqiang Chen', 'Yunbing Xing', 'Yang Gu', 'Xiangyuan Lan']
- **Abstrat**: Unlearnable data (ULD) has emerged as an innovative defense technique to prevent machine learning models from learning meaningful patterns from specific data, thus protecting data privacy and security. By introducing perturbations to the training data, ULD degrades model performance, making it difficult for unauthorized models to extract useful representations. Despite the growing significance of ULD, existing surveys predominantly focus on related fields, such as adversarial attacks and machine unlearning, with little attention given to ULD as an independent area of study. This survey fills that gap by offering a comprehensive review of ULD, examining unlearnable data generation methods, public benchmarks, evaluation metrics, theoretical foundations and practical applications. We compare and contrast different ULD approaches, analyzing their strengths, limitations, and trade-offs related to unlearnability, imperceptibility, efficiency and robustness. Moreover, we discuss key challenges, such as balancing perturbation imperceptibility with model degradation and the computational complexity of ULD generation. Finally, we highlight promising future research directions to advance the effectiveness and applicability of ULD, underscoring its potential to become a crucial tool in the evolving landscape of data protection in machine learning.


**Translated Abstract**: 

不可学习数据（ULD）已成为一种创新的防御技术，旨在防止机器学习模型从特定数据中学习有意义的模式，从而保护数据隐私和安全性。通过对训练数据引入扰动，ULD 降低模型性能，使得未授权模型难以提取有用的表示。尽管 ULD 的重要性日益增加，现有的综述主要关注相关领域，例如对抗攻击和机器不可学习，却很少对 ULD 作为独立研究领域给予关注。本综述填补了这一空缺，提供了 ULD 的全面回顾，考察了不可学习数据生成方法、公共基准、评估指标、理论基础和实际应用。我们比较和对比了不同的 ULD 方法，分析了它们在不可学习性、隐蔽性、效率和鲁棒性方面的优缺点和权衡。此外，我们讨论了一些关键挑战，例如在扰动隐蔽性和模型降级之间取得平衡以及 ULD 生成的计算复杂性。最后，我们强调了未来研究的有前景的方向，以提高 ULD 的有效性和适用性，强调其在机器学习数据保护发展的背景下成为关键工具的潜力。

**Summary**:

- (1): 本文的研究背景是机器学习对大规模数据的依赖加剧了对数据隐私、安全和知识产权的担忧，因此提出了防止机器学习模型学习有用模式的技术，即不可学习数据（ULD）。

- (2): 过去的方法主要涉及机器不可学习和对抗攻击，存在的问题是过于关注后期处理，而未能有效防止模型学习。从现有方法的角度来看，提出的 ULD 方法采用主动策略，通过扰动训练数据来阻止模型学习有意义的表示，从根本上改善了这些问题。这种方法得到充分的动机支持，因为它主动影响训练过程而不是模型推断。

- (3): 本文的贡献在于填补了 ULD 作为独立研究领域的文献空缺，提供了关于不可学习数据生成方法、评估指标及其在实际应用中的潜力的全面评述。

- (4): 本文提出的研究方法论包括对 ULD 生成方法的分类与比较，重点分析不同方法的优缺点及其在隐蔽性、效率和鲁棒性方面的权衡。

- (5): 本文虽然没有详细说明特定任务，但强调了 ULD 在数据保护中的应用潜力，表明其性能在未来研究中有望达到理想目标。


## Optimal generalisation and learning transition in extensive-width shallow neural networks near interpolation
- **Url**: http://arxiv.org/abs/2501.18530v2
- **Authors**: ['Jean Barbier', 'Francesco Camilli', 'Minh-Toan Nguyen', 'Mauro Pastore', 'Rudy Skerk']
- **Abstrat**: We consider a teacher-student model of supervised learning with a fully-trained two-layer neural network whose width $k$ and input dimension $d$ are large and proportional. We provide an effective theory for approximating the Bayes-optimal generalisation error of the network for any activation function in the regime of sample size $n$ scaling quadratically with the input dimension, i.e., around the interpolation threshold where the number of trainable parameters $kd+k$ and of data $n$ are comparable. Our analysis tackles generic weight distributions. We uncover a discontinuous phase transition separating a "universal" phase from a "specialisation" phase. In the first, the generalisation error is independent of the weight distribution and decays slowly with the sampling rate $n/d^2$, with the student learning only some non-linear combinations of the teacher weights. In the latter, the error is weight distribution-dependent and decays faster due to the alignment of the student towards the teacher network. We thus unveil the existence of a highly predictive solution near interpolation, which is however potentially hard to find by practical algorithms.


**Translated Abstract**: 

我们考虑一个监督学习的教师-学生模型，该模型涉及一个完全训练的两层神经网络，其宽度 $k$ 和输入维度 $d$ 均很大且成比例。我们提供了一种有效的理论，以近似该网络在样本量 $n$ 随输入维度平方变化的情况下的贝叶斯最优泛化误差，即在插值阈值附近，其中可训练参数数量 $kd+k$ 和数据 $n$ 可比。我们的分析处理了通用的权重分布。我们发现了一个断续相变，分隔“通用”相和“专业化”相。在前者中，泛化误差与权重分布无关，并且以采样率 $n/d^2$ 缓慢衰减，学生仅学习教师权重的一些非线性组合。在后者中，误差依赖于权重分布且以更快的速度衰减，因为学生对教师网络的对齐。因此，我们揭示了在插值附近存在一个高预测性的解，但实际算法可能难以找到。

**Summary**:

- (1): 该文章的研究背景是理解神经网络的表达能力和泛化能力对实际应用的重要性，尤其是在模型部署时的成本效益规划。

- (2): 过去的方法主要研究了贝叶斯神经网络在“比例”情况下的性能，但往往局限于高斯分布权重和特定激活函数，导致分析受到简化的限制。本文提出的方法通过建立更通用的统计力学框架，能够处理任意激活函数和不同的权重分布，从而克服了传统方法的局限性，具有明确的动机。

- (3): 本文的贡献在于提出了对两层贝叶斯神经网络预测性能的更全面的理解，并揭示了在插值阈值附近存在的无普适性的现象。

- (4): 本文采用了教师-学生模型，分析了在样本大小与可训练参数数量相当时的网络泛化表现，同时考虑了通用的权重分布和不同的激活函数。

- (5): 本文在近似贝叶斯最优泛化误差的任务上取得了重要结果，展现了在插值阈值附近的高预测能力，这一性能支持了其研究目标。


## NNsight and NDIF: Democratizing Access to Open-Weight Foundation Model Internals
- **Url**: http://arxiv.org/abs/2407.14561v4
- **Authors**: ['Jaden Fiotto-Kaufman', 'Alexander R. Loftus', 'Eric Todd', 'Jannik Brinkmann', 'Koyena Pal', 'Dmitrii Troitskii', 'Michael Ripa', 'Adam Belfki', 'Can Rager', 'Caden Juang', 'Aaron Mueller', 'Samuel Marks', 'Arnab Sen Sharma', 'Francesca Lucchetti', 'Nikhil Prakash', 'Carla Brodley', 'Arjun Guha', 'Jonathan Bell', 'Byron C. Wallace', 'David Bau']
- **Abstrat**: We introduce NNsight and NDIF, technologies that work in tandem to enable scientific study of the representations and computations learned by very large neural networks. NNsight is an open-source system that extends PyTorch to introduce deferred remote execution. The National Deep Inference Fabric (NDIF) is a scalable inference service that executes NNsight requests, allowing users to share GPU resources and pretrained models. These technologies are enabled by the Intervention Graph, an architecture developed to decouple experimental design from model runtime. Together, this framework provides transparent and efficient access to the internals of deep neural networks such as very large language models (LLMs) without imposing the cost or complexity of hosting customized models individually. We conduct a quantitative survey of the machine learning literature that reveals a growing gap in the study of the internals of large-scale AI. We demonstrate the design and use of our framework to address this gap by enabling a range of research methods on huge models. Finally, we conduct benchmarks to compare performance with previous approaches.   Code, documentation, and tutorials are available at https://nnsight.net/.


**Translated Abstract**: 

我们介绍了NNsight和NDIF，这两项技术协同工作，促进对非常大神经网络所学习的表征和计算的科学研究。NNsight是一个开源系统，扩展了PyTorch，引入了延迟远程执行。国家深度推理框架（NDIF）是一个可扩展的推理服务，用于执行NNsight请求，允许用户共享GPU资源和预训练模型。这些技术依赖于干预图架构，该架构旨在将实验设计与模型运行时解耦。总体而言，该框架为深度神经网络（如非常大的语言模型（LLMs））的内部提供了透明和高效的访问，无需单独托管定制模型的成本或复杂性。我们进行了机器学习文献的定量调查，揭示了对大规模AI内部研究的日益增长的差距。我们展示了设计和使用我们的框架以解决这个差距，从而启用对巨大模型的多种研究方法。最后，我们进行了基准测试，以比较与先前方法的性能。代码、文档和教程可在https://nnsight.net/获取。

**Summary**:

  - (1): 本文的研究背景在于，大规模AI研究面临透明模型访问不足和计算资源不足的挑战。

  - (2): 过去的方法主要依赖商业API，这些API缺乏透明性和灵活性，限制了研究人员对模型内部的深入研究。本文提出的NNsight和NDIF相较于已有方法，提供了高效的结构化模型访问，降低了成本和复杂性，从而解决了这些问题，具有明确的动机。

  - (3): 本文的贡献在于提供了一种新的干预图架构，以及NNsight和NDIF的开源实现，这些工具为科学研究提供了透明的神经网络内部访问。

  - (4): 研究方法论包括使用干预图架构来组织对大规模模型的实验，利用NNsight扩展PyTorch，并通过NDIF支持实时推理，同时确保资源共享的安全性。

  - (5): 文章通过基准测试评估了方法的性能，证实了该方法在大型语言模型（LLMs）研究中的高效性和有效性，支持了促进开放式AI研究的目标。


## Optimizing Posterior Samples for Bayesian Optimization via Rootfinding
- **Url**: http://arxiv.org/abs/2410.22322v4
- **Authors**: ['Taiwo A. Adebiyi', 'Bach Do', 'Ruda Zhang']
- **Abstrat**: Bayesian optimization devolves the global optimization of a costly objective function to the global optimization of a sequence of acquisition functions. This inner-loop optimization can be catastrophically difficult if it involves posterior sample paths, especially in higher dimensions. We introduce an efficient global optimization strategy for posterior samples based on global rootfinding. It provides gradient-based optimizers with two sets of judiciously selected starting points, designed to combine exploration and exploitation. The number of starting points can be kept small without sacrificing optimization quality. Remarkably, even with just one point from each set, the global optimum is discovered most of the time. The algorithm scales practically linearly to high dimensions, breaking the curse of dimensionality. For Gaussian process Thompson sampling (GP-TS), we demonstrate remarkable improvement in both inner-and outer-loop optimization, surprisingly outperforming alternatives like EI and GP-UCB in most cases. Our approach also improves the performance of other posterior sample-based acquisition functions, such as variants of entropy search. Furthermore, we propose a sample-average formulation of GP-TS, which has a parameter to explicitly control exploitation and can be computed at the cost of one posterior sample. Our implementation is available at https://github.com/UQUH/TSRoots .


**Translated Abstract**: 

贝叶斯优化将成本高昂的目标函数的全局优化问题转化为一系列获取函数的全局优化问题。如果涉及后验样本路径，内循环优化可能会非常困难，尤其是在高维情况下。我们提出了一种基于全局根查找的高效全局优化策略，旨在优化后验样本。该方法为基于梯度的优化器提供了两组经过精心选择的起始点，旨在结合探索与开发。起始点的数量可以保持较小，而不牺牲优化质量。值得注意的是，即使仅依赖于每组一个起始点，大多数情况下也能发现全局最优解。该算法在高维中呈现出实质上线性扩展，打破了维度诅咒。对于Gaussian process Thompson sampling (GP-TS)，我们在内外循环优化中展示了显著改进，出人意料地在大多数情况下优于EI和GP-UCB等替代方案。我们的方法还提升了其他基于后验样本的获取函数的性能，比如熵搜索的变种。此外，我们提出了一种GP-TS的样本平均形式，具有一个参数可明确控制开发，且仅需一个后验样本的计算成本。我们的实现可在https://github.com/UQUH/TSRoots获取。

**Summary**:

- (1): 本文研究领域为贝叶斯优化，关注如何高效优化高成本的目标函数，尤其是在高维情况下的挑战。

- (2): 过去的方法主要包括基于梯度的多起始点优化和无导数优化，存在对后验样本路径优化困难的问题，尤其是在高维空间中会产生多个局部最优。本文提出的基于全局根查找的方法，通过选择结合探索与开发的智能起始点，从而在保持低起始点数量的同时提高了优化质量，对解决上述问题有显著效果。

- (3): 本文的贡献在于提出了一种有效的贝叶斯优化策略，基于全局根查找的后验样本优化新方法，能够显著提升优化效果，并成功在高维空间应用。

- (4): 研究方法包括利用路径条件分解，通过全局根查找智能选择起始点，并应用于梯度优化，以提高探索与开发的平衡。

- (5): 在Gaussian process Thompson sampling (GP-TS)任务上，达到显著进展，在内外循环优化中表现超出传统方法如EI和GP-UCB，支持其研究目标。


## Mixture of Experts Made Personalized: Federated Prompt Learning for Vision-Language Models
- **Url**: http://arxiv.org/abs/2410.10114v4
- **Authors**: ['Jun Luo', 'Chen Chen', 'Shandong Wu']
- **Abstrat**: Federated prompt learning benefits federated learning with CLIP-like Vision-Language Model's (VLM's) robust representation learning ability through prompt learning. However, current federated prompt learning methods are habitually restricted to the traditional FL paradigm, where the participating clients are generally only allowed to download a single globally aggregated model from the server. While justifiable for training full-sized models under federated settings, in this work, we argue that this paradigm is ill-suited for lightweight prompts. By facilitating the clients to download multiple pre-aggregated prompts as fixed non-local experts, we propose Personalized Federated Mixture of Adaptive Prompts (pFedMoAP), a novel FL framework that personalizes the prompt learning process through the lens of Mixture of Experts (MoE). pFedMoAP implements a local attention-based gating network that learns to generate enhanced text features for better alignment with local image data, benefiting from both local and downloaded non-local adaptive prompt experts. Extensive experiments on 9 datasets under various federated settings demonstrate the efficacy of the proposed pFedMoAP algorithm. The code is available at https://github.com/ljaiverson/pFedMoAP.


**Translated Abstract**: 

联邦提示学习通过提示学习增强了联邦学习中类似于CLIP的视觉-语言模型（VLM）的稳健表示学习能力。然而，目前的联邦提示学习方法通常受到传统联邦学习范式的限制，其中参与的客户端通常只允许从服务器下载一个全局聚合模型。虽然该范式在联邦设置下训练全尺寸模型是合理的，但在本研究中，我们认为这一范式不适合轻量级提示。通过使客户端能够下载多个预聚合提示作为固定的非本地专家，我们提出了个性化联邦自适应提示混合（pFedMoAP），一个通过专家混合（MoE）视角个性化提示学习过程的新型联邦学习框架。pFedMoAP实现了一个基于局部注意力的门控网络，学习生成增强的文本特征，以更好地与本地图像数据对齐，受益于本地和下载的非本地自适应提示专家。我们在9个数据集上的广泛实验验证了所提出的pFedMoAP算法的有效性。相关代码可在https://github.com/ljaiverson/pFedMoAP获得。

**Summary**:

- (1): 本文的研究背景是联邦学习（FL）在去中心化数据源上训练机器学习模型的应用，但面临参与客户端数据分布异质性的问题。

- (2): 过去的方法包括传统的联邦学习和个性化联邦学习（PFL），其问题在于通常仅允许客户端下载单个全局聚合模型，限制了灵活性。所提的方法pFedMoAP允许客户端下载多个轻量级的预聚合提示，解决了传统方法不适合轻量级提示的问题，因此具有良好的动机。

- (3): 本文的贡献在于提出了个性化联邦自适应提示混合（pFedMoAP）框架，通过增加提示灵活性，提升了提示学习的个性化效果，特别适用于CLIP类VLMs。

- (4): 本文提出的研究方法是基于局部注意力的门控网络，生成与本地图像数据更好对齐的增强文本特征，同时利用本地和下载的非本地自适应提示专家。

- (5): 方法在9个数据集上进行了广泛实验，证明了其有效性，使其性能达到了预期目标。


## FastLloyd: Federated, Accurate, Secure, and Tunable $k$-Means Clustering with Differential Privacy
- **Url**: http://arxiv.org/abs/2405.02437v2
- **Authors**: ['Abdulrahman Diaa', 'Thomas Humphries', 'Florian Kerschbaum']
- **Abstrat**: We study the problem of privacy-preserving $k$-means clustering in the horizontally federated setting. Existing federated approaches using secure computation suffer from substantial overheads and do not offer output privacy. At the same time, differentially private (DP) $k$-means algorithms either assume a trusted central curator or significantly degrade utility by adding noise in the local DP model. Naively combining the secure and central DP solutions results in a protocol with impractical overhead. Instead, our work provides enhancements to both the DP and secure computation components, resulting in a design that is faster, more private, and more accurate than previous work. By utilizing the computational DP model, we design a lightweight, secure aggregation-based approach that achieves five orders of magnitude speed-up over state-of-the-art related work. Furthermore, we not only maintain the utility of the state-of-the-art in the central model of DP, but we improve the utility further by designing a new DP clustering mechanism.


**Translated Abstract**:  
我们研究在水平联邦环境中隐私保护的 $k$-均值聚类问题。现有的使用安全计算的联邦方法遭受了巨大的开销，并且无法提供输出隐私。同时，差分隐私（DP）$k$-均值算法要么假设有一个受信的中央管理者，要么通过在局部DP模型中添加噪声显著降低效用。简单地将安全和中央DP解决方案结合会导致协议开销不切实际。相反，我们的工作对DP和安全计算组件都进行了增强，导致一种设计比以前的工作更快速、更私密且更准确。通过利用计算DP模型，我们设计了一种轻量级的基于安全聚合的方法，其速度比最先进的相关工作快五个数量级。此外，我们不仅保持了中央DP模型的最先进效用，还通过设计一种新的DP聚类机制进一步提高了效用。

**Summary**:

- (1): 本文研究水平联邦环境中的隐私保护 $k$-均值聚类问题，强调了在处理敏感数据时需要隐私保护技术的重要性。

- (2): 以往的方法主要使用安全多方计算来解决联邦 $k$-均值问题，但它们面临重大开销且未能保护输出隐私。差分隐私方法虽可保护输出，但在高隐私水平下效用低下。本文提出的方法通过增强DP和安全计算组件，提供了一种更高效的解决方案。

- (3): 论文的贡献在于设计了一种新的方法，结合了差分隐私和安全计算的优势，实现了快速且保持聚类效用的 $k$-均值聚类。

- (4): 本文提出的方法利用计算 DP 模型，采用轻量级的安全聚合方式，显著减少了运行时间。

- (5): 方法在 $k$-均值聚类任务上实现了比最先进工作快五个数量级的性能提升，并且在保持聚类效用方面的表现支持了其目标。


## Optimal or Greedy Decision Trees? Revisiting their Objectives, Tuning, and Performance
- **Url**: http://arxiv.org/abs/2409.12788v2
- **Authors**: ['Jacobus G. M. van der Linden', 'Daniël Vos', 'Mathijs M. de Weerdt', 'Sicco Verwer', 'Emir Demirović']
- **Abstrat**: Recently there has been a surge of interest in optimal decision tree (ODT) methods that globally optimize accuracy directly, in contrast to traditional approaches that locally optimize an impurity or information metric. However, the value of optimal methods is not well understood yet, as the literature provides conflicting results, with some demonstrating superior out-of-sample performance of ODTs over greedy approaches, while others show the opposite. Through a novel extensive experimental study, we provide new insights into the design and behavior of learning decision trees. In particular, we identify and analyze two relatively unexplored aspects of ODTs: the objective function used in training trees, and tuning techniques. Thus, we address these three questions: what objective to optimize in ODTs; how to tune ODTs; and how do optimal and greedy methods compare? Our experimental evaluation examines 11 objective functions, six tuning methods, and six claims from the literature on optimal and greedy methods on 180 real and synthetic data sets. Through our analysis, both conceptually and experimentally, we show the effect of (non-)concave objectives in greedy and optimal approaches; we highlight the importance of proper tuning of ODTs; support and refute several claims from the literature; provide clear recommendations for researchers and practitioners on the usage of greedy and optimal methods; and code for future comparisons.


**Translated Abstract**: 

近年来，最优决策树（Optimal Decision Trees, ODT）方法引起了极大的关注，这些方法直接优化准确性，与传统方法通过局部优化杂质或信息度量不同。然而，目前尚不清楚最优方法的价值，因为文献中存在相互矛盾的结果，一些研究表明 ODT 在样本外表现优于贪心方法，而另一些则显示相反。通过一项新的广泛实验研究，我们提供了关于学习决策树的设计和行为的新见解。特别地，我们识别并分析了 ODT 的两个相对未被探索的方面：训练树时使用的目标函数和调优技术。因此，我们解决了以下三个问题：在 ODT 中优化什么目标；如何调优 ODT；最优与贪心方法的比较如何？我们的实验评估考察了 11 个目标函数、六种调优方法以及文献中关于最优和贪心方法的六项主张，涵盖了 180 个真实和合成数据集。通过我们的分析（概念和实验），我们展示了（非）凹目标在贪心和最优方法中的影响；强调了适当调优 ODT 的重要性；支持并驳斥文献中的若干主张；为研究者和从业者提供了关于使用贪心和最优方法的明确建议；并为未来的比较提供了代码。

**Summary**:

- (1): 本文的研究背景是最优决策树（ODT）方法对准确性进行全球优化的趋势，尽管已经存在对于最优和贪心方法的相互矛盾的文献结果。

- (2): 过去的方法主要是贪心的自顶向下的引导启发式，包括 CART 和 C4.5，这些方法局部优化某些杂质或信息增益度量。问题在于，缺乏对 ODT 方法调优和目标函数优化的系统研究，导致无法直接比较不同方法。本文通过新的实验研究提出了明确的目标函数和调优技术，动机良好。

- (3): 本文的贡献在于提供了对贪心和最优方法的全面比较，识别并分析了未被充分探讨的目标函数和调优技术，并提出了相应的改进建议和代码支持。

- (4): 本文采用的研究方法包括对 180 个数据集进行实验评估，考察 11 个目标函数和六种调优方法，以分析贪心与最优决策树的性能和行为。

- (5): 本文在多个真实与合成数据集上进行了评估，展示了最优方法在样本外准确率方面的优势，结果支持了对 ODT 方法有效性的研究目标。


## AI-Powered Bayesian Inference
- **Url**: http://arxiv.org/abs/2502.19231v2
- **Authors**: ['Veronika Ročková', "Sean O'Hagan"]
- **Abstrat**: The advent of Generative Artificial Intelligence (GAI) has heralded an inflection point that changed how society thinks about knowledge acquisition. While GAI cannot be fully trusted for decision-making, it may still provide valuable information that can be integrated into a decision pipeline. Rather than seeing the lack of certitude and inherent randomness of GAI as a problem, we view it as an opportunity. Indeed, variable answers to given prompts can be leveraged to construct a prior distribution which reflects assuredness of AI predictions. This prior distribution may be combined with tailored datasets for a fully Bayesian analysis with an AI-driven prior. In this paper, we explore such a possibility within a non-parametric Bayesian framework. The basic idea consists of assigning a Dirichlet process prior distribution on the data-generating distribution with AI generative model as its baseline. Hyper-parameters of the prior can be tuned out-of-sample to assess the informativeness of the AI prior. Posterior simulation is achieved by computing a suitably randomized functional on an augmented data that consists of observed (labeled) data as well as fake data whose labels have been imputed using AI. This strategy can be parallelized and rapidly produces iid samples from the posterior by optimization as opposed to sampling from conditionals. Our method enables (predictive) inference and uncertainty quantification leveraging AI predictions in a coherent probabilistic manner.


**Translated Abstract**: 

生成性人工智能（GAI）的出现标志着一个转折点，改变了社会对知识获取的思考方式。虽然GAI在决策中不能完全被信任，但它仍然可以提供可以融入决策流程的有价值信息。我们并不把GAI的确定性缺乏和内在随机性视为问题，而是视为机遇。实际上，对给定提示的可变答案可以被利用来构建反映AI预测可靠性的先验分布。该先验分布可以与定制的数据集结合，用于完全的贝叶斯分析，结合AI驱动的先验。在本文中，我们在非参数贝叶斯框架内探索这种可能性。基本想法是为数据生成分布分配一个Dirichlet过程先验分布，以AI生成模型作为其基准。先验的超参数可以在样本外进行调整，以评估AI先验的信息性。后验模拟通过计算一个合适的随机功能来实现，该功能基于包含观察（标记）数据及其标签通过AI填补的虚构数据的增强数据。该策略可以并行化，并通过优化快速产生来自后验的独立同分布样本，而不是从条件分布中抽样。我们的方法以连贯的概率方式实现（预测）推理和不确定性量化，利用AI预测。

**Summary**:

- (1): 本文研究的背景是生成性人工智能（GAI）迅速成为知识获取的重要来源，但其实用性受限于用户对其概率特性的理解。

- (2): 过去的方法主要依赖于对特定模型参数的直接推理，而这种方法在面对不确定性时存在稳定性问题。本文提出的方法通过将GAI的输出用作Dirichlet过程的先验，解决了传统方法在小样本情况下的不足，更具透明性和灵活性。

- (3): 本文的贡献在于提出一种基于GAI的非参数贝叶斯推理方法，以便利用虚构数据增强实际数据，进行更可靠的推理和不确定性量化。

- (4): 本文的方法论通过构建Dirichlet过程先验来实现，在此基础上进行参数θ0的后验推理，利用生成的数据样本进行优化。

- (5): 本文将在皮肤病学数据集等任务中进行分析，表现出更好的不确定性量化能力，能够支持使用GAI的目标。


## One Policy to Run Them All: an End-to-end Learning Approach to Multi-Embodiment Locomotion
- **Url**: http://arxiv.org/abs/2409.06366v3
- **Authors**: ['Nico Bohlinger', 'Grzegorz Czechmanowski', 'Maciej Krupka', 'Piotr Kicki', 'Krzysztof Walas', 'Jan Peters', 'Davide Tateo']
- **Abstrat**: Deep Reinforcement Learning techniques are achieving state-of-the-art results in robust legged locomotion. While there exists a wide variety of legged platforms such as quadruped, humanoids, and hexapods, the field is still missing a single learning framework that can control all these different embodiments easily and effectively and possibly transfer, zero or few-shot, to unseen robot embodiments. We introduce URMA, the Unified Robot Morphology Architecture, to close this gap. Our framework brings the end-to-end Multi-Task Reinforcement Learning approach to the realm of legged robots, enabling the learned policy to control any type of robot morphology. The key idea of our method is to allow the network to learn an abstract locomotion controller that can be seamlessly shared between embodiments thanks to our morphology-agnostic encoders and decoders. This flexible architecture can be seen as a potential first step in building a foundation model for legged robot locomotion. Our experiments show that URMA can learn a locomotion policy on multiple embodiments that can be easily transferred to unseen robot platforms in simulation and the real world.


**Translated Abstract**: 

深度强化学习技术在稳健的腿部运动方面已取得了最先进的成果。虽然存在多种腿部平台，如四足、类人和六足机器人，但该领域仍缺乏一个能够轻松有效控制所有不同形体的单一学习框架，并可能实现对未见机器人形体的零或少量样本迁移。我们引入了URMA（Unified Robot Morphology Architecture），以填补这一空白。我们的框架将端到端的多任务强化学习方法带入腿部机器人领域，使得学习到的策略能够控制任何类型的机器人形态。我们方法的关键思想是允许网络学习一个抽象的运动控制器，通过我们形态无关的编码器和解码器，能够在不同形体之间无缝共享。这种灵活的架构可以被视为建立腿部机器人运动基础模型的潜在第一步。我们的实验表明，URMA可以在多个形体上学习运动策略，并能够轻松迁移到未见的机器人平台，无论是在仿真环境中还是在现实世界中。

**Summary**:

- (1): 本文的研究背景是腿部机器人在复杂环境中的稳健运动控制问题，目前采用深度强化学习技术已取得一定成果，但缺乏统一的学习框架。

- (2): 过去的方法通常依赖特定于机器人平台的策略，导致不同形态机器人之间的知识迁移困难。本文提出的URMA方法通过形态无关的编码器和解码器来处理不同机器人形态的控制，克服了迁移学习的障碍。该方法动机明确，旨在构建通用的运动控制策略。

- (3): 本文的贡献在于提出了URMA框架，使得一个学习到的运动策略能够同时适用于多种腿部机器人，并能够实现对未见机器人平台的有效迁移。

- (4): 本文的研究方法论是通过多任务强化学习（MTRL）框架设计一种新型神经网络架构，能够适应不同大小的动作和观察空间，从而实现对多种机器人形态的共同学习。

- (5): 本文在16种机器人上进行了运动策略的学习，并在2个仿真和3个真实机器人上进行了零-shot迁移，验证了方法的有效性与鲁棒性，表明该性能支持其目标的实现。


## Explainable Bayesian Optimization
- **Url**: http://arxiv.org/abs/2401.13334v2
- **Authors**: ['Tanmay Chakraborty', 'Christian Wirth', 'Christin Seifert']
- **Abstrat**: Manual parameter tuning of cyber-physical systems is a common practice, but it is labor-intensive. Bayesian Optimization (BO) offers an automated alternative, yet its black-box nature reduces trust and limits human-BO collaborative system tuning. Experts struggle to interpret BO recommendations due to the lack of explanations. This paper addresses the post-hoc BO explainability problem for cyber-physical systems. We introduce TNTRules (Tune-No-Tune Rules), a novel algorithm that provides both global and local explanations for BO recommendations. TNTRules generates actionable rules and visual graphs, identifying optimal solution bounds and ranges, as well as potential alternative solutions. Unlike existing explainable AI (XAI) methods, TNTRules is tailored specifically for BO, by encoding uncertainty via a variance pruning technique and hierarchical agglomerative clustering. A multi-objective optimization approach allows maximizing explanation quality. We evaluate TNTRules using established XAI metrics (Correctness, Completeness, and Compactness) and compare it against adapted baseline methods. The results demonstrate that TNTRules generates high-fidelity, compact, and complete explanations, significantly outperforming three baselines on 5 multi-objective testing functions and 2 hyperparameter tuning problems.


**Translated Abstract**: 

手动调整网络物理系统的参数是一种常见做法，但它劳动密集。贝叶斯优化 (Bayesian Optimization, BO) 提供了一种自动化的替代方案，但其黑箱特性降低了信任度，限制了人类与 BO 的协作系统调整。由于缺乏解释，专家在理解 BO 推荐时面临困难。本文解决了网络物理系统的后验 BO 可解释性问题。我们提出了 TNTRules（调优-不调优规则），这是一种新算法，提供 BO 推荐的全局和局部解释。TNTRules 生成可行的规则和可视化图形，识别最优解的边界和范围，以及潜在的替代解。与现有的可解释 AI (Explainable AI, XAI) 方法不同，TNTRules 专门针对 BO，采用方差修剪技术编码不确定性，并进行分层聚类。多目标优化方法可以最大化解释质量。我们使用公认的 XAI 度量（正确性、完整性和紧凑性）评估 TNTRules，并与适应的基线方法进行比较。结果表明，TNTRules 生成高保真、紧凑且完整的解释，显著优于三条基线，在 5 个多目标测试函数和 2 个超参数调整问题上表现优异。

**Summary**:

- (1): 本文研究的背景是网络物理系统的手动参数调整通常劳动密集，而贝叶斯优化（Bayesian Optimization, BO）提供了一种自动化方案，但其黑箱特性使得专家难以信任推荐结果。

- (2): 过去方法主要集中在使用贝叶斯优化进行参数调优，但由于缺乏解释，给专家带来了理解上的困难。与现有方法相比，TNTRules 通过为 BO 推荐生成全局和局部解释，提供可行的规则和可视化图形，解决了专家在后验决策中的不信任感，并能够识别最优解的边界和替代解决方案，从而确保其动机充分。

- (3): 本文的贡献在于提出了 TNTRules 算法，针对 BO 的解释问题，生成高质量、紧凑和完整的解释，从而增强了 BO 的可解释性，促进了人机协同效果。

- (4): 本文提出的研究方法包括 TNTRules 算法，该算法结合方差修剪技术和分层聚类以编码不确定性，并采用多目标优化方法提高解释质量。

- (5): 本文的算法在 5 个多目标测试函数和 2 个超参数调整问题上进行评估，表现显著优于三条基线，证明了该方法在达到其目标方面的有效性。


## Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond
- **Url**: http://arxiv.org/abs/2503.10460v3
- **Authors**: ['Liang Wen', 'Yunke Cai', 'Fenrui Xiao', 'Xin He', 'Qi An', 'Zhenyu Duan', 'Yimin Du', 'Junchen Liu', 'Lifu Tang', 'Xiaowei Lv', 'Haosheng Zou', 'Yongchao Deng', 'Shousheng Jia', 'Xiangzheng Zhang']
- **Abstrat**: This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.   Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.   Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 \& 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.   Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at https://github.com/Qihoo360/Light-R1.


**Translated Abstract**: 

本文介绍了Light-R1，这是一个开源套件，用于使用可重复和成本效益的方法训练长推理模型。考虑到DeepSeek-R1系列中使用的数据的专有性，我们开发了一种替代方法，仅利用公共数据和模型。我们的课程训练逐步增加数据的难度，并结合多阶段后训练。我们训练的Light-R1-32B模型在数学推理方面优于DeepSeek-R1-Distill-Qwen-32B。实验结果表明，当不同的、各异的数据集在不同的训练阶段可用时，这种课程方法变得更加有效：通过使用来自我们课程数据集的3000个具有挑战性的示例微调DeepSeek-R1-Distilled模型（由DeepSeek团队在专有数据上预调优）产生了最先进的7B和14B模型，而32B模型Light-R1-32B-DS的表现与QwQ-32B和DeepSeek-R1相当。此外，我们通过在长推理模型上应用GRPO扩展了我们的工作。我们的最终Light-R1-14B-DS在数学方面实现了在14B模型中的SOTA表现，AIME24和25的分数分别为74.0和60.2，超过了许多32B模型和DeepSeek-R1-Distill-Llama-70B。尽管训练集中以数学为重点，Light-R1-14B-DS仍展示了很强的跨领域泛化能力。Light-R1在使复杂推理模型更易于获取和在现实应用中实现方面代表了重要的进步。我们的模型、训练数据和代码已在https://github.com/Qihoo360/Light-R1上发布。

**Summary**:

- (1): 本文的研究背景是长链推理模型（long reasoning models）的广泛应用以及现有大型模型（如DeepSeek-R1）的高计算成本限制了其在边缘设备和实时应用中的可行性。

- (2): 过去的方法主要依赖于专有数据和单一的SFT（supervised fine-tuning）阶段，存在数据利用不充分的问题。相较现有方法，本文提出的课程训练（curriculum training）策略通过逐步提高数据的难度来增强模型的推理能力，并采用多阶段的后训练方法，解决了训练过程中的知识吸收不足的问题，动机明确。

- (3): 该论文的贡献包括一个全面的开源课程后训练方法，通过逐步增加推理能力的难度实现长推理模型的训练，并且训练成本仅为1000美元，取得了最先进的性能。

- (4): 本文提出的研究方法包括收集多样的公开推理数据集，实施两阶段的难度过滤，并配合自适应的多阶段课程训练策略，以及最终的强化学习（reinforcement learning）后训练，显著提升模型性能。

- (5): 本文在数学推理任务上取得了优异的表现，最终的Light-R1-14B-DS模型在AIME24和25的分数分别为74.0和60.2，超过了多个32B模型，其性能支持了作者所设定的目标。


## Modeling and Analyzing the Influence of Non-Item Pages on Sequential Next-Item Prediction
- **Url**: http://arxiv.org/abs/2408.15953v4
- **Authors**: ['Elisabeth Fischer', 'Albin Zehe', 'Andreas Hotho', 'Daniel Schlör']
- **Abstrat**: Analyzing sequences of interactions between users and items, sequential recommendation models can learn user intent and make predictions about the next item. Next to item interactions, most systems also have interactions with what we call non-item pages: these pages are not related to specific items but still can provide insights into the user's interests, as, for example, navigation pages. We therefore propose a general way to include these non-item pages in sequential recommendation models to enhance next-item prediction.   First, we demonstrate the influence of non-item pages on following interactions using the hypotheses testing framework HypTrails and propose methods for representing non-item pages in sequential recommendation models. Subsequently, we adapt popular sequential recommender models to integrate non-item pages and investigate their performance with different item representation strategies as well as their ability to handle noisy data. To show the general capabilities of the models to integrate non-item pages, we create a synthetic dataset for a controlled setting and then evaluate the improvements from including non-item pages on two real-world datasets.   Our results show that non-item pages are a valuable source of information, and incorporating them in sequential recommendation models increases the performance of next-item prediction across all analyzed model architectures.


**Translated Abstract**: 

分析用户与项目之间的交互序列，顺序推荐模型可以学习用户意图并预测下一个项目。除了项目交互外，大多数系统还与所谓的非项目页面进行交互：这些页面与特定项目无关，但仍能提供用户兴趣的洞察，例如导航页面。因此，我们提出了一种通用的方法，将这些非项目页面纳入顺序推荐模型，以增强下一个项目的预测。首先，我们使用假设检验框架HypTrails演示非项目页面对后续交互的影响，并提出在顺序推荐模型中表示非项目页面的方法。随后，我们调整流行的顺序推荐模型，以集成非项目页面，并调查其在不同项目表示策略下的性能以及处理噪声数据的能力。为了展示模型集成非项目页面的通用能力，我们为受控环境创建了一个合成数据集，然后在两个真实数据集上评估包含非项目页面的改进。我们的结果表明，非项目页面是一个有价值的信息来源，将其纳入顺序推荐模型能够提高所有分析模型架构中的下一个项目预测性能。

**Summary**:

- (1): 本文研究的背景是顺序推荐模型通过分析用户与项目的交互序列来学习用户意图，并进行下一项目的预测，但当前研究通常忽视与非项目页面的交互。

- (2): 以往的方法主要集中于项目之间的交互，忽略了从非项目页面获取信息的潜力，导致无法准确捕捉用户意图。提出的方法通过整合非项目页面的信息来解决这一问题，充分利用用户与非项目页面的交互对推荐的影响，且具有良好的理论动机。

- (3): 本文的贡献在于提出了一种将非项目页面纳入顺序推荐模型的新方法，并通过实验验证了其对提高下一个项目预测性能的有效性。

- (4): 本文提出的研究方法包括使用HypTrails框架分析非项目页面的影响，调整多个顺序推荐模型以集成这些页面，并创建合成数据集进行控制实验。

- (5): 在两个真实的电子商务数据集上进行的任务中，提出的方法显著提高了下一个项目的预测性能，证明了其目标的可行性。


## Hierarchical Procedural Framework for Low-latency Robot-Assisted Hand-Object Interaction
- **Url**: http://arxiv.org/abs/2405.19531v2
- **Authors**: ['Mingqi Yuan', 'Huijiang Wang', 'Kai-Fung Chu', 'Fumiya Iida', 'Bo Li', 'Wenjun Zeng']
- **Abstrat**: Advances in robotics have been driving the development of human-robot interaction (HRI) technologies. However, accurately perceiving human actions and achieving adaptive control remains a challenge in facilitating seamless coordination between human and robotic movements. In this paper, we propose a hierarchical procedural framework to enable dynamic robot-assisted hand-object interaction. An open-loop hierarchy leverages the computer vision (CV)-based 3D reconstruction of the human hand, based on which motion primitives have been designed to translate hand motions into robotic actions. The low-level coordination hierarchy fine-tunes the robot's action by using the continuously updated 3D hand models. Experimental validation demonstrates the effectiveness of the hierarchical control architecture. The adaptive coordination between human and robot behavior has achieved a delay of $\leq 0.3$ seconds in the tele-interaction scenario. A case study of ring-wearing tasks indicates the potential application of this work in assistive technologies such as healthcare and manufacturing.


**Translated Abstract**: 

近年来，机器人技术的进步推动了人机互动（HRI）技术的发展。然而，准确感知人类动作并实现自适应控制在促进人类与机器人运动之间的无缝协调方面仍然是一个挑战。本文提出了一个分层程序框架，以实现动态的机器人辅助手-物体交互。开环层次利用基于计算机视觉（CV）的3D人手重建，在此基础上设计了运动原语，将手部动作转化为机器人动作。低级协调层次通过使用持续更新的3D手模型对机器人的动作进行微调。实验验证了分层控制架构的有效性。在遥控交互场景中，人和机器人行为之间的自适应协调的延迟达到≤0.3秒。一个关于佩戴戒指任务的案例研究表明，这项工作在医疗和制造等辅助技术中的潜在应用。

**Summary**:

- (1): 本文研究的背景是机器人技术与人工智能的发展，特别是其在人机物理互动（pHRI）中的应用，这些机器人可在多种场景中（如干预治疗和医疗）用于辅助人类。

- (2): 过去的方法集中在手部姿态估计和机器人控制接口设计上，然而它们在动态环境中准确检测和跟踪手部动作存在不足。相比之下，本文提出的基于分层程序的控制框架能够利用基于计算机视觉的3D重建技术，更加准确地实时调整机器人的动作，从而改善了人与机器人之间的协调。

- (3): 本文的贡献在于提出了一个有效的分层程序框架，实现了机器人辅助手-物体交互的自适应控制，且在延迟和响应时间上表现出显著改善。

- (4): 本文提出的研究方法包括基于RGB图像的3D手姿态估计模型，通过运动原语（MPs）使机器人能够实时适应人类的动作，并借助低级控制的闭环架构实现精确操作。

- (5): 本文在佩戴戒指任务中进行了实验，达到的性能指标是≤0.3秒的交互延迟，这一表现支持了作者提出的在医疗和制造等领域应用的目标。


## Semantic Learning for Molecular Communication in Internet of Bio-Nano Things
- **Url**: http://arxiv.org/abs/2502.08426v2
- **Authors**: ['Hanlin Cai', 'Ozgur B. Akan']
- **Abstrat**: Molecular communication (MC) provides a foundational framework for information transmission in the Internet of Bio-Nano Things (IoBNT), where efficiency and reliability are crucial. However, the inherent limitations of molecular channels, such as low transmission rates, noise, and intersymbol interference (ISI), limit their ability to support complex data transmission. This paper proposes an end-to-end semantic learning framework designed to optimize task-oriented molecular communication, with a focus on biomedical diagnostic tasks under resource-constrained conditions. The proposed framework employs a deep encoder-decoder architecture to efficiently extract, quantize, and decode semantic features, prioritizing taskrelevant semantic information to enhance diagnostic classification performance. Additionally, a probabilistic channel network is introduced to approximate molecular propagation dynamics, enabling gradient-based optimization for end-to-end learning. Experimental results demonstrate that the proposed semantic framework improves diagnostic accuracy by at least 25% compared to conventional JPEG compression with LDPC coding methods under resource-constrained communication scenarios.


**Translated Abstract**: 

分子通信（MC）为生物纳米物联网（IoBNT）中的信息传输提供了基础框架，其中效率和可靠性至关重要。然而，分子信道固有的限制，如低传输速率、噪声和符号间干扰（ISI），限制了它们支持复杂数据传输的能力。本文提出了一种端到端的语义学习框架，旨在优化以任务为导向的分子通信，重点关注资源受限条件下的生物医学诊断任务。所提出的框架采用深度编码器-解码器架构，有效提取、量化和解码语义特征，优先考虑任务相关的语义信息以增强诊断分类性能。此外，还引入了一种概率信道网络，以逼近分子传播动态，支持端到端学习的基于梯度的优化。实验结果表明，所提出的语义框架在资源受限的通信场景下，诊断准确性至少比传统的JPEG压缩和低密度奇偶校验（LDPC）编码方法提高25%。

**Summary**:

- (1): 本文的研究背景是分子通信（MC）作为生物纳米物联网（IoBNT）中的信息传输方案，面临着低速率、高噪声和符号间干扰等挑战。

- (2): 过去的方法主要是基于传统的编码和错误校正，存在无法有效处理复杂数据的局限性。提出的方法与现有方法的不同之处在于采用端到端的语义学习框架，优先考虑任务相关的语义信息，有效解决了现有方法在映射语义信息和传输分子参数时存在的问题。方法的提出是合理的，针对实际应用需求提供了创新的解决方案。

- (3): 本文的贡献在于提出了一种新的语义分子通信框架，通过深度学习技术有效提升了在资源受限情况下的诊断分类性能。

- (4): 研究方法包括使用深度编码器-解码器架构来提取、量化和解码任务相关的语义特征，同时引入概率信道网络以建模分子传播的随机动态，从而实现端到端的优化。

- (5): 本文的方法在生物医学诊断任务上取得了至少25%的准确性提升，相较于传统的压缩和编码方法，其性能表现有效支撑了研究目标。


## RedMotion: Motion Prediction via Redundancy Reduction
- **Url**: http://arxiv.org/abs/2306.10840v4
- **Authors**: ['Royden Wagner', 'Omer Sahin Tas', 'Marvin Klemp', 'Carlos Fernandez', 'Christoph Stiller']
- **Abstrat**: We introduce RedMotion, a transformer model for motion prediction in self-driving vehicles that learns environment representations via redundancy reduction. Our first type of redundancy reduction is induced by an internal transformer decoder and reduces a variable-sized set of local road environment tokens, representing road graphs and agent data, to a fixed-sized global embedding. The second type of redundancy reduction is obtained by self-supervised learning and applies the redundancy reduction principle to embeddings generated from augmented views of road environments. Our experiments reveal that our representation learning approach outperforms PreTraM, Traj-MAE, and GraphDINO in a semi-supervised setting. Moreover, RedMotion achieves competitive results compared to HPTR or MTR++ in the Waymo Motion Prediction Challenge. Our open-source implementation is available at: https://github.com/kit-mrt/future-motion


**Translated Abstract**:
我们介绍了 RedMotion，这是一种用于自驾车运动预测的变换器模型，通过减少冗余性来学习环境表示。我们的第一种冗余减少方式是由内部变换器解码器引导的，将表示道路图和代理数据的可变大小本地道路环境标记集合减少为固定大小的全局嵌入。第二种冗余减少是通过自监督学习获得的，并将冗余减少原理应用于从增强视图生成的嵌入。我们的实验表明，我们的表示学习方法在半监督设置中优于 PreTraM、Traj-MAE 和 GraphDINO。此外，RedMotion 在 Waymo 运动预测挑战赛中取得了与 HPTR 和 MTR++ 相当的成绩。我们的开源实现可用在：https://github.com/kit-mrt/future-motion。

**Summary**:

- (1): 本文研究的背景是自驾车需要理解交通参与者的运动与周围道路环境之间的关系，运动预测旨在基于过去的轨迹和给定的交通场景预测未来的轨迹。

- (2): 过去的方法主要是基于监督学习的深度学习模型，比如 Shi et al. (2022)、Wang et al. (2023) 等，但这需要大量标签数据，且自监督学习在运动预测领域刚开始应用。提出的方法通过冗余性减少来提升环境表示的学习，有效解决了传统方法依赖大量标注数据的问题，并具有较好的动机。

- (3): 论文的贡献在于提出了两种冗余减少机制，提供了丰富的上下文表示以支持运动预测。

- (4): 本文提出的研究方法论是利用红冗余减少原则，结合变换器结构，对本地和全局道路环境进行有效的嵌入学习，从而捕捉道路环境特征。

- (5): 在 Waymo 运动预测挑战中，所提出的方法展示了与 HPTR 和 MTR++ 的竞争性结果，说明其在运动预测任务上达到了预期的性能目标。


## Patient-specific prediction of glioblastoma growth via reduced order modeling and neural networks
- **Url**: http://arxiv.org/abs/2412.05330v2
- **Authors**: ['D. Cerrone', 'D. Riccobelli', 'S. Gazzoni', 'P. Vitullo', 'F. Ballarin', 'J. Falco', 'F. Acerbi', 'A. Manzoni', 'P. Zunino', 'P. Ciarletta']
- **Abstrat**: Glioblastoma is among the most aggressive brain tumors in adults, characterized by patient-specific invasion patterns driven by the underlying brain microstructure. In this work, we present a proof-of-concept for a mathematical model of GBL growth, enabling real-time prediction and patient-specific parameter identification from longitudinal neuroimaging data.   The framework exploits a diffuse-interface mathematical model to describe the tumor evolution and a reduced-order modeling strategy, relying on proper orthogonal decomposition, trained on synthetic data derived from patient-specific brain anatomies reconstructed from magnetic resonance imaging and diffusion tensor imaging. A neural network surrogate learns the inverse mapping from tumor evolution to model parameters, achieving significant computational speed-up while preserving high accuracy.   To ensure robustness and interpretability, we perform both global and local sensitivity analyses, identifying the key biophysical parameters governing tumor dynamics and assessing the stability of the inverse problem solution. These results establish a methodological foundation for future clinical deployment of patient-specific digital twins in neuro-oncology.


**Translated Abstract**: 

胶质母细胞瘤（GBL）是成人中最具侵袭性的脑肿瘤之一，其侵袭模式由特定患者的脑微环境驱动。在本研究中，我们提出了一种胶质母细胞瘤生长的数学模型概念验证，能够从纵向神经影像数据中实时预测和识别特定患者的参数。该框架利用扩散界面数学模型描述肿瘤演变，并依靠基于正交分解的降阶建模策略，针对从磁共振成像和扩散张量成像重建的特定患者脑解剖结构派生的合成数据进行训练。神经网络代理学习肿瘤演化与模型参数之间的反向映射，在保持高精度的同时实现显著的计算速度提升。为了确保鲁棒性和可解释性，我们进行了全局和局部灵敏度分析，识别出影响肿瘤动态的关键生物物理参数，并评估反问题解的稳定性。这些结果为未来在神经肿瘤学领域部署特定患者数字双胞胎的临床应用奠定了方法论基础。

**Summary**:

- (1): 胶质母细胞瘤（GBL）是最具侵袭性的脑肿瘤之一，患者在治疗后肿瘤复发率高，加上现有技术仍难以实现完全切除，急需发展新的方法以提高治疗效果。

- (2): 过去的研究采用了各种数学建模方法，如常微分方程、偏微分方程和基于代理的模型，但这些方法在处理个体患者的动态预测时存在一定的局限性。与之不同的是，本文提出了一种结合扩散界面模型与机器学习的降阶建模策略，能够快速估计个体患者的参数并预测肿瘤演变，从而解决了现有方法的计算复杂度和准确性问题。

- (3): 本文的贡献在于提出了一种新的计算框架，结合了降阶数学模型和深度学习，解决了胶质母细胞瘤生长的个体化预测及参数识别的挑战，为临床决策提供了一种新工具。

- (4): 本文的方法论包括建立一个基于扩散界面偏微分方程的数学模型，并利用proper orthogonal decomposition进行降阶建模，结合神经网络优化反向映射，进行生物物理参数的灵敏度分析。

- (5): 本文针对胶质母细胞瘤生长的个体化预测任务实现了显著的计算速度提升和高准确率，这些性能指标表明该方法能够支持其在实际临床环境中的应用目标。


## The Computational Complexity of Circuit Discovery for Inner Interpretability
- **Url**: http://arxiv.org/abs/2410.08025v3
- **Authors**: ['Federico Adolfi', 'Martina G. Vilas', 'Todd Wareham']
- **Abstrat**: Many proposed applications of neural networks in machine learning, cognitive/brain science, and society hinge on the feasibility of inner interpretability via circuit discovery. This calls for empirical and theoretical explorations of viable algorithmic options. Despite advances in the design and testing of heuristics, there are concerns about their scalability and faithfulness at a time when we lack understanding of the complexity properties of the problems they are deployed to solve. To address this, we study circuit discovery with classical and parameterized computational complexity theory: (1) we describe a conceptual scaffolding to reason about circuit finding queries in terms of affordances for description, explanation, prediction and control; (2) we formalize a comprehensive set of queries for mechanistic explanation, and propose a formal framework for their analysis; (3) we use it to settle the complexity of many query variants and relaxations of practical interest on multi-layer perceptrons. Our findings reveal a challenging complexity landscape. Many queries are intractable, remain fixed-parameter intractable relative to model/circuit features, and inapproximable under additive, multiplicative, and probabilistic approximation schemes. To navigate this landscape, we prove there exist transformations to tackle some of these hard problems with better-understood heuristics, and prove the tractability or fixed-parameter tractability of more modest queries which retain useful affordances. This framework allows us to understand the scope and limits of interpretability queries, explore viable options, and compare their resource demands on existing and future architectures.


**Translated Abstract**: 

许多神经网络在机器学习、认知/脑科学和社会中的应用都依赖于通过电路发现实现的内部可解释性。这需要对可行的算法选项进行经验和理论探索。尽管在启发式设计和测试方面取得了一些进展，但在缺乏对它们所解决问题的复杂性属性理解的情况下，人们对它们的可扩展性和忠实性存在疑虑。为了解决这个问题，我们使用经典和参数化计算复杂性理论研究电路发现： (1) 我们描述了一种概念框架，以便根据描述、解释、预测和控制的能力来推理电路发现查询； (2) 我们正式化了一套全面的机制解释查询，提出了其分析的正式框架； (3) 我们利用该框架解决了多层感知机中许多查询变体的复杂性问题及其放松。我们的研究结果揭示了一个具有挑战性的复杂性景观。许多查询是不可解的，相对于模型/电路特征保持固定参数不可解，并且在加法、乘法和概率近似方案下不可近似。为了应对这一复杂性景观，我们证明存在转化方法，可以通过更好理解的启发式方法解决一些这些难题，并证明了保留有用能力的更适度查询的可解性或固定参数可解性。该框架使我们能够理解可解释性查询的范围和限制，探索可行选项，并比较它们在现有和未来架构上的资源需求。

**Summary**:

- (1): 本文的研究背景是：随着人工神经网络（ANN）规模和能力的增长，内部可解释性成为理解其机制的重要领域，尤其在机器学习和认知科学中应用广泛。

- (2): 过去的方法主要是基于启发式算法，这些方法在可扩展性和忠实性方面存在问题。与现有方法不同，本文提出一种基于计算复杂性理论的分析框架，能够系统性地评估和界定查询的复杂性，从而解决这些问题。这个方法是经过理论支持的，具有较强的动机。

- (3): 本文的贡献在于提出了一个全面的框架用于分析电路发现查询的复杂性，明确了解释查询的能力范围与局限，并指出了一些查询是不可解的。

- (4): 本文的研究方法论是：构建一个概念框架，形式化机制解释查询的集合，并利用计算复杂性理论分析多层感知机上的查询复杂性。

- (5): 本文的任务是在多层感知机上分析各种查询的复杂性，其结果显示出许多查询的不可解性或固定参数不可解性，表明所提出的框架能够支持理解可解释性查询的范围和限制。


## TOBUGraph: Knowledge Graph-Based Retrieval for Enhanced LLM Performance Beyond RAG
- **Url**: http://arxiv.org/abs/2412.05447v2
- **Authors**: ['Savini Kashmira', 'Jayanaka L. Dantanarayana', 'Joshua Brodsky', 'Ashish Mahendra', 'Yiping Kang', 'Krisztian Flautner', 'Lingjia Tang', 'Jason Mars']
- **Abstrat**: Retrieval-Augmented Generation (RAG) is one of the leading and most widely used techniques for enhancing LLM retrieval capabilities, but it still faces significant limitations in commercial use cases. RAG primarily relies on the query-chunk text-to-text similarity in the embedding space for retrieval and can fail to capture deeper semantic relationships across chunks, is highly sensitive to chunking strategies, and is prone to hallucinations. To address these challenges, we propose TOBUGraph, a graph-based retrieval framework that first constructs the knowledge graph from unstructured data dynamically and automatically. Using LLMs, TOBUGraph extracts structured knowledge and diverse relationships among data, going beyond RAG's text-to-text similarity. Retrieval is achieved through graph traversal, leveraging the extracted relationships and structures to enhance retrieval accuracy, eliminating the need for chunking configurations while reducing hallucination. We demonstrate TOBUGraph's effectiveness in TOBU, a real-world application in production for personal memory organization and retrieval. Our evaluation using real user data demonstrates that TOBUGraph outperforms multiple RAG implementations in both precision and recall, significantly improving user experience through improved retrieval accuracy.


**Translated Abstract**: 

检索增强生成（RAG）是提升大语言模型（LLM）检索能力的领先技术之一，但在商业应用中仍面临重大限制。RAG主要依赖于查询与块文本之间在嵌入空间内的文本到文本相似性进行检索，无法捕捉块之间更深层次的语义关系，对块划分策略高度敏感，并且容易出现幻觉。为了解决这些挑战，我们提出了TOBUGraph，这是一种基于图的检索框架，它动态且自动地从非结构化数据中构建知识图。使用LLM，TOBUGraph提取结构化知识和数据之间的多样关系，超越了RAG的文本到文本相似性。检索通过图遍历实现，利用提取的关系和结构来提高检索精度，消除对块划分配置的需求，同时减少幻觉。我们在TOBU（一个用于个人记忆组织和检索的生产应用）中展示了TOBUGraph的有效性。对真实用户数据的评估表明，TOBUGraph在精确度和召回率方面超过了多种RAG实现，通过改进检索准确性显著提升用户体验。

**Summary**:

- (1): 本文的研究背景是当前的检索增强生成（RAG）技术在商业应用中存在的显著限制，尤其是在语义关系捕获和幻觉问题上。

- (2): 过去的方法主要是RAG，通过查询与文本块的相似性进行检索，但存在无法捕捉文本块之间语义关系、对块划分策略敏感、及幻觉问题等缺点。本文提出的TOBUGraph方法通过动态构建知识图，提取和利用多样的语义关系，克服了RAG的局限性，具有良好的动机。

- (3): 本文的贡献包括：提出了一种新颖的方法，通过图结构提取非结构化数据的知识和关系；开发了一种有效的基于知识图的检索机制；在实际应用TOBU中实现了此方法；并且通过真实用户数据对比评估了其优越性。

- (4): 本文提出的研究方法是TOBUGraph，它利用LLM自动从非结构化数据中构建知识图，并通过图遍历进行有效的记忆检索。

- (5): 本文在TOBU应用中实现了个人记忆的组织和检索，性能指标为93.74%的精确度和91.96%的召回率，超过了多个RAG基线实现的表现，足以支持其改善用户体验的目标。


## Innovative LSGTime Model for Crime Spatiotemporal Prediction Based on MindSpore Framework
- **Url**: http://arxiv.org/abs/2503.20136v3
- **Authors**: ['Zhenkai Qin', 'BaoZhong Wei', 'Caifeng Gao']
- **Abstrat**: With the acceleration of urbanization, the spatiotemporal characteristics of criminal activities have become increasingly complex. Accurate prediction of crime distribution is crucial for optimizing the allocation of police resources and preventing crime. This paper proposes LGSTime, a crime spatiotemporal prediction model that integrates Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and the Multi-head Sparse Self-attention mechanism. LSTM and GRU capture long-term dependencies in crime time series, such as seasonality and periodicity, through their unique gating mechanisms. The Multi-head Sparse Self-attention mechanism, on the other hand, focuses on both temporal and spatial features of criminal events simultaneously through parallel processing and sparsification techniques, significantly improving computational efficiency and prediction accuracy. The integrated model leverages the strengths of each technique to better handle complex spatiotemporal data. Experimental findings demonstrate that the model attains optimal performance across four real - world crime datasets. In comparison to the CNN model, it exhibits performance enhancements of 2.8\%, 1.9\%, and 1.4\% in the Mean Squared Error (MSE), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) metrics respectively. These results offer a valuable reference for tackling the challenges in crime prediction.


**Translated Abstract**: 

随着城市化进程的加速，犯罪活动的时空特征变得愈加复杂。准确预测犯罪分布对于优化警力资源配置和预防犯罪至关重要。本文提出了LGSTime，一种集成长短期记忆（LSTM）、门控循环单元（GRU）和多头稀疏自注意力机制的犯罪时空预测模型。LSTM和GRU通过其独特的门控机制捕捉犯罪时间序列中的长期依赖性，如季节性和周期性。另一方面，多头稀疏自注意力机制通过并行处理和稀疏化技术同时关注犯罪事件的时间和空间特征，显著提高了计算效率和预测准确性。集成模型利用了每种技术的优势，更好地处理复杂的时空数据。实验证明，该模型在四个真实世界的犯罪数据集中达到了最佳性能。与CNN模型相比，在均方误差（MSE）、平均绝对误差（MAE）和均方根误差（RMSE）指标上分别提高了2.8%、1.9%和1.4%。这些结果为应对犯罪预测中的挑战提供了宝贵的参考。

**Summary**:

- (1): 本文研究背景是随着城市化进程加速，犯罪活动的时空特征日益复杂，准确预测犯罪分布对优化警力资源配置和维护社会安全至关重要。

- (2): 过去的方法主要包括长短期记忆（LSTM）和门控循环单元（GRU），这些方法在捕捉长期依赖性方面表现良好，但存在计算资源需求大、对数据噪声敏感、处理大规模数据能力差等问题。不同于这些方法，本文提出的LGSTime模型将LSTM、GRU与多头稀疏自注意力机制相结合，解决了映射时空特征的不足，增强了对复杂时空数据的处理能力。

- (3): 本文的贡献在于提出了一种集成LSTM、GRU和多头稀疏自注意力机制的犯罪时空预测模型，显著提升了对复杂时空依赖性捕获的能力，并改善了模型的计算效率和预测准确性。

- (4): 研究方法包括结合LSTM、GRU的长期依赖性捕获能力以及多头稀疏自注意力机制的并行处理和特征关注能力，构建了更为强大的犯罪时空预测模型。

- (5): 本文的方法在四个真实世界的犯罪数据集上进行了验证，取得了较高的预测准确性，证明这一表现能够支撑其研究目标。


## Improving Vector-Quantized Image Modeling with Latent Consistency-Matching Diffusion
- **Url**: http://arxiv.org/abs/2410.14758v2
- **Authors**: ['Bac Nguyen', 'Chieh-Hsin Lai', 'Yuhta Takida', 'Naoki Murata', 'Toshimitsu Uesaka', 'Stefano Ermon', 'Yuki Mitsufuji']
- **Abstrat**: By embedding discrete representations into a continuous latent space, we can leverage continuous-space latent diffusion models to handle generative modeling of discrete data. However, despite their initial success, most latent diffusion methods rely on fixed pretrained embeddings, limiting the benefits of joint training with the diffusion model. While jointly learning the embedding (via reconstruction loss) and the latent diffusion model (via score matching loss) could enhance performance, end-to-end training risks embedding collapse, degrading generation quality. To mitigate this issue, we introduce VQ-LCMD, a continuous-space latent diffusion framework within the embedding space that stabilizes training. VQ-LCMD uses a novel training objective combining the joint embedding-diffusion variational lower bound with a consistency-matching (CM) loss, alongside a shifted cosine noise schedule and random dropping strategy. Experiments on several benchmarks show that the proposed VQ-LCMD yields superior results on FFHQ, LSUN Churches, and LSUN Bedrooms compared to discrete-state latent diffusion models. In particular, VQ-LCMD achieves an FID of 6.81 for class-conditional image generation on ImageNet with 50 steps.


**Translated Abstract**:  
通过将离散表示嵌入到连续潜在空间中，我们可以利用连续空间潜在扩散模型来处理离散数据的生成建模。然而，尽管起初取得了一定成功，大多数潜在扩散方法依赖于固定的预训练嵌入，这限制了与扩散模型的联合训练的益处。虽然通过重建损失共同学习嵌入（embedding）和潜在扩散模型（通过得分匹配损失）可以提升表现，但端到端的训练存在嵌入崩溃的风险，从而降低生成质量。为解决此问题，我们引入了VQ-LCMD，这是一种在嵌入空间中的连续潜在扩散框架，可以稳定训练。VQ-LCMD使用了一种新颖的训练目标，结合了联合嵌入-扩散变分下界和一致性匹配（CM）损失，以及移位余弦噪声调度和随机丢弃策略。实验表明，VQ-LCMD在FFHQ、LSUN Churches和LSUN Bedrooms等多个基准上，较离散状态潜在扩散模型产生了优越的结果。特别是在ImageNet上进行类条件图像生成时，VQ-LCMD以50步达到6.81的FID。

**Summary**:

- (1): 本文研究的背景是利用离散潜在表示生成图像，并探讨将其嵌入到连续潜在空间中的方法。

- (2): 过去的方法主要是使用固定的预训练嵌入进行潜在扩散，存在嵌入崩溃和生成质量降低等问题。提出的方法VQ-LCMD通过联合训练嵌入和扩散模型，结合了一致性匹配损失解决了这些问题，具有较好的动机基础。

- (3): 论文的贡献在于提出了VQ-LCMD框架，实现了在生成离散数据时性能的显著提升，并稳定了训练过程。

- (4): 本文提出的研究方法是在嵌入空间中使用VQ-LCMD框架，结合联结嵌入-扩散变分下界和一致性匹配损失，创新性地引入移位余弦噪声调度和随机丢弃策略。

- (5): 本文的方法在类条件图像生成任务上实现了6.81的FID性能，支持了其最终目标的实现。


## Statistically Testing Training Data for Unwanted Error Patterns using Rule-Oriented Regression
- **Url**: http://arxiv.org/abs/2503.18497v2
- **Authors**: ['Stefan Rass', 'Martin Dallinger']
- **Abstrat**: Artificial intelligence models trained from data can only be as good as the underlying data is. Biases in training data propagating through to the output of a machine learning model are a well-documented and well-understood phenomenon, but the machinery to prevent these undesired effects is much less developed. Efforts to ensure data is clean during collection, such as using bias-aware sampling, are most effective when the entity controlling data collection also trains the AI. In cases where the data is already available, how do we find out if the data was already manipulated, i.e., ``poisoned'', so that an undesired behavior would be trained into a machine learning model? This is a challenge fundamentally different to (just) improving approximation accuracy or efficiency, and we provide a method to test training data for flaws, to establish a trustworthy ground-truth for a subsequent training of machine learning models (of any kind). Unlike the well-studied problem of approximating data using fuzzy rules that are generated from the data, our method hinges on a prior definition of rules to happen before seeing the data to be tested. Therefore, the proposed method can also discover hidden error patterns, which may also have substantial influence. Our approach extends the abilities of conventional statistical testing by letting the ``test-condition'' be any Boolean condition to describe a pattern in the data, whose presence we wish to determine. The method puts fuzzy inference into a regression model, to get the best of the two: explainability from fuzzy logic with statistical properties and diagnostics from the regression, and finally also being applicable to ``small data'', hence not requiring large datasets as deep learning methods do. We provide an open source implementation for demonstration and experiments.


**Translated Abstract**: 

人工智能模型的训练效果取决于底层数据的质量。训练数据中的偏见会传递到机器学习模型的输出中，这一现象已得到充分记录和理解，但防止这些不良影响的机制却相对不成熟。在数据已可用的情况下，如何判断数据是否已被操纵、即“污染”，以至于机器学习模型会学习到不希望出现的行为，是一个与单纯提升近似精度或效率完全不同的挑战。我们提供了一种测试训练数据缺陷的方法，以建立可信的真实基础，以进行后续的机器学习模型训练。与通过数据生成模糊规则来近似数据的已研究问题不同，我们的方法基于在看到待测试数据之前的规则预定义。因此，提出的方法也可以发现隐藏的错误模式，这可能会产生重要影响。我们的方法通过让“测试条件”成为任何布尔条件，以描述我们希望确定的模式，扩展了传统统计检验的能力。此方法将模糊推理引入回归模型中，结合了模糊逻辑的可解释性和回归的统计属性及诊断能力，并且适用于“小数据”，因此不需要大规模数据集，如深度学习方法。我们提供了一个开源实现以供演示和实验。

**Summary**:

- (1): 本文的研究背景是训练数据的质量严重影响人工智能模型的成功，特别是在训练数据中存在偏见时，可能导致模型输出不希望的结果。

- (2): 目前的方法主要集中在通过手动标记来评估数据质量，但存在人工评估的主观性和效率低的问题。提出的方法与现有的方法的不同之处在于依赖于事先定义的规则进行测试，而不是基于数据生成模糊规则，从而能够发现潜在的错误模式，且提升了检测的客观性和效率。

- (3): 本文的贡献在于提供了一种新的方法来测试训练数据的缺陷和确定其可用性，扩展了传统统计检验的能力，提供了可解释的预测模型。

- (4): 本文提出的研究方法是将模糊推理与回归模型结合，通过事先定义的规则来测试数据，以发现数据中可能的错误模式和偏见。

- (5): 本文的方法适用于小规模数据集，并通过开源实现进行实验，能够识别出数据中的不良模式，支持研究目标，但具体任务和性能指标在文献中并未详细描述。


## MSCMNet: Multi-scale Semantic Correlation Mining for Visible-Infrared Person Re-Identification
- **Url**: http://arxiv.org/abs/2311.14395v2
- **Authors**: ['Xuecheng Hua', 'Ke Cheng', 'Hu Lu', 'Juanjuan Tu', 'Yuanquan Wang', 'Shitong Wang']
- **Abstrat**: The main challenge in the Visible-Infrared Person Re-Identification (VI-ReID) task lies in how to extract discriminative features from different modalities for matching purposes. While the existing well works primarily focus on minimizing the modal discrepancies, the modality information can not thoroughly be leveraged. To solve this problem, a Multi-scale Semantic Correlation Mining network (MSCMNet) is proposed to comprehensively exploit semantic features at multiple scales and simultaneously reduce modality information loss as small as possible in feature extraction. The proposed network contains three novel components. Firstly, after taking into account the effective utilization of modality information, the Multi-scale Information Correlation Mining Block (MIMB) is designed to explore semantic correlations across multiple scales. Secondly, in order to enrich the semantic information that MIMB can utilize, a quadruple-stream feature extractor (QFE) with non-shared parameters is specifically designed to extract information from different dimensions of the dataset. Finally, the Quadruple Center Triplet Loss (QCT) is further proposed to address the information discrepancy in the comprehensive features. Extensive experiments on the SYSU-MM01, RegDB, and LLCM datasets demonstrate that the proposed MSCMNet achieves the greatest accuracy.


**Translated Abstract**:

在可见-红外行人重识别（VI-ReID）任务中，主要挑战在于如何从不同模态中提取具有区分性的特征以进行匹配。目前的研究主要集中于最小化模态间的差异，而未能充分利用模态信息。为了解决这个问题，提出了一种多尺度语义关联挖掘网络（MSCMNet），旨在综合利用多尺度的语义特征，并在特征提取中尽可能减少模态信息损失。该网络包含三个新颖的组件。首先，设计了多尺度信息关联挖掘模块（MIMB）以探索不同尺度间的语义关联。其次，特别设计了一个四通道特征提取器（QFE），用于从数据集的不同维度提取信息。最后，提出了四重心三元损失（QCT），用以解决综合特征中的信息差异。在SYSU-MM01、RegDB和LLCM数据集上的大量实验表明，所提出的MSCMNet达到了最佳准确率。

**Summary**:

- (1): 本文的研究背景是可见-红外行人重识别（VI-ReID），即在非重叠摄像头拍摄的图像中检索和识别个人。

- (2): 过去的方法大多集中在利用双流网络提取跨模态共享特征，存在的问题包括红外模态缺乏颜色和纹理信息，特征提取过程中模态信息损失，以及由于相机视角、穿着和遮挡等导致的特征提取困难。拟提出的方法通过多尺度的信息关联挖掘（MIMB）和四通道特征提取（QFE），以及四重心三元损失（QCT），有效解决了这些问题，动机充分。

- (3): 本文的贡献在于提出了MSCMNet，能够在特征提取中更有效地利用模态信息，并显著提高了可见-红外行人重识别的准确率。

- (4): 本文提出的研究方法包括多尺度信息关联挖掘模块（MIMB）、四通道特征提取器（QFE）和四重心三元损失（QCT），通过这些模块实现对模态特征的综合挖掘和优化。

- (5): 本文在SYSU-MM01、RegDB和LLCM数据集上进行实验，所实现的性能表明所提出的方法在行人重识别任务中达到了最佳准确率，支持了其研究目标。


## Class-Dependent Perturbation Effects in Evaluating Time Series Attributions
- **Url**: http://arxiv.org/abs/2502.17022v2
- **Authors**: ['Gregor Baer', 'Isel Grau', 'Chao Zhang', 'Pieter Van Gorp']
- **Abstrat**: As machine learning models become increasingly prevalent in time series applications, Explainable Artificial Intelligence (XAI) methods are essential for understanding their predictions. Within XAI, feature attribution methods aim to identify which input features contribute the most to a model's prediction, with their evaluation typically relying on perturbation-based metrics. Through systematic empirical analysis across multiple datasets, model architectures, and perturbation strategies, we reveal previously overlooked class-dependent effects in these metrics: they show varying effectiveness across classes, achieving strong results for some while remaining less sensitive to others. In particular, we find that the most effective perturbation strategies often demonstrate the most pronounced class differences. Our analysis suggests that these effects arise from the learned biases of classifiers, indicating that perturbation-based evaluation may reflect specific model behaviors rather than intrinsic attribution quality. We propose an evaluation framework with a class-aware penalty term to help assess and account for these effects in evaluating feature attributions, offering particular value for class-imbalanced datasets. Although our analysis focuses on time series classification, these class-dependent effects likely extend to other structured data domains where perturbation-based evaluation is common.


**Translated Abstract**: 

随着机器学习模型在时间序列应用中的日益普及，解释性人工智能（XAI）方法对于理解其预测变得至关重要。在XAI中，特征归因方法旨在识别哪些输入特征对模型预测贡献最大，其评估通常依赖于基于扰动的度量。通过对多个数据集、模型架构和扰动策略的系统实证分析，我们揭示了这些度量中以前被忽视的类依赖效应：它们在不同类上表现出的有效性不同，对某些类效果显著，而对其他类则不那么敏感。特别地，我们发现最有效的扰动策略往往表现出最显著的类差异。我们的分析表明，这些效应源于分类器的学习偏差，意味着基于扰动的评估可能反映特定模型行为而不是内在的归因质量。我们提出了一种带有类感知惩罚项的评估框架，帮助评估和考虑在评估特征归因时的这些效应，对于类不平衡的数据集特别有价值。尽管我们的分析侧重于时间序列分类，但这些类依赖效应可能延伸到其他常见的结构化数据领域。

**Summary**:

- (1): 本文研究背景为机器学习模型在时间序列应用中的广泛使用，强调了解释性人工智能（XAI）方法对于理解模型预测的重要性。

- (2): 过去的方法主要是基于扰动的度量来评估特征归因，但存在评估效果因类差异而不均的缺陷。与现有方法不同，本文提出的框架引入类感知惩罚项，以解决这一问题，从而提供更可靠的评估。

- (3): 本文的贡献在于揭示了类依赖的扰动效应，提出了一种新的评估框架，增强了对特征归因的理解，尤其是在类不平衡的数据集上。

- (4): 本文的研究方法通过系统的实证分析，评估了不同数据集和扰动策略对特征归因评估的影响，着重分析类依赖效应的表现。

- (5): 本文的研究主要集中在时间序列分类任务上，尽管具体性能指标未详细给出，但提出的方法能够支持其评估目标，尤其是在考虑类不平衡时。


## Optimization Insights into Deep Diagonal Linear Networks
- **Url**: http://arxiv.org/abs/2412.16765v2
- **Authors**: ['Hippolyte Labarrière', 'Cesare Molinari', 'Lorenzo Rosasco', 'Silvia Villa', 'Cristian Vega']
- **Abstrat**: Overparameterized models trained with (stochastic) gradient descent are ubiquitous in modern machine learning. These large models achieve unprecedented performance on test data, but their theoretical understanding is still limited. In this paper, we take a step towards filling this gap by adopting an optimization perspective. More precisely, we study the implicit regularization properties of the gradient flow "algorithm" for estimating the parameters of a deep diagonal neural network. Our main contribution is showing that this gradient flow induces a mirror flow dynamic on the model, meaning that it is biased towards a specific solution of the problem depending on the initialization of the network. Along the way, we prove several properties of the trajectory.


**Translated Abstract**: 

在现代机器学习中，利用（随机）梯度下降训练的过参数化模型随处可见。这些大型模型在测试数据上取得了前所未有的表现，但它们的理论理解仍然有限。本文通过采用优化视角，迈出了填补这一空白的步骤。具体而言，我们研究了用于估计深度对角神经网络参数的梯度流“算法”的隐性正则化特性。我们主要的贡献是展示了这种梯度流在模型上诱导了一种镜面流动态，这意味着它由于网络的初始化而偏向问题的特定解。在此过程中，我们证明了轨迹的多个性质。

**Summary**:

- (1): 本文的研究背景是深度网络在处理复杂数据（如图像和自然语言）方面的成功，但对其理论理解相对有限。

- (2): 过去的方法主要是通过选择简单模型或显式添加惩罚项来实现正则化，然而深度神经网络仅通过最小化经验风险进行训练，缺乏明确的正则化机制。本文提出的方法从优化过程的角度出发，展示了训练动态本身具有自我正则化的特性，优先选择具有良好推广能力的解。该方法能够有效解释深度学习中隐性正则化的机制，具有坚实的动机基础。

- (3): 本文的贡献在于揭示了梯度流在深度对角神经网络中的隐性正则化特性，以及它如何根据网络初始化偏向特定解决方案。

- (4): 本文采用的研究方法是通过分析深度对角神经网络的梯度流动态，揭示其诱导的镜面流特性，并证明了轨迹的多个相关性质。

- (5): 本文未具体描述任务和性能指标，因此难以评估其实现的性能是否支持其目标。


## A stochastic gradient descent algorithm with random search directions
- **Url**: http://arxiv.org/abs/2503.19942v2
- **Authors**: ['Eméric Gbaguidi']
- **Abstrat**: Stochastic coordinate descent algorithms are efficient methods in which each iterate is obtained by fixing most coordinates at their values from the current iteration, and approximately minimizing the objective with respect to the remaining coordinates. However, this approach is usually restricted to canonical basis vectors of $\mathbb{R}^d$. In this paper, we develop a new class of stochastic gradient descent algorithms with random search directions which uses the directional derivative of the gradient estimate following more general random vectors. We establish the almost sure convergence of these algorithms with decreasing step. We further investigate their central limit theorem and pay particular attention to analyze the impact of the search distributions on the asymptotic covariance matrix. We also provide non-asymptotic $\mathbb{L}^p$ rates of convergence.


**Translated Abstract**: 

随机坐标下降算法是有效的方法，每次迭代通过固定大多数坐标的当前值，近似最小化与剩余坐标相关的目标。然而，这种方法通常仅限于$\mathbb{R}^d$的标准基向量。本文提出了一种新类的带有随机搜索方向的随机梯度下降算法，该算法使用更一般随机向量的梯度估计的方向导数。我们建立了这些具有递减步长算法的几乎必然收敛性，从而调查其中心极限定理，特别关注研究搜索分布对渐近协方差矩阵的影响。我们还提供了非渐近的$\mathbb{L}^p$收敛率。

**Summary**:

- (1): 本文研究背景为优化问题在高维空间中的广泛应用，特别是处理大规模机器学习问题时的计算效率。

- (2): 过去的方法主要是标准随机梯度下降（SGD）和随机坐标梯度下降（SCGD）算法。这些方法仍然需要在每次迭代中计算大小为d的向量，计算成本高。本文的方法与现有方法不同，提出了一种带有随机搜索方向的随机梯度下降算法，可使用更一般的随机向量，解决了计算高维梯度的效率问题，具有明确的研究动机。

- (3): 本文的贡献在于提出了带有随机搜索方向的新的随机梯度下降算法，展示了其几乎必然收敛性、中心极限定理以及渐近性能分析。

- (4): 本文的方法论通过生成随机搜索方向，并研究这些方向的选择对收敛率和收敛行为的影响，形成理论框架和实证分析。

- (5): 本文在模拟数据上进行了数值实验显示了该算法的性能，结果表明所提出的方法在渐近和非渐近收敛率方面具有良好的性能，支持了研究目标。


# AGN
## Galaxy Assembly and Evolution in the P-Millennium simulation: galaxy clustering
- **Url**: http://arxiv.org/abs/2409.02194v2
- **Authors**: ['Fabio Fontanot', 'Gabriella De Lucia', 'Lizhi Xie', 'Michaela Hirschmann', 'Carlton Baugh', 'John C. Helly']
- **Abstrat**: [abridged] We present results from the latest version of the GAEA theoretical model of galaxy formation coupled with merger trees extracted from the Planck Millennium Simulation (PMS). With respect to the Millennium Simulation, the PMS provides a better mass resolution, a larger volume and assumes cosmological parameters consistent with latest results from the Planck mission. The model includes a treatment for the partition of cold gas into atomic and molecular (H$_2$) components; a better treatment for environmental processes; an updated modelling of cold gas accretion on Super-Massive Black Holes. We compare GAEA predictions based on the PMS, with model realizations based on other simulations in the Millennium Suite, showing that the new model provides a remarkable consistency for most statistical properties of galaxy populations. We interpret this as due to the interplay between AGN feedback and H$_2$-based SFR, as model versions considering only one of the two mechanisms do not show the same level of consistency. We then compare model predictions with available data for the galaxy 2-point correlation function (2pCF) in the redshift range 0<z$\lesssim$3. We show that GAEA runs correctly recover the main dependencies of the 2pCF as a function of stellar mass, star formation activity, HI-content and redshift for galaxies more massive than 10$^{9}$ M$_\odot$. These results suggest that our model correctly captures both the distribution of galaxy populations in the Large Scale Structure and the interplay between the main physical processes regulating their baryonic content, both for central and satellite galaxies. The model predicts a small redshift evolution of the clustering amplitude, that results in an overprediction of z$\sim$3 clustering strength with respect to the available estimates, but is still consistent with data within 1-$\sigma$ uncertainties.


**Translated Abstract**: 

我们介绍了GAEA理论模型的最新版本，该模型与从普朗克千年模拟（PMS）提取的合并树相结合。与传统的千年模拟相比，PMS提供了更好的质量分辨率、更大的体积，并假定与普朗克任务的最新结果一致的宇宙参数。该模型同时包含对冷气体分为原子和分子（H$_2$）成分的处理；对环境过程中卫星星系的影响的更好处理；以及对超大质量黑洞冷气体吸积的更新建模。我们将基于PMS的GAEA预测与基于千年套件中其他模拟的模型实现进行比较，发现新模型在大多数星系族群的统计特性方面提供了显著的一致性。我们将这一点解释为由于AGN反馈与基于H$_2$的星形成率（SFR）之间的相互作用，因为仅考虑其中一个机制的模型版本没有表现出同样水平的一致性。然后，我们将模型预测与0<z<3范围内的星系二点相关函数（2pCF）可用数据进行比较。我们展示了GAEA运行正确恢复了星系质量、星形成活动、HI含量和红移作为函数的2pCF的主要依赖性。这些结果表明，我们的模型能够正确捕捉大尺度结构中星系族群的分布及调节其重子含量的主要物理过程之间的相互作用。模型预测簇集幅度的红移演化较小，这导致在z∼3时对簇集强度的过度预测，但仍在1σ不确定性范围内与数据一致。

**Summary**:

  - (1): 本文的研究背景是宇宙大尺度结构（LSS）的演化，以及使用星系二点相关函数（2pCF）作为统计推断工具对星系的分布特性进行更好的理解。

  - (2): 过去的研究方法基于传统的千年模拟，存在较低的质量分辨率和较小的体积，无法有效捕捉星系形成和演化的复杂物理过程。本文提出的方法利用普朗克千年模拟（PMS），改进了模拟精度，关注冷气体的相互作用和AGN反馈，从而更加全面地解决了传统方法的问题。

  - (3): 该论文的贡献在于通过GAEA模型结合PMS数据，提供了一致性的统计特性预测，并更好地捕捉了星系大尺度分布及其物理过程之间的相互作用。

  - (4): 本文的研究方法论是在GAEA模型中注入了来自PMS的合并树，强调了冷气体的原子和分子成分的划分，以及对环境过程和AGN反馈的处理。

  - (5): 本文的模型在0<z<3范围内的星系2pCF任务上表现良好，能够恢复主要依赖关系，支持其目标。虽然在z∼3时的聚类强度有所过度预测，但与数据的一致性仍在1σ不确定性范围内。


## Strong nebular HeII emission induced by He$^+$ ionizing photons escaping through the clumpy winds of massive stars
- **Url**: http://arxiv.org/abs/2501.08376v3
- **Authors**: ['Arpita Roy', 'Mark R. Krumholz', 'Stefania Salvadori', 'Georges Meynet', 'Sylvia Ekström', 'Jorick S. Vink', 'Andreas A. C. Sander', 'Ralph S. Sutherland', 'Sourabh Paul', 'Andrea Pallottini', 'Ása Skúladóttir']
- **Abstrat**: The origin of nebular HeII-emission in both local and high-redshift galaxies remains an unsolved problem. Various theories have been proposed to explain it, including HeII-ionization by high mass X-ray binaries, ultra-luminous X-ray sources, or "stripped" He stars, shock ionization, and hidden AGNs. All these theories have shortcomings, however, leaving the cause of nebular HeII emission unclear. We investigate the hypothesis that the photons responsible for driving nebular HeII emissions are produced by the evolution of single massive stars and/or WR stars. We combine models of stellar evolution with population synthesis and nebular models to identify the most favorable scenarios for producing nebular HeII via this channel. We find that, if WR winds are clumpy enough to become close to optically thin, stellar populations with a wide range of metallicities and rotation rates can produce HeII ionizing photons at rates sufficient to explain the observed nebular $I(HeII)/I(\mathrm{H}\beta)$ ratio $\sim 0.004-0.07$ found in HeII-emitting galaxies. Metal-poor, rapidly rotating stellar populations ($[\mathrm{Fe}/\mathrm{H}]=-2.0$, $v/v_\mathrm{crit}=0.4$) also reach these levels of HeII production even for partially clumpy winds. These scenarios also yield HeII, H$\beta$, and "Blue-Bump" line equivalent widths comparable to those observed in HeII emitters. Only for laminar, non-clumpy winds, do we fail to find combinations of metallicity and stellar rotation rate that yield $I(HeII)/I(\mathrm{H}\beta)$ values as high as those observed in HeII-emitters. Contrary to previous findings, we conclude that single WR stars can be a strong source for nebular HeII emission if their winds are sufficiently clumpy allowing significant escape of hard ionizing photons.


**Translated Abstract**: 

在局部和高红移星系中，氦二 (He II) 发射的起源仍然是一个未解的问题。已经提出了多种理论来解释这一现象，包括高质量 X 射线双星、超亮 X 射线源或因双星相互作用或快速旋转单个大质量恒星演化而形成的“剥离”He 星、冲击电离和隐藏的活跃星系核 (AGN)。然而，所有这些理论都有缺陷，导致氦二发射的原因仍不清楚。我们研究了一个假设，即驱动氦二发射的光子是由单个大质量恒星和/或其风接近光学薄的 Wolf-Rayet (WR) 恒星的演化产生的。我们结合了恒星演化模型与群体合成和星云模型，以确定通过此通道产生氦二的最有利场景。我们的研究发现，如果 WR 风足够不均匀接近光学薄，具有广泛金属丰度和旋转速率的恒星群体能够以足够的比率产生氦二电离光子，从而解释在发射氦二的星系中观察到的氦二与氢β比值 I(He II)/I(Hβ) ∼ 0.004 - 0.07。金属贫乏、快速旋转的恒星群体（[Fe/H] = -2.0, v/v_crit = 0.4）即使在部分不均匀的风中也能达到这些氦二产生水平。这些场景还产生了与氦二发射星系中观察到的氦二、氢β和“蓝峰”行强度相当的等效宽度。仅在均匀、无不均匀风的情况下，我们未能找到金属丰度和恒星旋转速率能够产生与氦二发射星系中观察到的 I(He II)/I(Hβ) 值同样高的组合。与先前的研究结果相反，我们得出结论，如果 WR 恒星的风足够不均匀，则它们可以成为氦二发射的强源。 

**Summary**:

- (1): 本文研究氦二 (He II) 发射的起源问题，这在局部和高红移星系中仍然未解，存在多种提出的理论但都有缺点。

- (2): 以往的方法涉及高质量 X 射线双星、超亮 X 射线源等，但这些理论无法完全解释氦二发射。本文提出的方法是基于单个大质量恒星和 WR 恒星风的演化，解决了以往理论的不足，并对氦二发射的源头进行了新的探索。

- (3): 本文的贡献在于提出了通过不均匀的 WR 风产生氦二电离光子的假设，提供了一个新的解释框架。

- (4): 研究方法结合了恒星演化模型、群体合成和星云模型，计算恒星演化轨迹，分析了不同金属丰度和旋转速率下的氦二电离光子产生情况。

- (5): 研究表明，WR 风足够不均匀时，可以实现与已观察到的氦二/氢β比值 I(He II)/I(Hβ) ∼ 0.004 - 0.07 相符的发射性能，从而支持作者的研究目标。


# ALMA